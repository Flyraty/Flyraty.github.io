<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>2020阴霾之下</title>
    <url>/posts/e6a74345/</url>
    <content><![CDATA[
        <h4 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h4>
      <p>2020 经历了太多的事情，很幸运的是我身边有很多朋友，是他们在我落寞的时候给了我鼓励。</p>
<a id="more"></a>


        <h4 id="清白之年"   >
          <a href="#清白之年" class="heading-link"><i class="fas fa-link"></i></a>清白之年</h4>
      <p>故事开始以前，最初的那些春天<br>阳光洒在杨树上，风吹来， 闪银光<br>街道平静而温暖，钟走得好慢<br>那是我还不识人生之味的年代</p>
<p>我情窦还不开，你的衬衣如雪<br>盼着杨树叶落下，眼睛不眨<br>心里像有一些话，我们先不讲<br>等待着那将要盛装出场的未来</p>
<p>人随风飘荡，天各自一方<br>在风尘中遗忘的清白脸庞<br>此生多勉强，此身越重洋<br>轻描时光漫长低唱语焉不详</p>
<p>数不清的流年，似是而非的脸<br>把你的故事对我讲，就让我笑出泪光<br>是不是生活太艰难，还是活色生香<br>我们都遍体鳞伤，也慢慢坏了心肠<br>你得到你想要的吗，换来的是铁石心肠<br>可曾还有什么人，再让你幻想</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>未知的征程</title>
    <url>/posts/b6b0cbc6/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>2022 年5月份之后发生了很多事情，面试的身心俱疲，亲人的突然病故，面对各种选择时的犹豫不决。未来到底要做成什么样子呢？工作的意义又是什么呢？</p>
<a id="more"></a>]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>Actions - Trigger real calculations</title>
    <url>/posts/8ff659de/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>你可能已经知道 action 会触发提交 Spark 作业，开始进行真正的计算。那么 action 到底是什么，又是如何触发计算的呢？希望本篇可以带你了解这些东西。</p>
<a id="more"></a>

        <h3 id="Action-算子是如何触发计算的？"   >
          <a href="#Action-算子是如何触发计算的？" class="heading-link"><i class="fas fa-link"></i></a>Action 算子是如何触发计算的？</h3>
      <p>这里可以简单看下源码，首先进入到 spark-core 中的 org.apache.spark.rdd.RDD.scala。这个文件是干啥的嘞。可以直接看注释，RDD.scala 声明了什么是 RDD，已经定义在一些 RDD 上的算子操作，spark 调度和执行是否完成都依赖这个文件里定义的方法。因此，可以在这里找到一个任意 action 算子进行分析。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd46hfy7xgj313w0nugsi.jpg"></p>
<p>这里可以看下 collect 算子。它的功能就是将各个分区的结果数据都拉取到 driver 端，并将结果放到 Array 里面。结果是通过 runJob 执行拿到的。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd46i0he7kj311a08udhj.jpg"></p>
<p>点开 runJob 方法，其参数是 RDD 以及在 RDD 每个分区上上执行的方法。其内部执行的 runJob 就是下方重载的 runJob，多了一个 RDD 分区的参数。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd46i82tv5j315c0n8afb.jpg"></p>
<p>内部又是一个重载的 runJob，通过注释可以看到这个方法是 Spark 中所有 action 最终执行 runJob 的地方。这里先判断 SparkContext 是否存活，如果存活，将这个算子交给 DAG Scheduler，完成后会做 in memory 的 checkpoint，应该是为了任务失败自动恢复。前面说过 DAG Scheduler 是用来构建 DAG 图，划分 Stage，分发 task 的。emnn，下面就是这些逻辑了。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd46ih7vq4j314g0o4dm1.jpg"></p>
<p>点进 DAG Scheduler 的 runJob，此处是提交 action job 到 Scheduler，并阻塞直到 Job 完成，如果 Job 执行失败，会抛出错误。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd46itxgrij318n0u0guf.jpg"></p>
<p>点进 submitJob，submitJob 首先会检查分区是否存在，如果存在，赋给该 job 一个 jobId，用以区分不同的 Job。如果分区数为0，返回一个 JobWaiter，这里涉及到 Scala future，promise 的知识。不太懂 。如果分区数大于 0，把该 job 发送到 eventProcessLoop。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd46j8jfnbj308c070mx7.jpg"></p>
<p>这里的 eventProcessLoop 就是 DAGSchedulerEventProcessLoop，Job 被发送到 DAGScheduler 的事件处理循环去处理。上面的 post 的方法是 EventLoop 实现的。DAGSchedulerEventProcessLoop 继承了 EventLoop。EventLoop 的 doOnReceive 方法里是事件触发的执行逻辑。 这里直接看 DAGSchedulerEventProcessLoop 里的实现。上一步 submitJob 中 post 的是 JobSubmitted。这里会对应执行里面的 handleJobSubmitted 方法。同样的可以点进去看看，里面是划分 stage 的具体逻辑。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd46jk2stdj317402uwf7.jpg"></p>
<p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd46juctasj317c07uta4.jpg"></p>
<p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd46k1j03mj315l0u0do0.jpg"></p>
<p>到这里，基本上可以看到 action 触发计算的逻辑。action → runJob → DAGScheduler runJob → submitJob → DAGSchedulerEventProcessLoop → handleJobSubmitted → createShuffleMapStage。</p>

        <h3 id="Action"   >
          <a href="#Action" class="heading-link"><i class="fas fa-link"></i></a>Action</h3>
      <p>根据源码里标识的函数签名，action 分为 basic action 和 action。但是到底有啥区别，这个还没搞清楚。</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">Basic actions are a group of operators (methods) of the Dataset API for transforming a Dataset into a session-scoped or global temporary view and other basic actions </span><br></pre></td></tr></table></div></figure>

<p>这里介绍下常见的 action 算子，我们也很容易想到，打印，输出到各种数据存储这些都是 action</p>
<div class="table-container"><table>
<thead>
<tr>
<th>Method</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody><tr>
<td>cache/persist</td>
<td>缓存或者持久化，区别是默认的缓存级别不一样</td>
<td></td>
</tr>
<tr>
<td>checkpoint</td>
<td>检查点，可用于恢复 Spark Job 的执行，</td>
<td></td>
</tr>
<tr>
<td>columns</td>
<td>返回包含所有列名的序列</td>
<td></td>
</tr>
<tr>
<td>createGlobalTempView</td>
<td>创建临时视图，创建视图的方法有好几个，可以去自己查下，区别是视图的声明周期不一样，和 SparkSession 有关</td>
<td></td>
</tr>
<tr>
<td>explain</td>
<td>输出详细的执行计划</td>
<td></td>
</tr>
<tr>
<td>isEmpty</td>
<td>数据集是否为空</td>
<td></td>
</tr>
<tr>
<td>printSchema</td>
<td>打印数据集的 schema</td>
<td></td>
</tr>
<tr>
<td>rdd</td>
<td>转换为 RDD</td>
<td></td>
</tr>
<tr>
<td>write</td>
<td>DataFrameWriter</td>
<td></td>
</tr>
<tr>
<td>toDF</td>
<td></td>
<td></td>
</tr>
<tr>
<td>schema</td>
<td></td>
<td></td>
</tr>
<tr>
<td>collect</td>
<td>收集计算结果到 driver 端，返回包含结果的Array</td>
<td></td>
</tr>
<tr>
<td>first</td>
<td></td>
<td></td>
</tr>
<tr>
<td>show</td>
<td>展示几条数据，常用的参数就是 numRows，和 truncate，分别是显示多少行，是否每列显示完整</td>
<td></td>
</tr>
</tbody></table></div>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Aggregate - 普通的聚合计算和基于 window 的聚合计算</title>
    <url>/posts/bfb6e21b/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>聚合计算对于数据统计有着重要的作用，比如常见的 Top N 问题。本文主要介绍常见的聚合计算函数以及基于 Window 的处理。</p>
<a id="more"></a>


        <h3 id="Basic-Aggregate"   >
          <a href="#Basic-Aggregate" class="heading-link"><i class="fas fa-link"></i></a>Basic Aggregate</h3>
      <p>Spark 提供了 groupBy 等聚合操作以及在这些操作的上的聚合函数。聚合操作符其实也就是转换算子，可以在 Dataset.scala RDD.scala 中查看。聚合函数定义在 org.apache.spark.sql.functions 中。函数签名  @group为 agg_funcs。</p>
<p>在讲转换算子的时候，已经简单提到了 groupBy，groupByKey，agg 这些聚合操作符。可以在回顾一下。</p>

        <h4 id="RelationGroupedDataset-与-KeyValueGroupedDataset"   >
          <a href="#RelationGroupedDataset-与-KeyValueGroupedDataset" class="heading-link"><i class="fas fa-link"></i></a>RelationGroupedDataset 与 KeyValueGroupedDataset</h4>
      <p>RelationGroupedDataset 是 DataFrame 聚合计算的接口，groupBy，cube，rollup，pivot 算子会生成 RelationGroupedDataset。其支持的聚合计算函数都定义在 spark-sql 的 RelationGroupedDataset.scala 里面。</p>
<p>KeyValueGroupedDataset 用于 TypedColumn 的聚合计算，其作用的数据集往往是自定义的 Scala 对象，而不是 Rows。由 groupByKey 算子生成。其支持的聚合计算函数都定义在 spark-sql 的 KeyValueGroupedDataset.scala 里面。</p>
<p>因为 KeyValueGroupedDataset 处在试验阶段，两者支持的聚合计算函数有些差别，具体可以查看源码。</p>

        <h4 id="groupBy"   >
          <a href="#groupBy" class="heading-link"><i class="fas fa-link"></i></a>groupBy</h4>
      <p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd7rp1yu06j313a0a4mzz.jpg"></p>
<p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd7rptny42j313c0m878r.jpg"></p>
<p>groupBy 就是返回 key 及 key 对应的数据集，不能保证每组有序。因此如果你想保证分组内有序，可能会用到下面的 Window Agg。使用 groupBy 的代价比较大，是先通过 shuffle 将各个 Key 对应的数据拉到各个对应分区下，再进行聚合计算。注释里建议使用 aggregateByKey 或者 reduceByKey。另外 Dataset groupBy 接收的参数是 String 类型。</p>
<p>aggregateByKey 是先聚合计算，在合并分区，相对来说计算量小，所以性能比较好。接收一个初始值，一个分区内聚合函数，一个分区结果合并函数作为参数。更多使用可以参考<span class="exturl"><a class="exturl__link"   href="https://gist.github.com/tmcgrath/dd8a0f5fb19201deb65f" >aggreagteByKey</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> nums = spark.range(<span class="number">10</span>).withColumn(<span class="string">&quot;remainder&quot;</span>, $<span class="string">&quot;id&quot;</span> % <span class="number">2</span>)</span><br><span class="line">nums: org.apache.spark.sql.<span class="type">DataFrame</span> = [id: bigint, remainder: bigint]</span><br><span class="line"></span><br><span class="line">scala&gt; nums.show()</span><br><span class="line">+---+---------+</span><br><span class="line">| id|remainder|</span><br><span class="line">+---+---------+</span><br><span class="line">|  <span class="number">0</span>|        <span class="number">0</span>|</span><br><span class="line">|  <span class="number">1</span>|        <span class="number">1</span>|</span><br><span class="line">|  <span class="number">2</span>|        <span class="number">0</span>|</span><br><span class="line">|  <span class="number">3</span>|        <span class="number">1</span>|</span><br><span class="line">|  <span class="number">4</span>|        <span class="number">0</span>|</span><br><span class="line">|  <span class="number">5</span>|        <span class="number">1</span>|</span><br><span class="line">|  <span class="number">6</span>|        <span class="number">0</span>|</span><br><span class="line">|  <span class="number">7</span>|        <span class="number">1</span>|</span><br><span class="line">|  <span class="number">8</span>|        <span class="number">0</span>|</span><br><span class="line">|  <span class="number">9</span>|        <span class="number">1</span>|</span><br><span class="line">+---+---------+</span><br><span class="line"></span><br><span class="line">scala&gt; scala&gt; nums.agg(sum(<span class="string">&quot;id&quot;</span>)).show()</span><br><span class="line">+-------+</span><br><span class="line">|sum(id)|</span><br><span class="line">+-------+</span><br><span class="line">|     <span class="number">45</span>|</span><br><span class="line">+-------+</span><br><span class="line">scala&gt; nums.groupBy(<span class="string">&quot;remainder&quot;</span>)</span><br><span class="line">res4: org.apache.spark.sql.<span class="type">RelationalGroupedDataset</span> = <span class="type">RelationalGroupedDataset</span>: [grouping expressions: [remainder: bigint], value: [id: bigint, remainder: bigint], <span class="class"><span class="keyword">type</span></span>: <span class="type">GroupBy</span>]</span><br><span class="line"></span><br><span class="line">scala&gt; nums.groupBy(<span class="string">&quot;remainder&quot;</span>).agg(sum(<span class="string">&quot;id&quot;</span>)).show()</span><br><span class="line">+---------+-------+</span><br><span class="line">|remainder|sum(id)|</span><br><span class="line">+---------+-------+</span><br><span class="line">|        <span class="number">0</span>|     <span class="number">20</span>|</span><br><span class="line">|        <span class="number">1</span>|     <span class="number">25</span>|</span><br><span class="line">+---------+-------+</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Seq</span>((<span class="number">1</span>, <span class="number">2</span>), (<span class="number">1</span>, <span class="number">4</span>), (<span class="number">2</span>, <span class="number">5</span>), (<span class="number">2</span>, <span class="number">6</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">12</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res19: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">1</span>,<span class="number">2</span>), (<span class="number">1</span>,<span class="number">4</span>), (<span class="number">2</span>,<span class="number">5</span>), (<span class="number">2</span>,<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.aggregateByKey(<span class="number">0</span>)((a, b) =&gt; a + b, (res1, res2) =&gt; res1 +res2).collect</span><br><span class="line">res20: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">1</span>,<span class="number">6</span>), (<span class="number">2</span>,<span class="number">11</span>))</span><br></pre></td></tr></table></div></figure>


        <h4 id="groupByKey"   >
          <a href="#groupByKey" class="heading-link"><i class="fas fa-link"></i></a>groupByKey</h4>
      <p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd7rq7b0l6j30zw0jg0wi.jpg"></p>
<p>groupByKey 还处在试验阶段，接收一个函数作为参数，返回 KeyValueGroupedDataset。想的是让分组的条件可以更加的灵活，不局限于基于列名的分组。比如基于数组里的第几个元素。与 groupBy 一样，是先 shuffle ，在聚合计算。通过下面的代码，我们可以看到 Dataset + groupByKey 后面跟的计算函数比较少，不支持 sum，avg 等。另外还需要注意这里已经是 TypedColumn。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">People</span>(<span class="params">name:<span class="type">String</span>, salary:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">defined</span> <span class="title">class</span> <span class="title">People</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">val</span> <span class="title">ds</span> </span>= <span class="type">Seq</span>(<span class="type">People</span>(<span class="string">&quot;Mike&quot;</span>, <span class="number">1000</span>), <span class="type">People</span>(<span class="string">&quot;Mike&quot;</span>, <span class="number">600</span>), <span class="type">People</span>(<span class="string">&quot;David&quot;</span>, <span class="number">400</span>)).toDS</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">People</span>] = [name: string, salary: int]</span><br><span class="line"></span><br><span class="line">scala&gt; ds.groupByKey(_.salary)</span><br><span class="line">res21: org.apache.spark.sql.<span class="type">KeyValueGroupedDataset</span>[<span class="type">Int</span>,<span class="type">People</span>] = <span class="type">KeyValueGroupedDataset</span>: [key: [value: int], value: [name: string, salary: int]]</span><br><span class="line"></span><br><span class="line">scala&gt; ds.groupByKey(_.salary).sum(<span class="string">&quot;salary&quot;</span>).show()</span><br><span class="line">&lt;console&gt;:<span class="number">26</span>: error: value sum is not a member of org.apache.spark.sql.<span class="type">KeyValueGroupedDataset</span>[<span class="type">Int</span>,<span class="type">People</span>]</span><br><span class="line">       ds.groupByKey(_.salary).sum(<span class="string">&quot;salary&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">scala&gt; ds.groupByKey(_.salary).agg(avg(<span class="string">&quot;salary&quot;</span>)).show()</span><br><span class="line">&lt;console&gt;:<span class="number">26</span>: error: <span class="class"><span class="keyword">type</span> <span class="title">mismatch</span></span>;</span><br><span class="line"> found   : org.apache.spark.sql.<span class="type">Column</span></span><br><span class="line"> required: org.apache.spark.sql.<span class="type">TypedColumn</span>[<span class="type">People</span>,?]</span><br><span class="line">       ds.groupByKey(_.salary).agg(avg(<span class="string">&quot;salary&quot;</span>)).show()</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.sql.expressions.scalalang._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.scalalang._</span><br><span class="line"></span><br><span class="line">scala&gt; ds.groupByKey(_.name).agg(typed.avg[<span class="type">People</span>](_.salary)).show()</span><br><span class="line">+-----+------------------------------------------+</span><br><span class="line">|value|<span class="type">TypedAverage</span>($line43.$read$$iw$$iw$<span class="type">People</span>)|</span><br><span class="line">+-----+------------------------------------------+</span><br><span class="line">| <span class="type">Mike</span>|                                     <span class="number">800.0</span>|</span><br><span class="line">|<span class="type">David</span>|                                     <span class="number">400.0</span>|</span><br><span class="line">+-----+------------------------------------------+</span><br></pre></td></tr></table></div></figure>

        <h4 id="agg"   >
          <a href="#agg" class="heading-link"><i class="fas fa-link"></i></a>agg</h4>
      <p>agg 一般作用于分组后的数据集上，也可以作用于整个数据集上。其具体的方法可以查看 spakr-sql Dataset.scala。agg 可以接收 map，expression 类型的参数。直接看例子吧</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; nums.groupBy(<span class="string">&quot;remainder&quot;</span>).agg(<span class="type">Map</span>(<span class="string">&quot;id&quot;</span> -&gt; <span class="string">&quot;sum&quot;</span>)).show()</span><br><span class="line">+---------+-------+</span><br><span class="line">|remainder|sum(id)|</span><br><span class="line">+---------+-------+</span><br><span class="line">|        <span class="number">0</span>|     <span class="number">20</span>|</span><br><span class="line">|        <span class="number">1</span>|     <span class="number">25</span>|</span><br><span class="line">+---------+-------+</span><br><span class="line"></span><br><span class="line">scala&gt; nums.groupBy(<span class="string">&quot;remainder&quot;</span>).agg(sum(<span class="string">&quot;id&quot;</span>).alias(<span class="string">&quot;sum&quot;</span>), avg(<span class="string">&quot;id&quot;</span>).alias(<span class="string">&quot;avg&quot;</span>)).show()</span><br><span class="line">+---------+---+---+</span><br><span class="line">|remainder|sum|avg|</span><br><span class="line">+---------+---+---+</span><br><span class="line">|        <span class="number">0</span>| <span class="number">20</span>|<span class="number">4.0</span>|</span><br><span class="line">|        <span class="number">1</span>| <span class="number">25</span>|<span class="number">5.0</span>|</span><br><span class="line">+---------+---+---+</span><br></pre></td></tr></table></div></figure>


        <h3 id="Window-Aggregate"   >
          <a href="#Window-Aggregate" class="heading-link"><i class="fas fa-link"></i></a>Window Aggregate</h3>
      
        <h4 id="Window"   >
          <a href="#Window" class="heading-link"><i class="fas fa-link"></i></a>Window</h4>
      <p>窗口计算，这里的 window 像是有序的分组。window 相比 groupBy 保证了分组有序。emnn，当然了，你也可以自己选择初始化 window 的时候不进行 orderBy。</p>
<p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd7rqjkae3j314q0es41o.jpg"></p>
<p>window 的源码在 spark.sql  的 expressions.window 中。从源码中可以看到 window 的类型是一个叫 windowSpec 的东西。点进去，可以发现创建一个 windowSpec 所需的东西 → parationBy，orderBy，Frame 边界。 parationBy 和 orderBy 比较容易理解。就是分区和区内排序。下面说要 Window Frame 的一些概念。</p>
<p>Frame 边界可以由 rowsbetween，rangeBetween 等指定。而边界会有一些变量和偏移量来指定。比如 （window.currentRow，2）的含义就是当前行和后面两行作为一个计算单元。</p>
<div class="table-container"><table>
<thead>
<tr>
<th>变量</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>window.currentRow</td>
<td>当前行</td>
</tr>
<tr>
<td>window.unboundedPreceding</td>
<td>分区内的第一行</td>
</tr>
<tr>
<td>window.unboundedFollowing</td>
<td>分区内的最后一行</td>
</tr>
</tbody></table></div>
<p>下面的例子定义了几个 window，你应该已经发现了问题，这几个 window 的 parationBy，orderBy和边界都是一样的，但是基于 window 的计算结果确是不一样的。这是因为定义边界的方法不一样。 </p>
<div class="table-container"><table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>rowsBetween</td>
<td>物理边界，rowsBetween(Window.currentRow, 1)，对于下面的例子来说，就是分区内当前行与前一行</td>
</tr>
<tr>
<td>rangeBetween</td>
<td>逻辑边界，rangeBetween(Window.currentRow, 1)，对于下面的例子来说，就是符合当前行 &lt;= id &lt;= 当前行的值 + 1 的所有行作为一个计算单元。rangeBetween(-1, 3)，就是 当前行的值 - 1 &lt;= id &lt;= 当前行的值 + 3 的所有行作为一个计算单元</td>
</tr>
</tbody></table></div>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = <span class="type">Seq</span>((<span class="number">1</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">1</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">1</span>, <span class="string">&quot;b&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;b&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;b&quot;</span>)).toDF(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;category&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [id: int, category: string]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> window = <span class="type">Window</span>.partitionBy(<span class="string">&quot;category&quot;</span>).orderBy(<span class="string">&quot;id&quot;</span>).rowsBetween(<span class="type">Window</span>.currentRow, <span class="number">1</span>)</span><br><span class="line">window: org.apache.spark.sql.expressions.<span class="type">WindowSpec</span> = org.apache.spark.sql.expressions.<span class="type">WindowSpec</span>@<span class="number">62</span>ff2e08</span><br><span class="line"></span><br><span class="line">scala&gt; df.withColumn(<span class="string">&quot;sum&quot;</span>, sum(<span class="string">&quot;id&quot;</span>) over window).show()</span><br><span class="line">+---+--------+---+</span><br><span class="line">| id|category|sum|</span><br><span class="line">+---+--------+---+</span><br><span class="line">|  <span class="number">1</span>|       b|  <span class="number">3</span>|</span><br><span class="line">|  <span class="number">2</span>|       b|  <span class="number">5</span>|</span><br><span class="line">|  <span class="number">3</span>|       b|  <span class="number">3</span>|</span><br><span class="line">|  <span class="number">1</span>|       a|  <span class="number">2</span>|</span><br><span class="line">|  <span class="number">1</span>|       a|  <span class="number">3</span>|</span><br><span class="line">|  <span class="number">2</span>|       a|  <span class="number">2</span>|</span><br><span class="line">+---+--------+---+</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> window = <span class="type">Window</span>.partitionBy(<span class="string">&quot;category&quot;</span>).orderBy(<span class="string">&quot;id&quot;</span>).rangeBetween(<span class="type">Window</span>.currentRow, <span class="number">1</span>)</span><br><span class="line">window: org.apache.spark.sql.expressions.<span class="type">WindowSpec</span> = org.apache.spark.sql.expressions.<span class="type">WindowSpec</span>@<span class="number">6</span>fae2aa5</span><br><span class="line"></span><br><span class="line">scala&gt; df.withColumn(<span class="string">&quot;sum&quot;</span>, sum(<span class="string">&quot;id&quot;</span>) over window).show()</span><br><span class="line">+---+--------+---+</span><br><span class="line">| id|category|sum|</span><br><span class="line">+---+--------+---+</span><br><span class="line">|  <span class="number">1</span>|       b|  <span class="number">3</span>|</span><br><span class="line">|  <span class="number">2</span>|       b|  <span class="number">5</span>|</span><br><span class="line">|  <span class="number">3</span>|       b|  <span class="number">3</span>|</span><br><span class="line">|  <span class="number">1</span>|       a|  <span class="number">4</span>|</span><br><span class="line">|  <span class="number">1</span>|       a|  <span class="number">4</span>|</span><br><span class="line">|  <span class="number">2</span>|       a|  <span class="number">2</span>|</span><br><span class="line">+---+--------+---+</span><br></pre></td></tr></table></div></figure>


        <h4 id="Window-Aggregate-Functions"   >
          <a href="#Window-Aggregate-Functions" class="heading-link"><i class="fas fa-link"></i></a>Window Aggregate Functions</h4>
      <p>除了和 groupBy 一样，有 sum，max，avg 等函数外。window 还为我们提供了处理每个分组内行与行之间计算的方法。</p>
<div class="table-container"><table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>rank</td>
<td>排行</td>
</tr>
<tr>
<td>dense_rank</td>
<td>排行</td>
</tr>
<tr>
<td>ntile</td>
<td>排行</td>
</tr>
<tr>
<td>precent_rank</td>
<td></td>
</tr>
<tr>
<td>cume_dist</td>
<td>累计概率分布</td>
</tr>
<tr>
<td>row_number</td>
<td>增加行号列</td>
</tr>
<tr>
<td>lag</td>
<td>计算当前行与前 offset 行的差值，没有会为 null，可以设置 defaultValue。</td>
</tr>
<tr>
<td>lead</td>
<td>计算当前行与后 offset 行的差值，没有会为 null，可以设置 defaultValue。</td>
</tr>
<tr>
<td>rank</td>
<td>遇到重复值和 dense_rank 遇到重复值的处理方式是不一样的。rank 遇到重复值使用相同排行，但是整体排行会增加，而 dense_rank 不会增加。</td>
</tr>
</tbody></table></div>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataset = spark.range(<span class="number">9</span>).withColumn(<span class="string">&quot;bucket&quot;</span>, <span class="symbol">&#x27;id</span> % <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line"><span class="keyword">val</span> byBucket = <span class="type">Window</span>.partitionBy(<span class="symbol">&#x27;bucket</span>).orderBy(<span class="symbol">&#x27;id</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; dataset.union(dataset).withColumn(<span class="string">&quot;rank&quot;</span>, rank over byBucket).show</span><br><span class="line">+---+------+----+</span><br><span class="line">| id|bucket|rank|</span><br><span class="line">+---+------+----+</span><br><span class="line">|  <span class="number">0</span>|     <span class="number">0</span>|   <span class="number">1</span>|</span><br><span class="line">|  <span class="number">0</span>|     <span class="number">0</span>|   <span class="number">1</span>|</span><br><span class="line">|  <span class="number">3</span>|     <span class="number">0</span>|   <span class="number">3</span>|</span><br><span class="line">|  <span class="number">3</span>|     <span class="number">0</span>|   <span class="number">3</span>|</span><br><span class="line">|  <span class="number">6</span>|     <span class="number">0</span>|   <span class="number">5</span>|</span><br><span class="line">|  <span class="number">6</span>|     <span class="number">0</span>|   <span class="number">5</span>|</span><br><span class="line">|  <span class="number">1</span>|     <span class="number">1</span>|   <span class="number">1</span>|</span><br><span class="line">|  <span class="number">1</span>|     <span class="number">1</span>|   <span class="number">1</span>|</span><br><span class="line">|  <span class="number">4</span>|     <span class="number">1</span>|   <span class="number">3</span>|</span><br><span class="line">|  <span class="number">4</span>|     <span class="number">1</span>|   <span class="number">3</span>|</span><br><span class="line">|  <span class="number">7</span>|     <span class="number">1</span>|   <span class="number">5</span>|</span><br><span class="line">|  <span class="number">7</span>|     <span class="number">1</span>|   <span class="number">5</span>|</span><br><span class="line">|  <span class="number">2</span>|     <span class="number">2</span>|   <span class="number">1</span>|</span><br><span class="line">|  <span class="number">2</span>|     <span class="number">2</span>|   <span class="number">1</span>|</span><br><span class="line">|  <span class="number">5</span>|     <span class="number">2</span>|   <span class="number">3</span>|</span><br><span class="line">|  <span class="number">5</span>|     <span class="number">2</span>|   <span class="number">3</span>|</span><br><span class="line">|  <span class="number">8</span>|     <span class="number">2</span>|   <span class="number">5</span>|</span><br><span class="line">|  <span class="number">8</span>|     <span class="number">2</span>|   <span class="number">5</span>|</span><br><span class="line">+---+------+----+</span><br><span class="line"></span><br><span class="line">scala&gt; dataset.union(dataset).withColumn(<span class="string">&quot;dense_rank&quot;</span>, dense_rank over byBucket).show</span><br><span class="line">+---+------+----------+</span><br><span class="line">| id|bucket|dense_rank|</span><br><span class="line">+---+------+----------+</span><br><span class="line">|  <span class="number">0</span>|     <span class="number">0</span>|         <span class="number">1</span>|</span><br><span class="line">|  <span class="number">0</span>|     <span class="number">0</span>|         <span class="number">1</span>|</span><br><span class="line">|  <span class="number">3</span>|     <span class="number">0</span>|         <span class="number">2</span>|</span><br><span class="line">|  <span class="number">3</span>|     <span class="number">0</span>|         <span class="number">2</span>|</span><br><span class="line">|  <span class="number">6</span>|     <span class="number">0</span>|         <span class="number">3</span>|</span><br><span class="line">|  <span class="number">6</span>|     <span class="number">0</span>|         <span class="number">3</span>|</span><br><span class="line">|  <span class="number">1</span>|     <span class="number">1</span>|         <span class="number">1</span>|</span><br><span class="line">|  <span class="number">1</span>|     <span class="number">1</span>|         <span class="number">1</span>|</span><br><span class="line">|  <span class="number">4</span>|     <span class="number">1</span>|         <span class="number">2</span>|</span><br><span class="line">|  <span class="number">4</span>|     <span class="number">1</span>|         <span class="number">2</span>|</span><br><span class="line">|  <span class="number">7</span>|     <span class="number">1</span>|         <span class="number">3</span>|</span><br><span class="line">|  <span class="number">7</span>|     <span class="number">1</span>|         <span class="number">3</span>|</span><br><span class="line">|  <span class="number">2</span>|     <span class="number">2</span>|         <span class="number">1</span>|</span><br><span class="line">|  <span class="number">2</span>|     <span class="number">2</span>|         <span class="number">1</span>|</span><br><span class="line">|  <span class="number">5</span>|     <span class="number">2</span>|         <span class="number">2</span>|</span><br><span class="line">|  <span class="number">5</span>|     <span class="number">2</span>|         <span class="number">2</span>|</span><br><span class="line">|  <span class="number">8</span>|     <span class="number">2</span>|         <span class="number">3</span>|</span><br><span class="line">|  <span class="number">8</span>|     <span class="number">2</span>|         <span class="number">3</span>|</span><br><span class="line">+---+------+----------+</span><br></pre></td></tr></table></div></figure>
<p>其他一些函数，我个人也写了一些示例，感兴趣可以运行一下</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">Window</span>, <span class="type">WindowSpec</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">window_aggregate</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder.master(<span class="string">&quot;local[*]&quot;</span>).getOrCreate()</span><br><span class="line">  <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">   * 详细可以查看源码，类型 org.apache.spark.sql.expressions.WindowSpec</span></span><br><span class="line"><span class="comment">   * val window:WindowSpec = Window.partitionBy(&quot;id&quot;).orderBy(&quot;year&quot;)</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Salary</span>(<span class="params">depName: <span class="type">String</span>, empNo: <span class="type">Long</span>, salary: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class">  <span class="title">val</span> <span class="title">empSalary</span> </span>= <span class="type">Seq</span>(</span><br><span class="line">    <span class="type">Salary</span>(<span class="string">&quot;sales&quot;</span>, <span class="number">1</span>, <span class="number">5000</span>),</span><br><span class="line">    <span class="type">Salary</span>(<span class="string">&quot;personnel&quot;</span>, <span class="number">2</span>, <span class="number">3900</span>),</span><br><span class="line">    <span class="type">Salary</span>(<span class="string">&quot;sales&quot;</span>, <span class="number">3</span>, <span class="number">4800</span>),</span><br><span class="line">    <span class="type">Salary</span>(<span class="string">&quot;sales&quot;</span>, <span class="number">4</span>, <span class="number">4800</span>),</span><br><span class="line">    <span class="type">Salary</span>(<span class="string">&quot;personnel&quot;</span>, <span class="number">5</span>, <span class="number">3500</span>),</span><br><span class="line">    <span class="type">Salary</span>(<span class="string">&quot;develop&quot;</span>, <span class="number">7</span>, <span class="number">4200</span>),</span><br><span class="line">    <span class="type">Salary</span>(<span class="string">&quot;develop&quot;</span>, <span class="number">8</span>, <span class="number">6000</span>),</span><br><span class="line">    <span class="type">Salary</span>(<span class="string">&quot;develop&quot;</span>, <span class="number">9</span>, <span class="number">4500</span>),</span><br><span class="line">    <span class="type">Salary</span>(<span class="string">&quot;develop&quot;</span>, <span class="number">10</span>, <span class="number">5200</span>),</span><br><span class="line">    <span class="type">Salary</span>(<span class="string">&quot;develop&quot;</span>, <span class="number">11</span>, <span class="number">5200</span>)).toDS</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * window Rank functions</span></span><br><span class="line"><span class="comment">     * rank</span></span><br><span class="line"><span class="comment">     * dense_rank</span></span><br><span class="line"><span class="comment">     * row_number</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> nameWindowDesc = <span class="type">Window</span>.partitionBy(<span class="string">&quot;depName&quot;</span>).orderBy($<span class="string">&quot;salary&quot;</span>.desc)</span><br><span class="line">    empSalary.withColumn(<span class="string">&quot;rank&quot;</span>, rank() over nameWindowDesc).show()</span><br><span class="line"></span><br><span class="line">    empSalary</span><br><span class="line">      .withColumn(<span class="string">&quot;dense_rank&quot;</span>, dense_rank() over nameWindowDesc)  <span class="comment">// Top n Salary</span></span><br><span class="line">      .filter($<span class="string">&quot;dense_rank&quot;</span> &lt;= <span class="number">2</span>)</span><br><span class="line">      .show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * window Analytic functions</span></span><br><span class="line"><span class="comment">     * lag  计算当前行与前 offset 行, 没有会置null, 也可以设置参数 default_value</span></span><br><span class="line"><span class="comment">     * lead 计算当前行与后 offset 行</span></span><br><span class="line"><span class="comment">     * cume_dist 出现的概率累计分布</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> diffSalaryPerRow = empSalary.withColumn(<span class="string">&quot;diff&quot;</span>, lag($<span class="string">&quot;salary&quot;</span>, <span class="number">1</span>) over nameWindowDesc)</span><br><span class="line">    <span class="keyword">val</span> diffSalaryTwoRow = empSalary.withColumn(<span class="string">&quot;diff&quot;</span>, lead($<span class="string">&quot;salary&quot;</span>, <span class="number">2</span>) over nameWindowDesc)</span><br><span class="line"></span><br><span class="line">    diffSalaryPerRow.show()</span><br><span class="line">    diffSalaryTwoRow.show()</span><br><span class="line"></span><br><span class="line">    empSalary</span><br><span class="line">      .withColumn(<span class="string">&quot;cume_dist&quot;</span>, cume_dist() over nameWindowDesc)</span><br><span class="line">      .show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * window Aggregate functions</span></span><br><span class="line"><span class="comment">     * 注意一下rangeBetween, rowBetween, 说白了就是为window Frame 的计算设置边界。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> nameWindow = <span class="type">Window</span>.partitionBy(<span class="string">&quot;depName&quot;</span>)</span><br><span class="line">    empSalary.withColumn(<span class="string">&quot;avg&quot;</span>, avg($<span class="string">&quot;salary&quot;</span>) over nameWindow).show()</span><br><span class="line">    empSalary.withColumn(<span class="string">&quot;avg&quot;</span>, max($<span class="string">&quot;salary&quot;</span>) over nameWindow).show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rangeWindow = <span class="type">Window</span>.rangeBetween(<span class="type">Window</span>.currentRow, <span class="number">1</span>)</span><br><span class="line">    empSalary.withColumn(<span class="string">&quot;avg&quot;</span>, sum($<span class="string">&quot;salary&quot;</span>) over rangeWindow).show() <span class="comment">// 打印出来会是什么呢？</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> maxSalary = max($<span class="string">&quot;salary&quot;</span>).over(nameWindow) - $<span class="string">&quot;salary&quot;</span></span><br><span class="line">    empSalary.withColumn(<span class="string">&quot;salary_max_diff&quot;</span>, maxSalary).show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> orderWindow = <span class="type">Window</span>.orderBy(<span class="string">&quot;salary&quot;</span>) <span class="comment">// 会报Warning, 没有partitionBy 会导致数据都到一个partition下面</span></span><br><span class="line">    empSalary</span><br><span class="line">      .withColumn(<span class="string">&quot;salary_total&quot;</span>, sum(<span class="string">&quot;salary&quot;</span>) over orderWindow)</span><br><span class="line">      .show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></div></figure>


        <h3 id="Multidimensional-Analysis"   >
          <a href="#Multidimensional-Analysis" class="heading-link"><i class="fas fa-link"></i></a>Multidimensional Analysis</h3>
      <p>多维度分析。根据指定列的排列组合进行 groupBy。</p>

        <h4 id="cube-rollup"   >
          <a href="#cube-rollup" class="heading-link"><i class="fas fa-link"></i></a>cube / rollup</h4>
      <p>cube，rollup 都用于多维度分析。区别在于 cube 是对传入的列的所有组合来 groupBy，而 rollup 是层次级别的。比如下面的例子，cube 的操作是 (age, city)，(age)， (city)，全表分组。rollup的操作是 (age, city)，(age)，全表分组。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> columns = <span class="type">Seq</span>(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>, <span class="string">&quot;city&quot;</span>, <span class="string">&quot;vip_level&quot;</span>, <span class="string">&quot;view_count&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> users = <span class="type">Seq</span>(</span><br><span class="line">		(<span class="string">&quot;Tom&quot;</span>, <span class="number">10</span>, <span class="string">&quot;北京&quot;</span>, <span class="number">0</span>, <span class="number">111</span>),</span><br><span class="line">		(<span class="string">&quot;Jack&quot;</span>, <span class="number">20</span>, <span class="string">&quot;上海&quot;</span>, <span class="number">1</span>, <span class="number">3000</span>),</span><br><span class="line">		(<span class="string">&quot;David&quot;</span>, <span class="number">23</span>, <span class="string">&quot;北京&quot;</span>, <span class="number">3</span>, <span class="number">4000</span>),</span><br><span class="line">		(<span class="string">&quot;Mary&quot;</span>, <span class="number">18</span>, <span class="string">&quot;上海&quot;</span>, <span class="number">1</span>, <span class="number">1234</span>),</span><br><span class="line">		(<span class="string">&quot;Tony&quot;</span>, <span class="number">50</span>, <span class="string">&quot;深圳&quot;</span>, <span class="number">0</span>, <span class="number">200</span>),</span><br><span class="line">		(<span class="string">&quot;Group&quot;</span>, <span class="number">60</span>, <span class="string">&quot;北京&quot;</span>, <span class="number">0</span>, <span class="number">40</span>),</span><br><span class="line">		(<span class="string">&quot;Ali&quot;</span>, <span class="number">34</span>, <span class="string">&quot;上海&quot;</span>, <span class="number">4</span>, <span class="number">6666</span>),</span><br><span class="line">		(<span class="string">&quot;Alas&quot;</span>, <span class="number">45</span>, <span class="string">&quot;北京&quot;</span>, <span class="number">2</span>, <span class="number">10000</span>)</span><br><span class="line">	).toDF(columns: _*)</span><br><span class="line"></span><br><span class="line">users</span><br><span class="line">	.rollup(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;city&quot;</span>)</span><br><span class="line">	.sum(<span class="string">&quot;view_count&quot;</span>).alias(<span class="string">&quot;sum_view&quot;</span>)</span><br><span class="line">	.na.fill(<span class="string">&quot;None&quot;</span>)</span><br><span class="line">	.show()</span><br><span class="line"></span><br><span class="line">users</span><br><span class="line">	.cube(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;city&quot;</span>)</span><br><span class="line">	.sum(<span class="string">&quot;view_count&quot;</span>).alias(<span class="string">&quot;sum_view&quot;</span>)</span><br><span class="line">	.show()</span><br></pre></td></tr></table></div></figure>



        <h4 id="pivot-透视"   >
          <a href="#pivot-透视" class="heading-link"><i class="fas fa-link"></i></a>pivot 透视</h4>
      <p>pivot 透视，利用 stack 反透视。关于透视表，请移步维基百科透视表。可以运行下面的例子自己看下。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> example = <span class="type">Seq</span>(</span><br><span class="line">      (<span class="string">&quot;北京&quot;</span>, <span class="number">10000</span>, <span class="number">2015</span>),</span><br><span class="line">      (<span class="string">&quot;北京&quot;</span>, <span class="number">11000</span>, <span class="number">2016</span>),</span><br><span class="line">      (<span class="string">&quot;北京&quot;</span>, <span class="number">12000</span>, <span class="number">2017</span>),</span><br><span class="line">      (<span class="string">&quot;上海&quot;</span>, <span class="number">10300</span>, <span class="number">2015</span>),</span><br><span class="line">      (<span class="string">&quot;上海&quot;</span>, <span class="number">11700</span>, <span class="number">2016</span>)</span><br><span class="line">    ).toDF(<span class="string">&quot;city&quot;</span>, <span class="string">&quot;value&quot;</span>, <span class="string">&quot;year&quot;</span>)</span><br><span class="line"></span><br><span class="line">example.show()</span><br><span class="line"><span class="keyword">val</span> piv = example.groupBy(<span class="string">&quot;city&quot;</span>).pivot(<span class="string">&quot;year&quot;</span>).sum(<span class="string">&quot;value&quot;</span>)</span><br><span class="line">piv.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> unPiv = piv</span><br><span class="line">	.selectExpr(<span class="string">&quot;city&quot;</span>, <span class="string">&quot;stack(3, &#x27;2015&#x27;, `2015`, &#x27;2016&#x27;, `2016`, &#x27;2017&#x27;, `2017`) as (year, value)&quot;</span>)</span><br><span class="line">    .filter($<span class="string">&quot;value&quot;</span>.isNotNull)</span><br><span class="line">unPiv.show()</span><br></pre></td></tr></table></div></figure>

]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Apache Kafka实战-搭建zookeeper与kafka集群</title>
    <url>/posts/5f8acd6f/</url>
    <content><![CDATA[
        <h4 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h4>
      <p>直接 brew 安装的 zookeeper 与 kafka 最新版本。但是搭建 zookeeper 集群的时候，zkServer 一直启动失败。遂记录。</p>
<a id="more"></a>


        <h4 id="zookeeper"   >
          <a href="#zookeeper" class="heading-link"><i class="fas fa-link"></i></a>zookeeper</h4>
      <p>brew 一下即可，需要注意的是安装目录，zookeeper 默认安装在 <code>/usr/local/Cellar/zookeeper</code>，配置文件默认在 <code>/usr/local/etc/zookeeper</code>。</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">brew install zookeeper</span><br></pre></td></tr></table></div></figure>
<p>因为是搭建伪集群，所以进入到配置文件目录下，复制三份 zoo.cfg，分别命名为 zoo1.cfg zoo2.cfg zoo3.cfg，编写配置文件如下（配置文件大抵一致，只需要注意端口号，因为是本机只有一台机器），以 zoo1.cfg 为例</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">tickTime&#x3D;2000 # 心跳和超时的时间单位</span><br><span class="line">initLimit&#x3D;10 # follower 连接 leader 的超时时间，10 * tickTime</span><br><span class="line">syncLimit&#x3D;5 # follower 与 leader 同步的超时时间，5 * tickTime</span><br><span class="line">dataDir&#x3D;&#x2F;usr&#x2F;local&#x2F;var&#x2F;run&#x2F;zookeeper&#x2F;data&#x2F;zookeeper1  # 快照及 myid 保存目录</span><br><span class="line">clientPort&#x3D;2181 # 提供服务的端口</span><br><span class="line">server.1&#x3D;localhost:2888:3888 # 1 是 myid，2888 用于连接 leader，3888 用于选举 leader</span><br><span class="line">server.2&#x3D;localhost:2889:3889</span><br><span class="line">server.3&#x3D;localhost:2890:3890</span><br></pre></td></tr></table></div></figure>

<p>配置 myid，如果目录未创建的话需要先创建目录</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&quot;1&quot;</span> &gt; /usr/<span class="built_in">local</span>/var/run/zookeeper/data/zookeeper1/myid</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;2&quot;</span> &gt; /usr/<span class="built_in">local</span>/var/run/zookeeper/data/zookeeper2/myid</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;3&quot;</span> &gt; /usr/<span class="built_in">local</span>/var/run/zookeeper/data/zookeeper3/myid</span><br></pre></td></tr></table></div></figure>
<p>开始启动 zkServer，因为我设置了环境变量，可以直接使用 zkServer，不在赘述</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">zkServer start zoo1.cfg</span><br><span class="line">zkServer start zoo2.cfg</span><br><span class="line">zkServer start zoo3.cfg</span><br></pre></td></tr></table></div></figure>
<p>启动过程中遇到的问题，zkServer 一直报 failed start。查看日志，有以下错，关于日志目录的话，可以直接查看 log4j.properties</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="code"><pre><span class="line">java.lang.NoSuchMethodError: java.nio.ByteBuffer.clear()Ljava/nio/ByteBuffer;</span><br><span class="line">        at org.apache.jute.BinaryOutputArchive.stringToByteBuffer(BinaryOutputArchive.java:<span class="number">80</span>)</span><br><span class="line">        at org.apache.jute.BinaryOutputArchive.writeString(BinaryOutputArchive.java:<span class="number">110</span>)</span><br><span class="line">        at org.apache.zookeeper.data.Id.serialize(Id.java:<span class="number">51</span>)</span><br><span class="line">        at org.apache.jute.BinaryOutputArchive.writeRecord(BinaryOutputArchive.java:<span class="number">126</span>)</span><br><span class="line">        at org.apache.zookeeper.data.ACL.serialize(ACL.java:<span class="number">52</span>)</span><br><span class="line">        at org.apache.zookeeper.server.ReferenceCountedACLCache.serialize(ReferenceCountedACLCache.java:<span class="number">152</span>)</span><br><span class="line">        at org.apache.zookeeper.server.DataTree.serializeAcls(DataTree.java:<span class="number">1359</span>)</span><br><span class="line">        at org.apache.zookeeper.server.DataTree.serialize(DataTree.java:<span class="number">1372</span>)</span><br><span class="line">        at org.apache.zookeeper.server.util.SerializeUtils.serializeSnapshot(SerializeUtils.java:<span class="number">171</span>)</span><br><span class="line">        at org.apache.zookeeper.server.persistence.FileSnap.serialize(FileSnap.java:<span class="number">227</span>)</span><br><span class="line">        at org.apache.zookeeper.server.persistence.FileSnap.serialize(FileSnap.java:<span class="number">246</span>)</span><br><span class="line">        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.save(FileTxnSnapLog.java:<span class="number">472</span>)</span><br><span class="line">        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:<span class="number">291</span>)</span><br><span class="line">        at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:<span class="number">285</span>)</span><br><span class="line">        at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:<span class="number">1090</span>)</span><br><span class="line">        at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:<span class="number">1075</span>)</span><br><span class="line">        at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:<span class="number">227</span>)</span><br><span class="line">        at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:<span class="number">136</span>)</span><br><span class="line">        at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:<span class="number">90</span>)</span><br></pre></td></tr></table></div></figure>
<p>查了下错误，是新版本的 zookeeper java 版本不向下兼容的问题，要么升 java 版本，要么降低 zk 的版本。emmn，因为我本机本来就有两套 java，可以在当前终端下直接切换，遂切换到 jdk10，正常启动。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="comment"># java</span></span><br><span class="line"><span class="built_in">export</span> JAVA_8_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home</span><br><span class="line"><span class="built_in">export</span> JAVA_10_HOME=/Library/Java/JavaVirtualMachines/jdk-10.0.2.jdk/Contents/Home</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=<span class="variable">$JAVA_8_HOME</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=<span class="variable">$JAVA_HOME</span>/lib/tools.jar:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:.</span><br><span class="line"></span><br><span class="line"><span class="built_in">alias</span> jdk10=<span class="string">&quot;export JAVA_HOME=<span class="variable">$JAVA_10_HOME</span>&quot;</span></span><br><span class="line"><span class="built_in">alias</span> jdk8=<span class="string">&quot;export JAVA_HOME=<span class="variable">$JAVA_8_HOME</span>&quot;</span></span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">查看 zkServer 的状态</span><br><span class="line">```sh</span><br><span class="line">zkServer status zoo1.cfg</span><br><span class="line">zkServer status zoo2.cfg</span><br><span class="line">zkServer status zoo3.cfg</span><br></pre></td></tr></table></div></figure>


        <h4 id="kafka"   >
          <a href="#kafka" class="heading-link"><i class="fas fa-link"></i></a>kafka</h4>
      <p>brew 一下即可</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">brew install kafka</span><br></pre></td></tr></table></div></figure>

<p>kafka 伪集群的搭建比较简单，进入到配置文件目录 <code>/usr/local/Cellar/kafka/2.6.0/libexec/config</code>，复制出三份 server.properties。以 server1.properties 为例，需要改动的就是 broker.id，listiners，log.dirs</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">broker.id&#x3D;0</span><br><span class="line">delete.topic.enable&#x3D;true</span><br><span class="line">listiners&#x3D;PLAINTEXT:&#x2F;&#x2F;localhost:9092</span><br><span class="line">log.dirs&#x3D;&#x2F;data&#x2F;kafka1</span><br><span class="line">zookeeper.connect&#x3D;localhost:2181,localhost:2182,localhost:2183</span><br><span class="line">unclean.leader.election.enable&#x3D;false</span><br><span class="line">zookeeper.connection.timeout.ms&#x3D;6000</span><br></pre></td></tr></table></div></figure>
<p>启动 kafka</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">kafka-server-start -daemon server1.properties</span><br><span class="line">kafka-server-start -daemon server2.properties</span><br><span class="line">kafka-server-start -daemon server3.properties</span><br></pre></td></tr></table></div></figure>

<p>测试 topic 的创建，消息的生产与消费</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">kafka-topics --zookeeper localhost:2181 --create --topic test_topic --partitions 3 --replication-factor 3</span><br><span class="line">kafka-topice --zookeeper localhost:2181 --list</span><br><span class="line">kafka-topice --zookeeper localhost:2181 --describe --topic test_topic</span><br><span class="line">kafka-console-producer --broker-list localhost:9092 --topic test_topic</span><br><span class="line">kafka-console-consumer --bootstrap-server localhost:9092 --topic test_topic --from-beginning  <span class="comment"># 新版 consumer</span></span><br><span class="line">kafka-console-consumer --zookeeper localhost:9092 --topic test_topic --from-beginning <span class="comment"># 旧版 consumer</span></span><br></pre></td></tr></table></div></figure>]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>Apache Kafka实战-认识Apache Kafka</title>
    <url>/posts/694b3529/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>最近读完了 DDIA 前五章，回过头来在看 Apache Kafka 实战，很多东西都是不谋而合的，从中也可以看出来很多分布式处理框架在解决自身消息编码设计，复制分区故障转移的方法思维都是差不多的，只是根据各自主攻的场景选择更合适的解决方案。</p>
<ul>
<li>可靠性，可伸缩性，可维护性</li>
<li>数据模型与查询语言</li>
<li>存储与检索</li>
<li>编码与演化</li>
<li>复制与分区<a id="more"></a>

</li>
</ul>

        <h3 id="Kafka"   >
          <a href="#Kafka" class="heading-link"><i class="fas fa-link"></i></a>Kafka</h3>
      <p>Kafka 是一个消息流处理引擎并支持实时流处理。Kafka 将消息以 topic 为单位进行归纳，将向 Kafka topic 发送消息的程序称为 producer，将订阅 topic 消息的程序称为 consumer。Kafka 以集群的方式运行，由一个或者多个服务组成，每个服务被称作 broker。producer 通过网络向 kafka 集群发送消息，consumer 通过 poll 的方式向 kafka 集群订阅消息。<br>Kafka 并不只是单纯的消息队列，其实所有的分布式处理框架相对于传统的处理框架都有高可靠，高容错，易于伸缩的特性。Kafka 是怎么实现这些特性的呢？<br>Kafka 经常用作接收实时数据流，应用解耦合，流量削峰，如何保证 Kafak 集群的高效运行呢？<br>在数据处理过程中，我们往往作为数据下游消费者，如何编写一个高效的 consumer 呢？Kafka 与其他大数据处理框架（比如 Spark，Flink ）是怎么集成的呢？</p>

        <h4 id="消息引擎系统"   >
          <a href="#消息引擎系统" class="heading-link"><i class="fas fa-link"></i></a>消息引擎系统</h4>
      <p>消息引擎系统定义了一组规范，用以在不同系统间传递语义的消息。根据上述定义也可以看出来消息引擎的两个重要因素</p>
<ul>
<li>消息设计</li>
<li>传输协议设计<br>消息设计对应着 DDIA 中的编码与演化这章。计算机中数据的表现形式除了内存数据结构，就是字节序列。内存结构针对不同的场景更高效更快速，字节序列更多是为了存储压缩，还涉及到序列化与反序列化。常见的数据编码格式有 json，xml，csv，二进制字节序列。良好的消息设计对于系统不同组件间的网络传输速率及持久化过程中的磁盘IO，占用存储空间的大小影响很大。</li>
</ul>
<p>Kafka 的消息设计是什么样子的呢？其采用了二进制字节序列，主要包括头部信息（校验位，时间戳等），key，value。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/0081Kckwgy1glap9qp6nej30md042myj.jpg"></p>
<p>消息是静态的，需要双方之间传输才有意义，传输的方式叫作传输协议，可能我们听到过最多的就是 RPC。说的直白点，就是传递消息的规范（消息的接受，发送，失败重试，序列化，反序列化等），也可以与一些网络协议做对比。<br>Kakfa 自己设计了一套二进制传输协议。</p>

        <h4 id="Kakka-特性"   >
          <a href="#Kakka-特性" class="heading-link"><i class="fas fa-link"></i></a>Kakka 特性</h4>
      
        <h5 id="高吞吐，低延时"   >
          <a href="#高吞吐，低延时" class="heading-link"><i class="fas fa-link"></i></a>高吞吐，低延时</h5>
      <p>吞吐量代表每秒能够处理的消息数或者字节数，是衡量系统性能的一个重要指标。看这个定义，我们肯定是希望吞吐量越高越好，但是正常环境中，不得不考虑延时。延时与吞吐量又什么联系呢。作者这里举了一个比较形象的例子.</p>
<ul>
<li>Kafka 处理一条消息需要 2ms，那么计算得到的吞吐量不会超过 500 条消息/秒（1000/2）</li>
<li>由一条条发送消息改为批量发送消息，假设发送消息前先等待 8ms，在 8ms 的时间里累积了 1000 条的数据，此时的处理延时为 (8+2)ms，吞吐量为 100000 条/秒（1000/0.01）</li>
</ul>
<p>可以看到适当增加延时，采用微批处理，吞吐量提升了 200 倍。<br>Kafka 是如何做到高吞吐，低延时的呢？在数据持久化的过程中应该也用到了微批的思想。</p>
<ul>
<li>大量使用操作系统页缓存，内存操作速度快且命中率高。</li>
<li>Kafka 不直接参与物理 IO 操作，而是交由操作系统来完成。</li>
<li>采用追加写入方式，避免磁盘随机读写。ps: 这里是 DDIA 存储与检索章节的内容，追加写入方式的有点，如何检索，如何在其上建立高效的索引。LSM。</li>
<li>使用 sendfile 为代表的零拷贝技术加强网络间的数据传输效率，关于零拷贝，可以参考 <a href="https://timemachine.icu/posts/4f67b333/">zero_copy</a></li>
</ul>
<p>总结一下就是，在良好的消息设计前提下，高吞吐可以通过微批的方式进行提高（但是要结合机器性能等因素处理好延时问题）。低延时体现在网络传输和处理性能上（包括数据持久化），针对这个问题，目前大部分分布式框架都采用了零拷贝和 LSM。</p>

        <h5 id="消息持久化"   >
          <a href="#消息持久化" class="heading-link"><i class="fas fa-link"></i></a>消息持久化</h5>
      <p>为什么需要消息持久化？</p>
<ul>
<li>解耦合 -&gt; Kafka ，也可以说是消息队列比较重要的功能就是应用解耦合，分离生产者与消费者的业务代码逻辑。生产者只管生产消息到 Kafka 服务器持久化，而不用关心消费者何时消费数据。</li>
<li>可靠性 -&gt; 生产者数据的备份</li>
<li>重复消费 -&gt; 正如新闻一样，一条消息会由很多人看，持久化便于下游的各个消费者消费同一批数据，这个可以了解消息引擎范型中的生产/订阅模型。</li>
</ul>
<p>Kafka 是如何做到消息持久化的？追加写日志文件，不过方式有所不同，大多数消息持久化都是先放到内存缓冲区，达到阈值在溢写到磁盘。而 Kakfa 是直接写到磁盘，为什么这样做，这样做真的对整体性能有帮助吗？这个是后面的学习过程中需要思考与探讨的问题。</p>

        <h5 id="负载均衡和故障转移"   >
          <a href="#负载均衡和故障转移" class="heading-link"><i class="fas fa-link"></i></a>负载均衡和故障转移</h5>
      <p>与 DDIA 的第五章相对应，对于分布式数据系统，不得不考虑的问题是复制，是每台机器上都保持一模一样的数据呢，还是每台机器上保持不同的分区呢？是只有 leader 节点提供服务还是所有节点都提供服务？如果所有节点都提供服务，怎么保证每个节点上的数据一致性呢？当一台 leader 数据节点挂掉后，如何选举出来其他的节点保证服务呢？<br>Kafka 是每台节点上都保存一份数据，数据会进行分区，每个分区都有对应的 leader 和 replica。Kafka 通过某种算法选举每个分区的 leader，leader 较均匀的分布在每个节点上，由此实现负载均衡。<br>Kafka 依赖 zookeeper 实现故障转移，每个节点启动时都会向 zk 注册会话，当某个会话失效后，kafka 会自动选举出来其他服务器作为 leader 继续提供服务。</p>

        <h5 id="伸缩性"   >
          <a href="#伸缩性" class="heading-link"><i class="fas fa-link"></i></a>伸缩性</h5>
      <p>Kafka 适合水平伸缩，producer，consumer 与 broker 之间的状态全部由 zk 托管，当有新节点加入时，只需要修改配置并像 zk 注册就行。</p>

        <h4 id="Kafka-基本概念"   >
          <a href="#Kafka-基本概念" class="heading-link"><i class="fas fa-link"></i></a>Kafka 基本概念</h4>
      
        <h5 id="topic-和-partition"   >
          <a href="#topic-和-partition" class="heading-link"><i class="fas fa-link"></i></a>topic 和 partition</h5>
      <p>topic 代指一类消息，由多个 partition 组成。partition 对消息分区。举个例子，商品订单数据可以看做 topic，日常用品订单，娱乐消费订单等可以看做 partition。分区的作用是为了便于查找。如果只有一个分区，我们为了找到娱乐消费订单数据可能还会轮询到很多其他类订单数据。</p>

        <h5 id="offset"   >
          <a href="#offset" class="heading-link"><i class="fas fa-link"></i></a>offset</h5>
      <p>每条消息的位移。在 producer 和 consumer 端有不同的含义。</p>
<ul>
<li>producer 端标识每个消息的位置</li>
<li>consumer 端标识消费者读到了什么位置</li>
</ul>
<p>&lt;topic, partition, offset&gt; 可以标识一条数据</p>

        <h5 id="replica"   >
          <a href="#replica" class="heading-link"><i class="fas fa-link"></i></a>replica</h5>
      <p>副本，冗余存储，保证高可靠。每个 replica 都有一个 leader 和若干个 follower，follower 会主动追随 leader 上的数据，如果该 follower 在 ISR 中，其还可以作为备用 leader。replica 肯定是分布在不同 broker 上的，否则无法实现备份冗余的效果。</p>

        <h5 id="leader-和-follower"   >
          <a href="#leader-和-follower" class="heading-link"><i class="fas fa-link"></i></a>leader 和 follower</h5>
      <p>replica 中涉及到的两个角色概念，只有 leader 对外提供服务，follower 只是被动同步 leader 并作为故障转移后的备选 leader。</p>

        <h5 id="ISR"   >
          <a href="#ISR" class="heading-link"><i class="fas fa-link"></i></a>ISR</h5>
      <p>in-sync replica。Kafka 为 partition 动态维护了一个 replica 集合，该集合中所有 replica 保存的消息日志都与 leader replica 保持同步状态，只有这个集合中的 replica 才能被选举成 leader，也只有该集合中的所有 replica 都接收到消息后，Kafka 才会将该条消息置为已提交状态。这个集合被称作 ISR。Kafka 只保证已提交状态消息不会丢失。<br>正常情况下，partition 下的所有 replica 都在 ISR 中。但在实际生产环境中，由于各种原因，一些 replica 跟不上 leader 就会被踢出 ISR。追上进度后，replica 还会被重新加入到 ISR。</p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>Apache Kafka实战-配置篇（WIP）</title>
    <url>/posts/dd1316da/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>kafka 与 zookeeper 的一些重要配置参数</p>
<a id="more"></a>]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>zookeeper</tag>
        <tag>WIP</tag>
      </tags>
  </entry>
  <entry>
    <title>Application UI - Monitoring and Instrumentation</title>
    <url>/posts/c0388aaa/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>Spark Application UI 可以帮助我们直观的了解 Spark 的执行过程，作业占用的 CPU，内存资源的多少，GC耗费的时间，Stage 是如何划分的，每个 Task 的执行时间等。像如果 Spark 作业执行时间过长，我们就可以去 UI 上查看到底是哪个 Task 执行占用时间最长，这个 Task 的操作是什么，进而找到问题并解决优化。</p>
<a id="more"></a>

        <h3 id="Application-UI"   >
          <a href="#Application-UI" class="heading-link"><i class="fas fa-link"></i></a>Application UI</h3>
      
        <h4 id="Jobs-Tab"   >
          <a href="#Jobs-Tab" class="heading-link"><i class="fas fa-link"></i></a>Jobs Tab</h4>
      <p>Jobs 页面为我们提供了 Spark 应用程序中所有的 Job 摘要及其详情页。摘要主要展示一些高维度的信息，比如 Job 的状态，运行时长，当前进度，整体作业执行的时间线（event timeline）。你可以点击相应的 Job 查看该 Job 的更详细的信息，比如 DAG 视图及该 Job 划分出的 Stage。<br>本节中显示的信息是</p>
<ul>
<li><p>User： 当前 Spark 作业的用户</p>
</li>
<li><p>Total uptime：Spark 作业目前执行的总时间</p>
</li>
<li><p>Scheduling mode：作业调度模式，比如 FIFO</p>
</li>
<li><p>Number of jobs per status：Job 的执行情况，Active， Completed， Failed</p>
<img src="https://spark.apache.org/docs/3.0.0-preview/img/AllJobsPageDetail1.png" width="40%" height="40%">
</li>
<li><p>Event timeline：按时间顺序展示 executors 和 job 的 事件（及动作，比如 executor 的添加删除，一些算子的触发）<br><img src="https://spark.apache.org/docs/3.0.0-preview/img/AllJobsPageDetail2.png"></p>
</li>
<li><p>Details of jobs grouped by status: 以表格形式展现每个 Job 的详细信息，比如 Job ID，描述信息，提交时间，运行时长，Stage 的划分，包含的 tasks 的执行进度条。<br><img src="https://spark.apache.org/docs/3.0.0-preview/img/AllJobsPageDetail3.png"></p>
</li>
</ul>
<p>在 Job 的描述信息中，可以点击到 Job 的详情页。</p>

        <h5 id="Jobs-detail"   >
          <a href="#Jobs-detail" class="heading-link"><i class="fas fa-link"></i></a>Jobs detail</h5>
      <p>Job 详情页提供了特定 Job （由 Job Id 标识）更完善的信息</p>
<ul>
<li><p>Job Status: (running, succeeded, failed)</p>
</li>
<li><p>每种状态的 Stage 数量 (active, pending, completed, skipped, failed)</p>
</li>
<li><p>关联的 SQL 查询</p>
</li>
<li><p>Event timeline：按时间顺序展示 executors 和 job 的 事件（及动作，比如 executor 的添加删除，一些算子的触发）<br><img src="https://spark.apache.org/docs/3.0.0-preview/img/JobPageDetail1.png"></p>
</li>
<li><p>DAG 视图：该 Job 的 DAG 视图，顶点代表 RDD 或者 DF，边线代表应用在上面的操作。像里面的一些 WholeStageCodegen，其到底是啥嘞。<br><img src="https://spark.apache.org/docs/3.0.0-preview/img/JobPageDetail2.png"></p>
</li>
<li><p>各个状态的 Stage 列表</p>
<ul>
<li>Stage ID</li>
<li>Stage 的描述信息</li>
<li>提交时间</li>
<li>Stage 的运行时长</li>
<li>该 Stage 包含的 Task 的运行时长</li>
<li>Input，该 Stage 阶段 从 executor storage 中读取的字节数</li>
<li>Output， 该 Stage 阶段 写 executor storage 的字节数</li>
<li>shuffle read， shuffle read 阶段的字节和记录数，包含远端 executor 的读取</li>
<li>shuffle write，shuffle write 阶段写磁盘的字节和记录数，目的是为了下一个 Stage 阶段开始的 shuffle read<br><img src="https://spark.apache.org/docs/3.0.0-preview/img/JobPageDetail3.png"></li>
</ul>
</li>
</ul>

        <h4 id="Stages-Tab"   >
          <a href="#Stages-Tab" class="heading-link"><i class="fas fa-link"></i></a>Stages Tab</h4>
      <p>该页面显示所有 Job 中的 Stage 信息摘要。和 Job Tab 一样，一开始是各个状态的 Stage 的汇总。<br>在公平调度模式下，页面上还有一个 pools properties 的显示，这个是干啥的嘞？需要后面了解下<br><img src="https://spark.apache.org/docs/3.0.0-preview/img/AllStagesPageDetail2.png"></p>
<p>下面就是各个状态的 Stage 的详细信息，和 Job 一样，唯一标识ID，描述信息，提交时间，运行时长，Tasks 的执行进度条，点击描述信息中链接可以查看该 Stage 中 Task 的详细信息。<br><img src="https://spark.apache.org/docs/3.0.0-preview/img/AllStagesPageDetail3.png"></p>

        <h5 id="Stage-detail"   >
          <a href="#Stage-detail" class="heading-link"><i class="fas fa-link"></i></a>Stage detail</h5>
      <p>页面最开始显示的摘要信息，该 Stage 的 DAG 视图</p>
<ul>
<li>所有 Task 执行的时间</li>
<li>本地化级别</li>
<li>Shuffle Read</li>
<li>关联的 Job，这个 Stage 是哪个 Job 划分出来的 <br><img src="https://spark.apache.org/docs/3.0.0-preview/img/AllStagesPageDetail4.png"></li>
</ul>
<p>下面是该 Stage 中 Task 运行的一些指标</p>
<ul>
<li>Tasks deserialization time，Task 在反序列化上耗费的时间，如果该部分耗时较长，则可以选择更高效的序列化方法，比如 kyro</li>
<li>Duration of tasks，Tasks 运行的时长</li>
<li>GC time，GC 时间，如果耗费时间比例比较大或者发现频繁GC，可以适当调高 executor memory 的比例。</li>
<li>Getting result time，Worker 获取到 Tasks 的结果集的时间</li>
<li>Result serialization time，Task 计算出的结果集序列化耗费的时间</li>
<li>Scheduler delay，等待调度执行的时间</li>
<li>Peak execution memory，shuffle，聚合等阶段生成的数据对象占用的最大内存</li>
<li>Shuffle Read Size / Records</li>
<li>Shuffle Read Blocked Time，task 等待远端 executor 读取数据的时间</li>
<li>Shuffle Remote Reads，从远端读取的数据字节，这里和上面几个属性，可以去了解下 RDD 的最佳计算结点位置，及本地化级别</li>
<li>Shuffle spill (memory), 反序列化 Shuffle Data，shuffle read</li>
<li>Shuffle spill (disk)，序列化数据数据到磁盘，shuffle write</li>
</ul>
<p><img src="https://spark.apache.org/docs/3.0.0-preview/img/AllStagesPageDetail6.png"></p>
<p>最下面展现了 Task 列表，emnn，里面的信息应该一目了然了。<br><img src="https://spark.apache.org/docs/3.0.0-preview/img/AllStagesPageDetail9.png"></p>

        <h3 id="Storage-Tab"   >
          <a href="#Storage-Tab" class="heading-link"><i class="fas fa-link"></i></a>Storage Tab</h3>
      <p>在 Storage 界面可以看到持久化的 RDD 信息。spark-shell 中执行以下代码</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.storage.<span class="type">StorageLevel</span>._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.<span class="type">StorageLevel</span>._</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.range(<span class="number">0</span>, <span class="number">100</span>, <span class="number">1</span>, <span class="number">5</span>).setName(<span class="string">&quot;rdd&quot;</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Long</span>] = rdd <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at range at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.persist(<span class="type">MEMORY_ONLY_SER</span>)</span><br><span class="line">res0: rdd.<span class="keyword">type</span> = rdd MapPartitionsRDD[<span class="number">1</span>] at range at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.count</span><br><span class="line">res1: <span class="type">Long</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = <span class="type">Seq</span>((<span class="number">1</span>, <span class="string">&quot;andy&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;bob&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;andy&quot;</span>)).toDF(<span class="string">&quot;count&quot;</span>, <span class="string">&quot;name&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [count: int, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; df.persist(<span class="type">DISK_ONLY</span>)</span><br><span class="line">res2: df.<span class="keyword">type</span> = [count: int, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; df.count</span><br><span class="line">res3: <span class="type">Long</span> = <span class="number">3</span></span><br></pre></td></tr></table></div></figure>
<p>现在在页面上就可以看到持久化的 RDD 和 DataFrame。<br><img src="https://spark.apache.org/docs/3.0.0-preview/img/webui-storage-tab.png"></p>
<p>点击 RDD Name 可以看到缓存的 RDD 的详细信息，比如分区数， 每个分区下 block 的持久化级别和大小。<br><img src="https://spark.apache.org/docs/3.0.0-preview/img/webui-storage-detail.png"></p>

        <h3 id="Environment-Tab"   >
          <a href="#Environment-Tab" class="heading-link"><i class="fas fa-link"></i></a>Environment Tab</h3>
      <p>该页面显示一些环境变量和配置值，包括 JVM，Spark，和一些系统属性。相对重要的是 Spark Properties，在这里可以看我们的配置是否生效。<br><img src="https://spark.apache.org/docs/3.0.0-preview/img/webui-env-tab.png"></p>

        <h3 id="Executors-Tab"   >
          <a href="#Executors-Tab" class="heading-link"><i class="fas fa-link"></i></a>Executors Tab</h3>
      <p>Executors Tab，名如其意，会显示该 Spark Appliaction 关联的 Executor 的信息。经常关注的是 executor 的资源信息，比如内存，CPU core，GC 时间，shuflle 阶段数据的大小。当然也可以直接查看日志（页面上有 stderr，stdout）。<br><img src="https://spark.apache.org/docs/3.0.0-preview/img/webui-exe-tab.png"></p>

        <h3 id="SQL-Tab"   >
          <a href="#SQL-Tab" class="heading-link"><i class="fas fa-link"></i></a>SQL Tab</h3>
      <p>如果你的应用程序执行的是 sql 而非 Spark DSL。该页面会展示 Spark sql 执行相关的信息。，比如该 sql 执行各阶段的提交时间，运行时长，关联的 Job，查询的逻辑计划和物理计划。下面我们运行一个例子来说明 SQL Tab。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = <span class="type">Seq</span>((<span class="number">1</span>, <span class="string">&quot;andy&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;bob&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;andy&quot;</span>)).toDF(<span class="string">&quot;count&quot;</span>, <span class="string">&quot;name&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [count: int, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; df.count</span><br><span class="line">res0: <span class="type">Long</span> = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">scala&gt; df.createGlobalTempView(<span class="string">&quot;df&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;select name,sum(count) from global_temp.df group by name&quot;</span>).show</span><br><span class="line">+----+----------+</span><br><span class="line">|name|sum(count)|</span><br><span class="line">+----+----------+</span><br><span class="line">|andy| <span class="number">3</span>|</span><br><span class="line">| bob| <span class="number">2</span>|</span><br><span class="line">+----+----------+</span><br></pre></td></tr></table></div></figure>
<p><img src="https://spark.apache.org/docs/3.0.0-preview/img/webui-sql-tab.png"></p>
<p>上图中就展现了运行了 sql 的操作列表。点击 Description 描述信息可以看到该 sql query 的 DAG 图。<br><img src="https://spark.apache.org/docs/3.0.0-preview/img/webui-sql-dag.png"></p>
<p>点击 details，可以看到该 sql query 的执行计划。<br><img src="https://spark.apache.org/docs/3.0.0-preview/img/webui-sql-plan.png"></p>
<p>在 sql query 生成的 DAG 图中有 stage 阶段中更为详尽的信息，比如上面的 shuffle records written，records read，number of output rows等。</p>

        <h3 id="Streaming-Tab"   >
          <a href="#Streaming-Tab" class="heading-link"><i class="fas fa-link"></i></a>Streaming Tab</h3>
      <p>关于Spark Streaming 应用程序的页面，主要展示数据流中每一个 micro-batch 的调度延迟和处理时间，方便查找问题</p>

        <h3 id="JDBC-ODBC-Server-Tab"   >
          <a href="#JDBC-ODBC-Server-Tab" class="heading-link"><i class="fas fa-link"></i></a>JDBC/ODBC Server Tab</h3>
      <p>Spark SQL 还可以使用其JDBC / ODBC或命令行界面充当分布式查询引擎。在这种模式下，最终用户或应用程序可以直接与 Spark SQL 交互并运行 SQL 查询，而无需编写任何代码。<br>该页面主要用来展示 Spark 直接充当分布式查询引擎时的信息。<br><img src="https://spark.apache.org/docs/3.0.0-preview/img/JDBCServer3.png"></p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Azkaban工作流的编写</title>
    <url>/posts/f428d959/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>本篇主要讲解如何使用 AzKaban Flow 2.0 来编写工作流。Flow 1.0 在以后的版本中会逐步移除。</p>
<a id="more"></a>



        <h3 id="工作流"   >
          <a href="#工作流" class="heading-link"><i class="fas fa-link"></i></a>工作流</h3>
      
        <h4 id="基础工作流"   >
          <a href="#基础工作流" class="heading-link"><i class="fas fa-link"></i></a>基础工作流</h4>
      <ul>
<li><p>建立 flow 2.0 标识文件 flow20.project（文件名字一定要是这个），并输入以下内容并保存文件</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">azkaban-flow-version: 2.0</span><br></pre></td></tr></table></div></figure>
</li>
<li><p>建立 .flow 工作流文件，就是常见的 yml 格式。nodes 作为根节点，下面定义该工作流下的 job 和子工作流。type 代表 job 的类型，config 代表其配置。</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">	<span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobA</span></span><br><span class="line">	  <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">	  <span class="attr">config:</span></span><br><span class="line">	  	<span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;simple basic flow&quot;</span></span><br></pre></td></tr></table></div></figure>
</li>
<li><p>zip 压缩上述两个文件，然后上传压缩包到 azkaban project 下，就可以看到此工作流，可以手动执行 Execute Flow。</p>
</li>
</ul>

        <h4 id="定义-job-间的依赖关系"   >
          <a href="#定义-job-间的依赖关系" class="heading-link"><i class="fas fa-link"></i></a>定义 job 间的依赖关系</h4>
      <figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">	<span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobA</span></span><br><span class="line">	  <span class="attr">type:</span> <span class="string">noop</span></span><br><span class="line">	  <span class="attr">config:</span></span><br><span class="line">	  	<span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;this is jobA&quot;</span></span><br><span class="line">	  <span class="attr">dependsOn:</span></span><br><span class="line">	  	<span class="bullet">-</span> <span class="string">jobB</span></span><br><span class="line">	  	<span class="bullet">-</span> <span class="string">jobC</span></span><br><span class="line">	<span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobB</span></span><br><span class="line">	  <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">	  <span class="attr">config:</span></span><br><span class="line">	  	<span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;this is jobB&quot;</span></span><br><span class="line">	<span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobC</span></span><br><span class="line">	  <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">	  <span class="attr">config:</span></span><br><span class="line">	  	<span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;this is jobC&quot;</span>  		</span><br></pre></td></tr></table></div></figure>


        <h4 id="定义子工作流"   >
          <a href="#定义子工作流" class="heading-link"><i class="fas fa-link"></i></a>定义子工作流</h4>
      <p>工作流中也可以嵌套子工作流，</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">	<span class="bullet">-</span> <span class="attr">name:</span> <span class="string">child_flow</span></span><br><span class="line">	  <span class="attr">type:</span> <span class="string">flow</span></span><br><span class="line">	  <span class="attr">config:</span></span><br><span class="line">	  	<span class="attr">key:</span> <span class="string">value</span></span><br><span class="line">	  <span class="attr">nodes:</span></span><br><span class="line">	  	<span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobA</span></span><br><span class="line">	  	  <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">	  	  <span class="attr">config:</span></span><br><span class="line">	  	  	<span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;this is a chile flow job&quot;</span></span><br><span class="line">	<span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobB</span></span><br><span class="line">	  <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">	  <span class="attr">config:</span></span><br><span class="line">	   	<span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;this is jobB&quot;</span>	  	  	</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>


        <h4 id="job-也可以依赖其他工作流"   >
          <a href="#job-也可以依赖其他工作流" class="heading-link"><i class="fas fa-link"></i></a>job 也可以依赖其他工作流</h4>
      <p>比如下面的 jobB 依赖于子工作流 child_flow</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">	<span class="bullet">-</span> <span class="attr">name:</span> <span class="string">child_flow</span></span><br><span class="line">	  <span class="attr">type:</span> <span class="string">flow</span></span><br><span class="line">	  <span class="attr">config:</span></span><br><span class="line">	  	<span class="attr">key:</span> <span class="string">value</span></span><br><span class="line">	  <span class="attr">nodes:</span></span><br><span class="line">	  	<span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobA</span></span><br><span class="line">	  	  <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">	  	  <span class="attr">config:</span></span><br><span class="line">	  	  	<span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;this is a chile flow job&quot;</span></span><br><span class="line">	<span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobB</span></span><br><span class="line">	  <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">	  <span class="attr">config:</span></span><br><span class="line">	   	<span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;this is jobB&quot;</span></span><br><span class="line">	  <span class="attr">dependsOn:</span> </span><br><span class="line">	  	<span class="bullet">-</span>  <span class="string">child_flow</span>	</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>



        <h4 id="条件工作流"   >
          <a href="#条件工作流" class="heading-link"><i class="fas fa-link"></i></a>条件工作流</h4>
      <p>只有当满足某些条件时，才会触发目标 job。Azkaban 提供了一些预定义宏作为条件</p>
<ul>
<li>all_done    对应的作业状态 FAILED, KILLED, SUCCEEDED, SKIPPED, FAILED_SUCCEEDED, CANCELLED</li>
<li>all_success \ one_success    对应的作业状态 SUCCEEDED, SKIPPED, FAILED_SUCCEEDED</li>
<li>all_failed \ one_failed    对应的作业状态 FAILED, KILLED, CANCELLED</li>
</ul>
<p>下面是一些条件举例<br><code>all_success || $&#123;file_ready:file1&#125; == &quot;ready&quot;</code><br><code>all_success || $&#123;file_ready:file1&#125; == &quot;ready&quot; &amp;&amp; $&#123;job:param1&#125; == 1 </code></p>
<p>下面是一个条件工作流的例子，只有当文件准备好之后，才会执行 start_import。关于 <code>$JOB_OUTPUT_PROP_FILE</code> 是一个运行时环境变量，上游 job 产生的条件信息会写入到该临时文件，并传递到下游 job。下游 job 通过  <code>$&#123;上游job名称:变量名&#125;</code> 读取。不过需要注意，参数只能在同一个 flow 的上下游 job 间传递，而不能跨 flow。</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">file_ready</span></span><br><span class="line">   <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">   <span class="attr">config:</span></span><br><span class="line">     <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&#x27;&#123;&quot;file1&quot;:&quot;ready&quot;&#125;&#x27;</span> <span class="string">&gt;</span> <span class="string">$JOB_OUTPUT_PROP_FILE</span></span><br><span class="line"></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">start_import</span></span><br><span class="line">   <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">   <span class="attr">dependsOn:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">file_ready</span></span><br><span class="line">   <span class="attr">config:</span></span><br><span class="line">     <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;start import&quot;</span></span><br><span class="line">   <span class="attr">condition:</span> <span class="string">$&#123;file_ready:file1&#125;</span> <span class="string">==</span> <span class="string">&quot;ready&quot;</span></span><br></pre></td></tr></table></div></figure>

]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Azkaban</tag>
      </tags>
  </entry>
  <entry>
    <title>Azkaban的搭建与配置</title>
    <url>/posts/79bc37bf/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>AzKaban 是一个任务流调度器，可以组织作业及工作流之间的依赖关系，使得任务按照我们所想的方式有序执行。并且可以轻便的实现报警监控。本文主要讲解如何以 mutible executor mode 部署 AzKaban，并提交简单的工作流做测试使用。</p>
<a id="more"></a>


        <h3 id="AzKaban-部署"   >
          <a href="#AzKaban-部署" class="heading-link"><i class="fas fa-link"></i></a>AzKaban 部署</h3>
      <p>AzKaban 主要有两部分组成，AzKabanWebServer 和 AzKabanExecutor。 顾名思义，webserver 主要用来接收请求，UI展示，executor 用于真实的任务执行，任务执行的状态，日志等信息依赖 Mysql 存储。<br>AzKaban 的部署和其他的分布式系统一样，下载安装包，配置好一台机器后，进行配置分发，然后先启动所有的 executor，在启动 webserver。</p>

        <h3 id="使用-docker-构建"   >
          <a href="#使用-docker-构建" class="heading-link"><i class="fas fa-link"></i></a>使用 docker 构建</h3>
      <p>这里不多说了，可以参考我的 github <span class="exturl"><a class="exturl__link"   href="https://github.com/Flyraty/docker_bigdata/tree/master/docker_azkaban" >docker_azkaban</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Azkaban</tag>
      </tags>
  </entry>
  <entry>
    <title>Build In Functions</title>
    <url>/posts/4ac75a46/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>需要处理的数据结构往往是复杂的，在 Spark 中该如何操作 Map，Array，struct 这些结构呢？Spark 已经为我们提供了很多内置函数来处理这一切。这些函数大多定义在 org.apache.saprk.sql.functions。</p>
<a id="more"></a>


        <h3 id="日期处理"   >
          <a href="#日期处理" class="heading-link"><i class="fas fa-link"></i></a>日期处理</h3>
      
        <h4 id="获取当前-date，timestamp"   >
          <a href="#获取当前-date，timestamp" class="heading-link"><i class="fas fa-link"></i></a>获取当前 date，timestamp</h4>
      <p>current_date，current_timestamp，包括下面说的处理函数可以直接在 sql 语句中使用，spark.sql(“Your  SQL”)</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; spark.emptyDataFrame.withColumn(<span class="string">&quot;current_date&quot;</span>, current_date).schema</span><br><span class="line">res3: org.apache.spark.sql.types.<span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(current_date,<span class="type">DateType</span>,<span class="literal">false</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; spark.range(<span class="number">1</span>).select(current_date).show()</span><br><span class="line">+--------------+</span><br><span class="line">|current_date()|</span><br><span class="line">+--------------+</span><br><span class="line">|    <span class="number">2020</span><span class="number">-03</span><span class="number">-17</span>|</span><br><span class="line">+--------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; spark.range(<span class="number">1</span>).select(current_timestamp).show()</span><br><span class="line">+--------------------+</span><br><span class="line">| current_timestamp()|</span><br><span class="line">+--------------------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-03</span><span class="number">-17</span> <span class="number">10</span>:<span class="number">37</span>:...|</span><br><span class="line">+--------------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; spark.range(<span class="number">1</span>).select(current_timestamp).show(<span class="literal">false</span>)</span><br><span class="line">+-----------------------+</span><br><span class="line">|current_timestamp()    |</span><br><span class="line">+-----------------------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-03</span><span class="number">-17</span> <span class="number">10</span>:<span class="number">39</span>:<span class="number">53.896</span>|</span><br><span class="line">+-----------------------+</span><br></pre></td></tr></table></div></figure>


        <h4 id="column-转换到-date，timestamp，时间戳"   >
          <a href="#column-转换到-date，timestamp，时间戳" class="heading-link"><i class="fas fa-link"></i></a>column 转换到 date，timestamp，时间戳</h4>
      <p>to_date，to_timestamp 接受 date，timestamp ，string 类型的参数。如果参数为 string，指定可选的 format，转换失败不会报错，会返回 null。默认的 format 是 yyyy-MM-dd HH:mm:ss。</p>
<p>unix_timestamp 返回当前时间戳或者根据指定字段生成时间戳，这个经常会用到。</p>
<p>from_unixtime 转换时间戳到 timestamp</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; spark.range(<span class="number">1</span>).select(current_date).withColumn(<span class="string">&quot;to_timestamp&quot;</span>, to_timestamp($<span class="string">&quot;current_date()&quot;</span>, <span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>)).show()</span><br><span class="line">+--------------+-------------------+</span><br><span class="line">|current_date()|       to_timestamp|</span><br><span class="line">+--------------+-------------------+</span><br><span class="line">|    <span class="number">2020</span><span class="number">-03</span><span class="number">-17</span>|<span class="number">2020</span><span class="number">-03</span><span class="number">-17</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>|</span><br><span class="line">+--------------+-------------------+</span><br><span class="line"></span><br><span class="line">scala&gt; spark.range(<span class="number">1</span>).select(current_timestamp).withColumn(<span class="string">&quot;to_date&quot;</span>, to_date($<span class="string">&quot;current_timestamp()&quot;</span>)).show()</span><br><span class="line">+--------------------+----------+</span><br><span class="line">| current_timestamp()|   to_date|</span><br><span class="line">+--------------------+----------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-03</span><span class="number">-17</span> <span class="number">10</span>:<span class="number">43</span>:...|<span class="number">2020</span><span class="number">-03</span><span class="number">-17</span>|</span><br><span class="line">+--------------------+----------+</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">Seq</span>(<span class="string">&quot;2020-03-17 00:00:00&quot;</span>).toDF(<span class="string">&quot;time&quot;</span>).withColumn(<span class="string">&quot;unix_timestamp&quot;</span>, unix_timestamp($<span class="string">&quot;time&quot;</span>)).show()</span><br><span class="line">+-------------------+--------------+</span><br><span class="line">|               time|unix_timestamp|</span><br><span class="line">+-------------------+--------------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-03</span><span class="number">-17</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>|    <span class="number">1584374400</span>|</span><br><span class="line">+-------------------+--------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; spark.range(<span class="number">1</span>).select(unix_timestamp).show()</span><br><span class="line">+--------------------------------------------------------+</span><br><span class="line">|unix_timestamp(current_timestamp(), yyyy-<span class="type">MM</span>-dd <span class="type">HH</span>:mm:ss)|</span><br><span class="line">+--------------------------------------------------------+</span><br><span class="line">|                                              <span class="number">1584413996</span>|</span><br><span class="line">+--------------------------------------------------------+</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;select current_date as current_date&quot;</span>).show()</span><br><span class="line">+------------+</span><br><span class="line">|current_date|</span><br><span class="line">+------------+</span><br><span class="line">|  <span class="number">2020</span><span class="number">-03</span><span class="number">-17</span>|</span><br><span class="line">+------------+</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">Seq</span>(<span class="number">1582720143</span>).toDF(<span class="string">&quot;time&quot;</span>).withColumn(<span class="string">&quot;from_unixtime&quot;</span>, from_unixtime($<span class="string">&quot;time&quot;</span>)).show()</span><br><span class="line">+----------+-------------------+</span><br><span class="line">|      time|      from_unixtime|</span><br><span class="line">+----------+-------------------+</span><br><span class="line">|<span class="number">1582720143</span>|<span class="number">2020</span><span class="number">-02</span><span class="number">-26</span> <span class="number">20</span>:<span class="number">29</span>:<span class="number">03</span>|</span><br><span class="line">+----------+-------------------+</span><br></pre></td></tr></table></div></figure>


        <h4 id="格式化日期"   >
          <a href="#格式化日期" class="heading-link"><i class="fas fa-link"></i></a>格式化日期</h4>
      <p>date_format 用来格式化日期</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="type">Seq</span>(<span class="string">&quot;2020-03-17 00:00:00&quot;</span>).toDF(<span class="string">&quot;time&quot;</span>).withColumn(<span class="string">&quot;date_foramt&quot;</span>, date_format($<span class="string">&quot;time&quot;</span>, <span class="string">&quot;yyyy/MM/dd hh:mm:ss&quot;</span>)).show()</span><br><span class="line">+-------------------+-------------------+</span><br><span class="line">|               time|        date_foramt|</span><br><span class="line">+-------------------+-------------------+</span><br><span class="line">|<span class="number">2020</span><span class="number">-03</span><span class="number">-17</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>|<span class="number">2020</span>/<span class="number">03</span>/<span class="number">17</span> <span class="number">12</span>:<span class="number">00</span>:<span class="number">00</span>|</span><br><span class="line">+-------------------+-------------------+</span><br></pre></td></tr></table></div></figure>

        <h3 id="复杂数据结构处理"   >
          <a href="#复杂数据结构处理" class="heading-link"><i class="fas fa-link"></i></a>复杂数据结构处理</h3>
      <p>就跟我们平常使用这些数据结构一样，取元素，追加元素，求长度，根据里面的数据做处理，修改数组，记录状态等。 org.apache.spark.sql.functions 中 @group 为 collection_funcs 的函数就是用来实现这些功能的。下面只介绍下经常遇到和使用的函数，更多的可以去看源码。</p>

        <h4 id="处理-Array"   >
          <a href="#处理-Array" class="heading-link"><i class="fas fa-link"></i></a>处理 Array</h4>
      <div class="table-container"><table>
<thead>
<tr>
<th>Method</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>array_contains</td>
<td>某个值是否在 array 中。</td>
</tr>
<tr>
<td>array_join</td>
<td>类似于 Python 中的 “,”.join。指定分隔符和可选的空值替换符连接 array 中的值。</td>
</tr>
<tr>
<td>element_at</td>
<td>按索引取值，索引从 1 开始。</td>
</tr>
<tr>
<td>explode</td>
<td>平铺数组中内容到多行</td>
</tr>
<tr>
<td>reverse</td>
<td>反转数组</td>
</tr>
<tr>
<td>array_zip</td>
<td>合并数组到结构体，array1 array2 →  array<struct></td>
</tr>
</tbody></table></div>
<p>这里 explode 比较重要，像一个用户的多个事件被存在了数组里面，就可以使用 explode 提取出来，变成一行一个事件处理。explode 变多行，那么我想把数组里面的值依次取出来变成多列呢？</p>
<p>这是一个问题，你可以去了解下 transform，想想如何实现。</p>
<p>再有一些比较复杂的操作，比如我们想通过数组前面的均值来计算填充 null 值，这时候你就需要用 UDF 自己写处理逻辑来实现了。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> array = <span class="type">Seq</span>(<span class="string">&quot;1 2 3 4 5&quot;</span>).toDF(<span class="string">&quot;id_string&quot;</span>).withColumn(<span class="string">&quot;id_array&quot;</span>, split($<span class="string">&quot;id_string&quot;</span>, <span class="string">&quot; &quot;</span>))</span><br><span class="line">array: org.apache.spark.sql.<span class="type">DataFrame</span> = [id_string: string, id_array: array&lt;string&gt;]</span><br><span class="line"></span><br><span class="line">scala&gt; array.show()</span><br><span class="line">+---------+---------------+</span><br><span class="line">|id_string|       id_array|</span><br><span class="line">+---------+---------------+</span><br><span class="line">|<span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span>|[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]|</span><br><span class="line">+---------+---------------+</span><br><span class="line"></span><br><span class="line">scala&gt; array.withColumn(<span class="string">&quot;head&quot;</span>, element_at($<span class="string">&quot;id_array&quot;</span>, <span class="number">1</span>)).show()</span><br><span class="line">+---------+---------------+----+</span><br><span class="line">|id_string|       id_array|head|</span><br><span class="line">+---------+---------------+----+</span><br><span class="line">|<span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span>|[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]|   <span class="number">1</span>|</span><br><span class="line">+---------+---------------+----+</span><br><span class="line"></span><br><span class="line">scala&gt; array.withColumn(<span class="string">&quot;array_contains&quot;</span>, array_contains($<span class="string">&quot;id_array&quot;</span>, <span class="string">&quot;1&quot;</span>)).show()</span><br><span class="line">+---------+---------------+--------------+</span><br><span class="line">|id_string|       id_array|array_contains|</span><br><span class="line">+---------+---------------+--------------+</span><br><span class="line">|<span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span>|[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]|          <span class="literal">true</span>|</span><br><span class="line">+---------+---------------+--------------+</span><br><span class="line"></span><br><span class="line">scala&gt; array.withColumn(<span class="string">&quot;array_join&quot;</span>, array_join($<span class="string">&quot;id_array&quot;</span>, <span class="string">&quot;,&quot;</span>)).show()</span><br><span class="line">+---------+---------------+----------+</span><br><span class="line">|id_string|       id_array|array_join|</span><br><span class="line">+---------+---------------+----------+</span><br><span class="line">|<span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span>|[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]| <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>|</span><br><span class="line">+---------+---------------+----------+</span><br><span class="line"></span><br><span class="line">scala&gt; array.select(explode($<span class="string">&quot;id_array&quot;</span>)).show()</span><br><span class="line">+---+</span><br><span class="line">|col|</span><br><span class="line">+---+</span><br><span class="line">|  <span class="number">1</span>|</span><br><span class="line">|  <span class="number">2</span>|</span><br><span class="line">|  <span class="number">3</span>|</span><br><span class="line">|  <span class="number">4</span>|</span><br><span class="line">|  <span class="number">5</span>|</span><br><span class="line">+---+</span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.sql(<span class="string">&quot;select arrays_zip(array(1,2,3),array(&#x27;4&#x27;,&#x27;5&#x27;)) as array_zip&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [array_zip: array&lt;struct&lt;<span class="number">0</span>:int,<span class="number">1</span>:string&gt;&gt;]</span><br><span class="line"></span><br><span class="line">scala&gt; df.show(<span class="literal">false</span>)</span><br><span class="line">+----------------------+</span><br><span class="line">|array_zip             |</span><br><span class="line">+----------------------+</span><br><span class="line">|[[<span class="number">1</span>, <span class="number">4</span>], [<span class="number">2</span>, <span class="number">5</span>], [<span class="number">3</span>,]]|</span><br><span class="line">+----------------------+</span><br></pre></td></tr></table></div></figure>


        <h4 id="处理-Map"   >
          <a href="#处理-Map" class="heading-link"><i class="fas fa-link"></i></a>处理 Map</h4>
      <div class="table-container"><table>
<thead>
<tr>
<th>Method</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>map_keys</td>
<td>返回 map 的 key</td>
</tr>
<tr>
<td>map_values</td>
<td>返回 map 的 value</td>
</tr>
<tr>
<td>map_from_arrays</td>
<td>接收两个数组，第一个数组为 key 数组，第二个数组为 value 数组，key 数组不允许存在空值。</td>
</tr>
<tr>
<td>getField / getItem</td>
<td>根据 key 取值</td>
</tr>
</tbody></table></div>
<p>不同于结构体，map 里面的数据类型要求是一致的。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> map = <span class="type">Seq</span>(<span class="type">Map</span>(<span class="number">1</span> -&gt; <span class="string">&quot;1&quot;</span>), <span class="type">Map</span>(<span class="number">2</span> -&gt; <span class="string">&quot;2&quot;</span>)).toDF(<span class="string">&quot;map&quot;</span>)</span><br><span class="line">map: org.apache.spark.sql.<span class="type">DataFrame</span> = [map: map&lt;int,string&gt;]</span><br><span class="line"></span><br><span class="line">scala&gt; map.show()</span><br><span class="line">+--------+</span><br><span class="line">|     map|</span><br><span class="line">+--------+</span><br><span class="line">|[<span class="number">1</span> -&gt; <span class="number">1</span>]|</span><br><span class="line">|[<span class="number">2</span> -&gt; <span class="number">2</span>]|</span><br><span class="line">+--------+</span><br><span class="line">scala&gt; map.withColumn(<span class="string">&quot;getField&quot;</span>, $<span class="string">&quot;map&quot;</span>.getItem(<span class="number">1</span>)).show()</span><br><span class="line">+--------+--------+</span><br><span class="line">|     map|getField|</span><br><span class="line">+--------+--------+</span><br><span class="line">|[<span class="number">1</span> -&gt; <span class="number">1</span>]|       <span class="number">1</span>|</span><br><span class="line">|[<span class="number">2</span> -&gt; <span class="number">2</span>]|    <span class="literal">null</span>|</span><br><span class="line">+--------+--------+</span><br></pre></td></tr></table></div></figure>


        <h4 id="处理-struct"   >
          <a href="#处理-struct" class="heading-link"><i class="fas fa-link"></i></a>处理 struct</h4>
      <p>类似于 C++ 中的结构体。在 Scala 中可以用 case class  实现。取出数据可以用 getField。如果你想构造自己的结构体，可以直接使用 struct 函数，接收多列组合成一列，组合列的类型是结构体。如果你想展开结构体，可以直接在 select 中使用 * 通配符。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> simple_struct = df.select(explode($<span class="string">&quot;array_zip&quot;</span>))</span><br><span class="line">simple_struct: org.apache.spark.sql.<span class="type">DataFrame</span> = [col: struct&lt;<span class="number">0</span>: int, <span class="number">1</span>: string&gt;]</span><br><span class="line"></span><br><span class="line">scala&gt; simple_struct.select(<span class="string">&quot;col.*&quot;</span>).show()</span><br><span class="line">+---+----+</span><br><span class="line">|  <span class="number">0</span>|   <span class="number">1</span>|</span><br><span class="line">+---+----+</span><br><span class="line">|  <span class="number">1</span>|   <span class="number">4</span>|</span><br><span class="line">|  <span class="number">2</span>|   <span class="number">5</span>|</span><br><span class="line">|  <span class="number">3</span>|<span class="literal">null</span>|</span><br><span class="line">+---+----+</span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;select struct(1, 2, 3, 4,  5)&quot;</span>).show()</span><br><span class="line">+---------------------------------------------------------+</span><br><span class="line">|named_struct(col1, <span class="number">1</span>, col2, <span class="number">2</span>, col3, <span class="number">3</span>, col4, <span class="number">4</span>, col5, <span class="number">5</span>)|</span><br><span class="line">+---------------------------------------------------------+</span><br><span class="line">|                                          [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]|</span><br><span class="line">+---------------------------------------------------------+</span><br></pre></td></tr></table></div></figure>

        <h4 id="处理-Json"   >
          <a href="#处理-Json" class="heading-link"><i class="fas fa-link"></i></a>处理 Json</h4>
      <p>Spark 在读取 json 数据格式的时候会自动解析出各列。有时候你可能需要在数据中生成 json 字符串。这里会用到 to_json。 </p>

        <h3 id="聚合计算"   >
          <a href="#聚合计算" class="heading-link"><i class="fas fa-link"></i></a>聚合计算</h3>
      <p>聚合计算主要是 sum，min，max，avg，countDistinct 等等，常和 groupBy，agg 等结合使用。在后面的 Aggregate 中详细讲</p>

        <h3 id="其他"   >
          <a href="#其他" class="heading-link"><i class="fas fa-link"></i></a>其他</h3>
      <p>这里的其他函数指的是  org.apache.spark.sql.functions 中 @group 为 normal_funcs 的函数。介绍一下常用的。</p>
<div class="table-container"><table>
<thead>
<tr>
<th>Method</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>lit</td>
<td>生成一个 Column，该 Column 的值是不变的</td>
</tr>
<tr>
<td>typedLit</td>
<td>生成一个 Column，该 Column 的值是不变的，和上面的区别是支持 Scala 中的 Seq，Map，List 类型</td>
</tr>
<tr>
<td>struct</td>
<td>生成结构体</td>
</tr>
<tr>
<td>when</td>
<td>类似于 sql 中的 when，和 otherwise 结合使用</td>
</tr>
</tbody></table></div>
<p>tip：对于 lit，我们在指定 track 的类型。对于 typedLit，我们可以用于指定生成一个空的 properties。对与 struct，我们用来选取指定字段构造 properties。这里直接拿一段 Spark SQL 的代码举例。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">pvInfoDF</span><br><span class="line">      .select(cols.map(col(_)): _*)</span><br><span class="line">      .filter(adjust_valid)</span><br><span class="line">      .filter(delimiter_valid)</span><br><span class="line">      .filter(!col(<span class="string">&quot;$ip&quot;</span>).contains(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">      .filter($<span class="string">&quot;channel_id&quot;</span> isin (channel: _*))</span><br><span class="line">      .na.replace(cols, <span class="type">Map</span>(<span class="string">&quot;-&quot;</span> -&gt; <span class="string">&quot;&quot;</span>))</span><br><span class="line">      .withColumn(<span class="string">&quot;original_id&quot;</span>, <span class="type">UDF</span>.getUUID($<span class="string">&quot;original_id&quot;</span>))</span><br><span class="line">      .withColumn(<span class="string">&quot;distinct_id&quot;</span>, when($<span class="string">&quot;distinct_id&quot;</span>.notEqual(<span class="string">&quot;&quot;</span>), $<span class="string">&quot;distinct_id&quot;</span>).otherwise($<span class="string">&quot;original_id&quot;</span>))</span><br><span class="line">      .withColumn(<span class="string">&quot;hour&quot;</span>, <span class="type">UDF</span>.intUDF($<span class="string">&quot;hour&quot;</span>))</span><br><span class="line">      .withColumn(<span class="string">&quot;type&quot;</span>, lit(<span class="string">&quot;track&quot;</span>))</span><br><span class="line">      .withColumn(<span class="string">&quot;time_free&quot;</span>, lit(<span class="literal">true</span>))</span><br><span class="line">      .withColumn(<span class="string">&quot;project&quot;</span>, lit(<span class="string">s&quot;<span class="subst">$project</span>&quot;</span>))</span><br><span class="line">      .withColumn(<span class="string">&quot;event&quot;</span>, lit(<span class="string">&quot;$pageview&quot;</span>))</span><br><span class="line">      .withColumn(<span class="string">&quot;$screen_width&quot;</span>, <span class="type">UDF</span>.getScreen($<span class="string">&quot;screen&quot;</span>, lit(<span class="number">0</span>)))</span><br><span class="line">      .withColumn(<span class="string">&quot;$screen_height&quot;</span>, <span class="type">UDF</span>.getScreen($<span class="string">&quot;screen&quot;</span>, lit(<span class="number">1</span>)))</span><br><span class="line">      .withColumn(<span class="string">&quot;time&quot;</span>, unix_timestamp($<span class="string">&quot;time&quot;</span>, <span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>))</span><br><span class="line">      .transform(<span class="type">UDF</span>.transformInt(int_cols))</span><br></pre></td></tr></table></div></figure>


]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>DataSource API - Managing Datasets in External Data Sources</title>
    <url>/posts/a5a69ed8/</url>
    <content><![CDATA[
        <h2 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h2>
      <p>前面已经讲解了 Spark SQL 程序的入口，使用到的数据结构以及一些定义在上面的简单操作。那么我们工作中该如何将各种数据源中的数据转换成 Spark SQL 可以处理的数据结构进而进行各种计算呢？这就是本篇幅要讲解的 DataSource API（DataSource API 提供了读写各种数据源的 format，你甚至可以自定义 format 来连接外部数据源）。</p>
<a id="more"></a>


        <h2 id="DataFrameReader"   >
          <a href="#DataFrameReader" class="heading-link"><i class="fas fa-link"></i></a>DataFrameReader</h2>
      <p>在 Spark 中，连接处理各种形式的数据源是通过 DataSource API 中的 DataFrameReader 接口来实现的。你可以使用 SparkSession 来创建一个 DataFrameReader。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> reader = spark.read</span><br><span class="line">reader: org.apache.spark.sql.<span class="type">DataFrameReader</span> = org.apache.spark.sql.<span class="type">DataFrameReader</span>@<span class="number">65067</span>d37</span><br><span class="line"><span class="type">DataFrameReader</span> 将数据源中的数据通过合适的 format 转换成 <span class="type">Spark</span> <span class="type">SQL</span> 可以处理的 <span class="type">DataFrame</span>。<span class="type">Spark</span> <span class="number">2.0</span> 提供了 textFile 方法，此时并不会返回 <span class="type">DataFrame</span>，而是 <span class="type">Dataset</span>[<span class="type">String</span>]</span><br><span class="line"></span><br><span class="line">scala&gt; reader.format(<span class="string">&quot;csv&quot;</span>).load(<span class="string">&quot;people.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; reader.json.load(<span class="string">&quot;people.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; reader.textFile(<span class="string">&quot;get-pip.py&quot;</span>)</span><br><span class="line">res0: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">String</span>] = [value: string]</span><br></pre></td></tr></table></div></figure>
<p>下面 DataFrameReader 常见的接口</p>
<div class="table-container"><table>
<thead>
<tr>
<th>Method</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody><tr>
<td>csv</td>
<td>从 CSV 文件或者DataSet[String] 中读取 CSV 格式数据</td>
<td><code>spark.read.csv(&quot;xxx.csv&quot;)</code></td>
</tr>
<tr>
<td>format</td>
<td>读取数据源的格式，允许自定义</td>
<td><br><code>spark.read.foramt(&quot;csv&quot;)</code></br><br>spark.read.foramt(“json”)</br><br>spark.read.foramt(“custome source”)</br></td>
</tr>
<tr>
<td>jdbc</td>
<td>通过 JDBC 连接</td>
<td></td>
</tr>
<tr>
<td>json</td>
<td>从 json 文件或者DataSet[String] 中读取 json 格式数据</td>
<td><code>spark.read.json(&quot;*.json&quot;)</code></td>
</tr>
<tr>
<td>load</td>
<td>从数据源加载数据</td>
<td><code>spark.read.foramt(&quot;csv&quot;).load(&quot;input_path&quot;)</code></td>
</tr>
<tr>
<td>option</td>
<td>设置加载数据源的一些可选项，比如不加载 CSV 的 header，指定分隔符等</td>
<td>`spark.read.format(“csv”).option(“delimiter”,  “</td>
</tr>
<tr>
<td>options</td>
<td>接收 Map 来设置可选项</td>
<td>`spark.read.format(“csv”).options(Map(“delimiter” → “</td>
</tr>
<tr>
<td>orc</td>
<td>orc 格式文件    spark.read.orc(input_path)</td>
<td></td>
</tr>
<tr>
<td>parquet</td>
<td>parquet 格式文件，这也是 Spark 的默认 format 。可以通过 spark.sql.sources.default 更改</td>
<td><code>spark.read.parquet(input_path)</code></td>
</tr>
<tr>
<td>schema</td>
<td>读取文件时指定 schema，schema 的生成后面会讲解</td>
<td><code>spark.read.format(&quot;csv&quot;).schema(inferScheam)</code></td>
</tr>
<tr>
<td>text</td>
<td>读取文件，返回 DataFrame</td>
<td></td>
</tr>
<tr>
<td>textFile</td>
<td>读取文件，返回 Dataset[String]</td>
<td></td>
</tr>
</tbody></table></div>
<p>tip：Spark read 数据并不会触发计算。</p>
<p>Spark load 的路径可以是目录，并且支持通配符，Spark 会自动递归查找到目录下的所有文件 </p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">spark.read.csv(<span class="string">&quot;/data/sa_cluster/etl/*/*.csv&quot;</span>)</span><br><span class="line">````</span><br><span class="line"></span><br><span class="line"><span class="type">Spark</span> <span class="number">2.2</span> 开始支持从 <span class="type">Dataset</span>[<span class="type">String</span>] 中加载数据。大概会有这样的场景，对格式不统一的数据先加载成 <span class="type">Dataset</span>[<span class="type">String</span>]，然后按 <span class="type">Row</span> 处理每行数据成标准格式，最后从 <span class="type">Dataset</span>[<span class="type">String</span>] 中读取格式化的数据。</span><br><span class="line">```scala</span><br><span class="line">scala&gt; <span class="keyword">val</span> people = <span class="type">Seq</span>(<span class="string">&quot;Mike, 40&quot;</span>).toDS</span><br><span class="line">people: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">String</span>] = [value: string]</span><br><span class="line"></span><br><span class="line">scala&gt; spark.read.csv(people)</span><br><span class="line">res3: org.apache.spark.sql.<span class="type">DataFrame</span> = [_c0: string, _c1: string]</span><br><span class="line"></span><br><span class="line">scala&gt; spark.read.csv(people).show()</span><br><span class="line">+----+---+</span><br><span class="line">| _c0|_c1|</span><br><span class="line">+----+---+</span><br><span class="line">|<span class="type">Mike</span>| <span class="number">40</span>|</span><br><span class="line">+----+---+</span><br></pre></td></tr></table></div></figure>

<p>tip：spark.readStream 用来读取流数据</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; spark.readStream</span><br><span class="line">res5: org.apache.spark.sql.streaming.<span class="type">DataStreamReader</span> = org.apache.spark.sql.streaming.<span class="type">DataStreamReader</span>@<span class="number">45354715</span></span><br></pre></td></tr></table></div></figure>


        <h2 id="DataFrameWriter"   >
          <a href="#DataFrameWriter" class="heading-link"><i class="fas fa-link"></i></a>DataFrameWriter</h2>
      <p>DataFrameWriter 与 DataFrameReader 对应，将计算处理好的数据以各种数据格式存储。可以用处理好的 Dataset 直接创建。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> writer = spark.range(<span class="number">5</span>).write</span><br><span class="line">writer: org.apache.spark.sql.<span class="type">DataFrameWriter</span>[<span class="type">Long</span>] = org.apache.spark.sql.<span class="type">DataFrameWriter</span>@<span class="number">1131</span>fcfd</span><br><span class="line"><span class="type">DataFrameWriter</span> 支持的 format 和 <span class="type">DataFrameReader</span> 是一样的。当然默认 format 还是 parquet 文件。 详细的 <span class="type">API</span> 可以去查看 org.apache.spark.sql.<span class="type">DataFrameWriter</span> 的源代码。可以去读一下 save 方法。里面告诉了我们直接将存入 <span class="type">Hive</span> 中是不允许的，最后的 save 的触发操作是由 runcommand 实现的。这里的一些逻辑我个人还没有搞清楚，仅仅是建议大家可以去读一下源代码。</span><br><span class="line"></span><br><span class="line">如果你想了解存储，actions 算子到底是如何触发计算，触发存储的，那么读源代码是一个很好的选择。</span><br><span class="line"></span><br><span class="line">scala&gt; writer.format(<span class="string">&quot;json&quot;</span>).save(<span class="string">&quot;id&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; writer.json(<span class="string">&quot;id&quot;</span>)</span><br></pre></td></tr></table></div></figure>


        <h2 id="DataSource-API-v2"   >
          <a href="#DataSource-API-v2" class="heading-link"><i class="fas fa-link"></i></a>DataSource API v2</h2>
      <p>在今天之前，我只是看到过 V1  V2这样的字眼。并没有详细了解过。这里只作为一个引路人的角色。</p>
<p>先介绍几个概念</p>
<ul>
<li>列裁剪： 过滤掉查询不需要使用到的列。就是这样子的啦，select id, name from table。</li>
<li>谓词下推 ：将过滤过程尽可能的推到底层，最好数据源端，这样子在执行阶段数据计算量就会相应变少。举个极端的例子，如果数据在上层才过滤，那么从读取到 fliter 都要保持着全表才可以，这无疑加大了计算量和资源消耗，我们希望的是读取出来的数据就是已经过滤的。<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> ds = spark.range(<span class="number">3</span>).withColumn(<span class="string">&quot;idPlus&quot;</span>, $<span class="string">&quot;id&quot;</span> + <span class="number">1</span>)</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">DataFrame</span> = [id: bigint, idPlus: bigint]</span><br><span class="line"></span><br><span class="line">scala&gt; ds.select(<span class="string">&quot;idPlus&quot;</span>).filter($<span class="string">&quot;id&quot;</span> === <span class="number">1</span>).explain(<span class="literal">true</span>)</span><br><span class="line">== <span class="type">Parsed</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line"><span class="symbol">&#x27;Filter</span> (<span class="symbol">&#x27;id</span> = <span class="number">1</span>)</span><br><span class="line">+- <span class="type">Project</span> [idPlus#<span class="number">6</span>L]</span><br><span class="line">   +- <span class="type">Project</span> [id#<span class="number">4</span>L, (id#<span class="number">4</span>L + cast(<span class="number">1</span> as bigint)) <span class="type">AS</span> idPlus#<span class="number">6</span>L]</span><br><span class="line">      +- <span class="type">Range</span> (<span class="number">0</span>, <span class="number">3</span>, step=<span class="number">1</span>, splits=<span class="type">Some</span>(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">== <span class="type">Analyzed</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line">idPlus: bigint</span><br><span class="line"><span class="type">Project</span> [idPlus#<span class="number">6</span>L]</span><br><span class="line">+- <span class="type">Filter</span> (id#<span class="number">4</span>L = cast(<span class="number">1</span> as bigint))</span><br><span class="line">   +- <span class="type">Project</span> [idPlus#<span class="number">6</span>L, id#<span class="number">4</span>L]</span><br><span class="line">      +- <span class="type">Project</span> [id#<span class="number">4</span>L, (id#<span class="number">4</span>L + cast(<span class="number">1</span> as bigint)) <span class="type">AS</span> idPlus#<span class="number">6</span>L]</span><br><span class="line">         +- <span class="type">Range</span> (<span class="number">0</span>, <span class="number">3</span>, step=<span class="number">1</span>, splits=<span class="type">Some</span>(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">== <span class="type">Optimized</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line"><span class="type">Project</span> [(id#<span class="number">4</span>L + <span class="number">1</span>) <span class="type">AS</span> idPlus#<span class="number">6</span>L]</span><br><span class="line">+- <span class="type">Filter</span> (id#<span class="number">4</span>L = <span class="number">1</span>)</span><br><span class="line">   +- <span class="type">Range</span> (<span class="number">0</span>, <span class="number">3</span>, step=<span class="number">1</span>, splits=<span class="type">Some</span>(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">== <span class="type">Physical</span> <span class="type">Plan</span> ==</span><br><span class="line">*(<span class="number">1</span>) <span class="type">Project</span> [(id#<span class="number">4</span>L + <span class="number">1</span>) <span class="type">AS</span> idPlus#<span class="number">6</span>L]</span><br><span class="line">+- *(<span class="number">1</span>) <span class="type">Filter</span> (id#<span class="number">4</span>L = <span class="number">1</span>)</span><br><span class="line">   +- *(<span class="number">1</span>) <span class="type">Range</span> (<span class="number">0</span>, <span class="number">3</span>, step=<span class="number">1</span>, splits=<span class="number">4</span>)</span><br></pre></td></tr></table></div></figure>
可以看到上面例子的 Optimized Logical Plan 。filter 被下推到了 project 前面 。</li>
</ul>
<p>Spark 1.3 为了提供一个统一的数据源 API 开始引入 DataSource V1。有了 DataSource V1，我们可以很方便的读取各种来源的数据，而且 Spark 使用 SQL 组件的一些优化引擎对数据源的读取进行优化，比如列裁剪、过滤下推等等。 你可以在  org.apache.spark.sql.source 中查看源代码。Spark SQL 的谓词下推是根据某些规则来的，并不是任何谓词任何条件下都会下推。</p>
<p>既然 Datasource API 可以满足我们绝大多数的需求，那为什么又出来个 DataSource v2。主要是由于以下几点因素。</p>
<p>Datasource API v1 依赖于一些上层 API，如 SqlContext 和 DataFrame。我们知道 Spark 2.x 里面 SqlContext 被 SparkSession 代替，DataFrame 被统一到 Dataset。上层 API 在不断更新发展，在 Datasource API v1 中确没有什么体现。<br>DataSource API v1 不支持列式读取。Spark SQL 引擎本身支持列式存储，但是在 DataSource API v1 里没有体现。<br>DataSource API v1 实现一些算子下推太过繁琐。比如 limit 下推，如果实现的话，就是一大推接口，TableScan，PrunedScan<br>DataSource API v1 缺乏分区和排序信息。数据源的分区和排序信息并不能传递给 Spark 优化器<br>DataSource API v1 不支持流处理<br>DataSource API v1 写操作不支持事务。比如，像 Mysql 中存入数据过程中发生异常，已经存进去的数据不会被清理，破坏数据的一致性。需要引入事务。<br>DataSource API v2 应运而生，可以简单看下，v2 基本已经解决了上述的问题，支持自定义分区信息。</p>
<p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd420l7p8vj305z08cgm1.jpg"></p>
<p>这里直接用一个网上 DataSource v2 API 读取 MySQL 数据的例子，来看看如何自定义你的 format，实现你的读写逻辑，谓词下推。</p>
<p>通过 DataSource API v2 的 ReadSupport 接口来实现自定义数据源 reader，这里是读取 Mysql，如果是写 Mysql 需要 WriteSupport</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> mysqlReader</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.sources.v2.reader.<span class="type">DataSourceReader</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.sources.v2.&#123;<span class="type">DataSourceOptions</span>, <span class="type">DataSourceV2</span>, <span class="type">ReadSupport</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConverters</span>._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultSource</span> <span class="keyword">extends</span> <span class="title">DataSourceV2</span> <span class="keyword">with</span> <span class="title">ReadSupport</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createReader</span></span>(options: <span class="type">DataSourceOptions</span>): <span class="type">DataSourceReader</span> = &#123;</span><br><span class="line">    <span class="type">MySQLSourceReader</span>(options.asMap().asScala.toMap)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

<p>通过 DatasourceReader 具体实现读操作，读取的 scheam，列裁剪，支持的谓词下推，分区信息都可以在这里重写</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> mysqlReader</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.catalyst.<span class="type">InternalRow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.execution.datasources.jdbc.&#123;<span class="type">JDBCOptions</span>, <span class="type">JDBCRDD</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.sources.v2.reader.&#123;<span class="type">DataSourceReader</span>, <span class="type">InputPartition</span>, <span class="type">SupportsPushDownFilters</span>, <span class="type">SupportsPushDownRequiredColumns</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.sources.&#123;<span class="type">EqualTo</span>, <span class="type">Filter</span>, <span class="type">GreaterThan</span>, <span class="type">IsNotNull</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.<span class="type">StructType</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConverters</span>._</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">MySQLSourceReader</span>(<span class="params">options: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]</span>) <span class="keyword">extends</span> <span class="title">DataSourceReader</span> <span class="keyword">with</span> <span class="title">SupportsPushDownFilters</span></span></span><br><span class="line"><span class="class">  <span class="keyword">with</span> <span class="title">SupportsPushDownRequiredColumns</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> supportedFilters: <span class="type">ArrayBuffer</span>[<span class="type">Filter</span>] = <span class="type">ArrayBuffer</span>[<span class="type">Filter</span>]()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> requiredSchema: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> jdbcOptions = <span class="keyword">new</span> <span class="type">JDBCOptions</span>(options)</span><br><span class="line">    <span class="type">JDBCRDD</span>.resolveTable(jdbcOptions)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">readSchema</span></span>(): <span class="type">StructType</span> = &#123;</span><br><span class="line">    requiredSchema</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">planInputPartitions</span></span>(): util.<span class="type">List</span>[<span class="type">InputPartition</span>[<span class="type">InternalRow</span>]] = &#123;</span><br><span class="line">    <span class="type">List</span>[<span class="type">InputPartition</span>[<span class="type">InternalRow</span>]](<span class="type">MySQLInputPartition</span>(requiredSchema, supportedFilters.toArray, options)).asJava</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">pushFilters</span></span>(filters: <span class="type">Array</span>[<span class="type">Filter</span>]): <span class="type">Array</span>[<span class="type">Filter</span>] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (filters.isEmpty) &#123;</span><br><span class="line">      <span class="keyword">return</span> filters</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> unsupportedFilters = <span class="type">ArrayBuffer</span>[<span class="type">Filter</span>]()</span><br><span class="line">    filters foreach &#123;</span><br><span class="line">      <span class="keyword">case</span> f: <span class="type">EqualTo</span> =&gt; supportedFilters += f</span><br><span class="line">      <span class="keyword">case</span> f: <span class="type">GreaterThan</span> =&gt; supportedFilters += f</span><br><span class="line">      <span class="keyword">case</span> f: <span class="type">IsNotNull</span> =&gt; supportedFilters += f</span><br><span class="line">      <span class="keyword">case</span> f@_ =&gt; unsupportedFilters += f</span><br><span class="line">    &#125;</span><br><span class="line">    unsupportedFilters.toArray</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">pushedFilters</span></span>(): <span class="type">Array</span>[<span class="type">Filter</span>] = supportedFilters.toArray</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">pruneColumns</span></span>(requiredSchema: <span class="type">StructType</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.requiredSchema = requiredSchema</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

<p>InputPartitionReader 实现具体的分区读取操作</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> mysqlReader</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.&#123;<span class="type">DriverManager</span>, <span class="type">ResultSet</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.catalyst.<span class="type">InternalRow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.catalyst.util.<span class="type">DateTimeUtils</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.jdbc.<span class="type">JdbcDialects</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.sources.v2.reader.&#123;<span class="type">InputPartition</span>, <span class="type">InputPartitionReader</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.sources.&#123;<span class="type">EqualTo</span>, <span class="type">Filter</span>, <span class="type">GreaterThan</span>, <span class="type">IsNotNull</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.unsafe.types.<span class="type">UTF8String</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">MySQLInputPartition</span>(<span class="params">requiredSchema: <span class="type">StructType</span>, pushed: <span class="type">Array</span>[<span class="type">Filter</span>], options: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]</span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">InputPartition</span>[<span class="type">InternalRow</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createPartitionReader</span></span>(): <span class="type">InputPartitionReader</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">    <span class="type">MySQLInputPartitionReader</span>(requiredSchema, pushed, options)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">MySQLInputPartitionReader</span>(<span class="params">requiredSchema: <span class="type">StructType</span>, pushed: <span class="type">Array</span>[<span class="type">Filter</span>], options: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]</span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">InputPartitionReader</span>[<span class="type">InternalRow</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> tableName = options(<span class="string">&quot;dbtable&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> driver = options(<span class="string">&quot;driver&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> url = options(<span class="string">&quot;url&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initSQL</span> </span>= &#123;</span><br><span class="line">    <span class="keyword">val</span> selected = <span class="keyword">if</span> (requiredSchema.isEmpty) <span class="string">&quot;1&quot;</span> <span class="keyword">else</span> requiredSchema.fieldNames.mkString(<span class="string">&quot;,&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (pushed.nonEmpty) &#123;</span><br><span class="line">      <span class="keyword">val</span> dialect = <span class="type">JdbcDialects</span>.get(url)</span><br><span class="line">      <span class="keyword">val</span> filter = pushed.map &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">EqualTo</span>(attr, value) =&gt; <span class="string">s&quot;<span class="subst">$&#123;dialect.quoteIdentifier(attr)&#125;</span> = <span class="subst">$&#123;dialect.compileValue(value)&#125;</span>&quot;</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">GreaterThan</span>(attr, value) =&gt; <span class="string">s&quot;<span class="subst">$&#123;dialect.quoteIdentifier(attr)&#125;</span> &gt; <span class="subst">$&#123;dialect.compileValue(value)&#125;</span>&quot;</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">IsNotNull</span>(attr) =&gt; <span class="string">s&quot;<span class="subst">$&#123;dialect.quoteIdentifier(attr)&#125;</span> IS NOT NULL&quot;</span></span><br><span class="line"></span><br><span class="line">      &#125;.mkString(<span class="string">&quot; AND &quot;</span>)</span><br><span class="line"></span><br><span class="line">      <span class="string">s&quot;select <span class="subst">$selected</span> from <span class="subst">$tableName</span> where <span class="subst">$filter</span>&quot;</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="string">s&quot;select <span class="subst">$selected</span> from <span class="subst">$tableName</span>&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> rs: <span class="type">ResultSet</span> = &#123;</span><br><span class="line">    <span class="type">Class</span>.forName(driver)</span><br><span class="line">    <span class="keyword">val</span> conn = <span class="type">DriverManager</span>.getConnection(url)</span><br><span class="line">    println(initSQL)</span><br><span class="line">    <span class="keyword">val</span> stmt = conn.prepareStatement(initSQL, <span class="type">ResultSet</span>.<span class="type">TYPE_FORWARD_ONLY</span>, <span class="type">ResultSet</span>.<span class="type">CONCUR_READ_ONLY</span>)</span><br><span class="line">    stmt.setFetchSize(<span class="number">1000</span>)</span><br><span class="line">    stmt.executeQuery()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>(): <span class="type">Boolean</span> = rs.next()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">get</span></span>(): <span class="type">InternalRow</span> = &#123;</span><br><span class="line">    <span class="type">InternalRow</span>(requiredSchema.fields.zipWithIndex.map &#123; element =&gt;</span><br><span class="line">      element._1.dataType <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">IntegerType</span> =&gt; rs.getInt(element._2 + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">case</span> <span class="type">LongType</span> =&gt; rs.getLong(element._2 + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">case</span> <span class="type">StringType</span> =&gt; <span class="type">UTF8String</span>.fromString(rs.getString(element._2 + <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">DecimalType</span> =&gt; <span class="keyword">val</span> d = rs.getBigDecimal(element._2 + <span class="number">1</span>)</span><br><span class="line">          <span class="type">Decimal</span>(d, d.precision, d.scale)</span><br><span class="line">        <span class="keyword">case</span> <span class="type">TimestampType</span> =&gt; <span class="keyword">val</span> t = rs.getTimestamp(element._2 + <span class="number">1</span>)</span><br><span class="line">          <span class="type">DateTimeUtils</span>.fromJavaTimestamp(t)</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    &#125;: _*)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = rs.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

<p>使用自定义的 format 读取 Mysql 数据库</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">test</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">&quot;MySQL&quot;</span>).master(<span class="string">&quot;local[*]&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    spark.read</span><br><span class="line">      .format(<span class="string">&quot;mysqlReader&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://127.0.0.1:3306/fangtianxia?user=root&amp;password=123456789&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;newfangdetail&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;driver&quot;</span>, <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">      .load()</span><br><span class="line">      .selectExpr(<span class="string">&quot;url_name&quot;</span>, <span class="string">&quot;score&quot;</span>)</span><br><span class="line">      .filter($<span class="string">&quot;url_name&quot;</span>.equalTo(<span class="string">&quot;121119&quot;</span>) &amp;&amp; $<span class="string">&quot;score&quot;</span> &gt;= <span class="string">&quot;3.6&quot;</span>)</span><br><span class="line">      .explain(<span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

<p>执行后，可以看到执行的物理计划如下，equalTo 被下推到数据源端，而 &gt;= 没有被下推，因为我们自定义的 pushedFilter 中不支持 &gt;= 下推</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">&#x3D;&#x3D; Physical Plan &#x3D;&#x3D;</span><br><span class="line">*(1) Project [url_name#3, score#11]</span><br><span class="line">+- *(1) Filter (score#11 &gt;&#x3D; 3.6)</span><br><span class="line">   +- *(1) ScanV2 DefaultSource[url_name#3, score#11] (Filters: [isnotnull(url_name#3), isnotnull(score#11), (url_name#3 &#x3D; 121119)], Options: [dbtable&#x3D;newfangdetail,driver&#x3D;com.mysql.jdbc.Driver,url&#x3D;*********(redacted),paths&#x3D;[]])</span><br></pre></td></tr></table></div></figure>


        <h2 id="本文参考"   >
          <a href="#本文参考" class="heading-link"><i class="fas fa-link"></i></a>本文参考</h2>
      <p><span class="exturl"><a class="exturl__link"   href="https://zhuanlan.zhihu.com/p/83006243" >一文理解 Apache Spark DataSource V2 诞生背景及入门实战</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Dataset - Structured Query with Data Encoder</title>
    <url>/posts/f6ce126b/</url>
    <content><![CDATA[
        <h2 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h2>
      <p>Dataset 是 Spark SQL 中的一种强类型数据结构。用于抽象结构化查询。在 Spark 2.x 中，我们常常会用到 Dataset  API 来表达我们对数据集的操作。</p>
<a id="more"></a>

        <h2 id="Dataset"   >
          <a href="#Dataset" class="heading-link"><i class="fas fa-link"></i></a>Dataset</h2>
      <p>我们知道 Dataset 代表的是数据集，那么 Dataset 的数据结构构成就是数据集合吗？下面这张图为我们展现了是什么构成了 Dataset。Dataset 到底是什么？<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd41bh53dpj30dp0b7aak.jpg"><br>Dataset 包含以下三种元素（这在我们程序 debug 的时候也可以看到）</p>
<p>QuerExecution （解析未分析的 LogicalPlan 逻辑计划）<br>Encoder （解析每行数据，序列化或者反序列化对应数据。eg：DataFrame 用的是 RowEncoder。）<br>SparkSession<br>Dataset 是惰性计算的，只有遇到 action 算子在才会真正的触发计算。相比 DataFrame，Dataset 提供了声明式和类型安全的操作符。更通俗点来说，Dataset 是强类型的，而 DataFrame 是弱类型的。</p>
<p>Dataset 是强类型 typedrel 的，会在编译的时候进行类型检测；而 DataFrame 是弱类型 untypedrel 的，在执行的时候进行类型检测。<br>Dataset 是通过 Encoder 进行序列化，支持动态的生成代码，直接在 bytes 的层面进行排序，过滤等的操作。而 DataFrame 是采用可选的 java 的标准序列化或是 kyro 进行序列化。<br>Dataset 在 Spark 1.6的 feature 中被引入。到了Spark 2.x 中，对 Dataset 和 DataFrame 做了统一。type DataFrame = Dataset[Row]<br>我们可以对比一下 Dataset 和 DataFrame 的执行计划，可以看到 DataFrame 在分析执行计划时并没有提供类型检查。而 Dataset 却可以做到，这些都是由 scala 编译器自动完成的。这也是 Dataset 更吸引人的地方。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; spark.range(<span class="number">1</span>).toDF.filter(_ == <span class="number">0</span>).explain(<span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">== <span class="type">Parsed</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line"><span class="symbol">&#x27;TypedFilter</span> &lt;function1&gt;, interface org.apache.spark.sql.<span class="type">Row</span>, [<span class="type">StructField</span>(id,<span class="type">LongType</span>,<span class="literal">false</span>)], unresolveddeserializer(createexternalrow(getcolumnbyordinal(<span class="number">0</span>, <span class="type">LongType</span>), <span class="type">StructField</span>(id,<span class="type">LongType</span>,<span class="literal">false</span>)))</span><br><span class="line">+- <span class="type">Range</span> (<span class="number">0</span>, <span class="number">1</span>, step=<span class="number">1</span>, splits=<span class="type">Some</span>(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">== <span class="type">Analyzed</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line">id: bigint</span><br><span class="line"><span class="type">TypedFilter</span> &lt;function1&gt;, interface org.apache.spark.sql.<span class="type">Row</span>, [<span class="type">StructField</span>(id,<span class="type">LongType</span>,<span class="literal">false</span>)], createexternalrow(id#<span class="number">23</span>L, <span class="type">StructField</span>(id,<span class="type">LongType</span>,<span class="literal">false</span>))</span><br><span class="line">+- <span class="type">Range</span> (<span class="number">0</span>, <span class="number">1</span>, step=<span class="number">1</span>, splits=<span class="type">Some</span>(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">== <span class="type">Optimized</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line"><span class="type">TypedFilter</span> &lt;function1&gt;, interface org.apache.spark.sql.<span class="type">Row</span>, [<span class="type">StructField</span>(id,<span class="type">LongType</span>,<span class="literal">false</span>)], createexternalrow(id#<span class="number">23</span>L, <span class="type">StructField</span>(id,<span class="type">LongType</span>,<span class="literal">false</span>))</span><br><span class="line">+- <span class="type">Range</span> (<span class="number">0</span>, <span class="number">1</span>, step=<span class="number">1</span>, splits=<span class="type">Some</span>(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">== <span class="type">Physical</span> <span class="type">Plan</span> ==</span><br><span class="line">*(<span class="number">1</span>) <span class="type">Filter</span> &lt;function1&gt;.apply</span><br><span class="line">+- *(<span class="number">1</span>) <span class="type">Range</span> (<span class="number">0</span>, <span class="number">1</span>, step=<span class="number">1</span>, splits=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; spark.range(<span class="number">1</span>).filter(_ == <span class="number">0</span>).explain(<span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">== <span class="type">Parsed</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line"><span class="symbol">&#x27;TypedFilter</span> &lt;function1&gt;, <span class="class"><span class="keyword">class</span> <span class="title">java</span>.<span class="title">lang</span>.<span class="title">Long</span>, [<span class="type">StructField</span>(value,<span class="type">LongType</span>,true)], <span class="title">unresolveddeserializer</span>(<span class="params">staticinvoke(class java.lang.<span class="type">Long</span>, <span class="type">ObjectType</span>(class java.lang.<span class="type">Long</span></span>), <span class="title">valueOf</span>, <span class="title">upcast</span>(<span class="params">getcolumnbyordinal(0, <span class="type">LongType</span></span>), <span class="title">LongType</span>, <span class="title">-</span> <span class="title">root</span> <span class="title">class</span></span>: <span class="string">&quot;java.lang.Long&quot;</span>), <span class="literal">true</span>, <span class="literal">false</span>))</span><br><span class="line">+- <span class="type">Range</span> (<span class="number">0</span>, <span class="number">1</span>, step=<span class="number">1</span>, splits=<span class="type">Some</span>(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">== <span class="type">Analyzed</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line">id: bigint</span><br><span class="line"><span class="type">TypedFilter</span> &lt;function1&gt;, <span class="class"><span class="keyword">class</span> <span class="title">java</span>.<span class="title">lang</span>.<span class="title">Long</span>, [<span class="type">StructField</span>(value,<span class="type">LongType</span>,true)], <span class="title">staticinvoke</span>(<span class="params">class java.lang.<span class="type">Long</span>, <span class="type">ObjectType</span>(class java.lang.<span class="type">Long</span></span>), <span class="title">valueOf</span>, <span class="title">cast</span>(<span class="params">id#27L as bigint</span>), <span class="title">true</span>, <span class="title">false</span>)</span></span><br><span class="line"><span class="class"><span class="title">+-</span> <span class="title">Range</span> (<span class="params">0, 1, step=1, splits=<span class="type">Some</span>(4</span>))</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">==</span> <span class="title">Optimized</span> <span class="title">Logical</span> <span class="title">Plan</span> <span class="title">==</span></span></span><br><span class="line"><span class="class"><span class="title">TypedFilter</span> <span class="title">&lt;function1&gt;</span>, <span class="title">class</span> <span class="title">java</span>.<span class="title">lang</span>.<span class="title">Long</span>, [<span class="type">StructField</span>(value,<span class="type">LongType</span>,true)], <span class="title">staticinvoke</span>(<span class="params">class java.lang.<span class="type">Long</span>, <span class="type">ObjectType</span>(class java.lang.<span class="type">Long</span></span>), <span class="title">valueOf</span>, <span class="title">id#27L</span>, <span class="title">true</span>, <span class="title">false</span>)</span></span><br><span class="line"><span class="class"><span class="title">+-</span> <span class="title">Range</span> (<span class="params">0, 1, step=1, splits=<span class="type">Some</span>(4</span>))</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">==</span> <span class="title">Physical</span> <span class="title">Plan</span> <span class="title">==</span></span></span><br><span class="line"><span class="class"><span class="title">*</span>(<span class="params">1</span>) <span class="title">Filter</span> <span class="title">&lt;function1&gt;</span>.<span class="title">apply</span></span></span><br><span class="line"><span class="class"><span class="title">+-</span> <span class="title">*</span>(<span class="params">1</span>) <span class="title">Range</span> (<span class="params">0, 1, step=1, splits=4</span>)</span></span><br></pre></td></tr></table></div></figure>
<p>Dataset 是可查询的，可序列化的，并且可以作持久化存储。</p>
<p>SparkSession 和 QueryExecution 作为 Dataset 的临时属性不会被序列化。但是 Encoder 会被序列化，反序列化的时候还需要 Encoder 来解析。<br>Dataset 默认的存储级别是 MEMORY_AND_DISK。这里后面的 cache &amp;&amp; persist 会讲到。<br>Spark 2.X 提供了 Structured Streaming。其还是使用 Dataset 来做为底层的数据结构来进行静态有界数据流和无界数据流的计算。通过 Dataset 提供的统一 API。我们可以更关注不同编程模型的计算逻辑。</p>

        <h2 id="DataFrame"   >
          <a href="#DataFrame" class="heading-link"><i class="fas fa-link"></i></a>DataFrame</h2>
      <p>在 Dataset 小节里面也简单提到了 DataFrame 是什么以及和 Dataset 的区别。这里单独拿出来讲下如何创建 DataFrame 以及 DataFrame 的一些简单操作。因为工作中很多情况下都是从文件或者数据库中读取（DataFrameReader 在读取时会调用 ofRows 生成 DataFrame）。</p>
<p>DataFrame 可以被看作是由 row 和 named columns 组成的分布式表格数据集。就跟关系型数据中的一张数据表一样，我们可以对其进行 select，filter，join，group 等操作。其具有 RDD 的一切特性，比如，弹性，并行，分布式，只读。</p>
<p>下面是一个简单的 word count 程序</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = <span class="type">Seq</span>((<span class="string">&quot;one&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;one&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;two&quot;</span>, <span class="number">1</span>)).toDF(<span class="string">&quot;word&quot;</span>, <span class="string">&quot;count&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; df.groupBy(<span class="string">&quot;word&quot;</span>).count().show()</span><br><span class="line">+----+-----+</span><br><span class="line">|word|count|</span><br><span class="line">+----+-----+</span><br><span class="line">| two|    <span class="number">1</span>|</span><br><span class="line">| one|    <span class="number">2</span>|</span><br><span class="line">+----+-----+</span><br></pre></td></tr></table></div></figure>
<p>从 Scala 序列或者 case class 或者 createDataFrame 创建 DataFrame</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> df = <span class="type">Seq</span>((<span class="number">1</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">4</span>)).toDF(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;id+&quot;</span>)</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">People</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">people</span> </span>= <span class="type">Seq</span>(<span class="type">People</span>(<span class="string">&quot;zz&quot;</span>, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> df1 = spark.createDataFrame(people)</span><br><span class="line"><span class="keyword">val</span> df2 = people.toDF </span><br></pre></td></tr></table></div></figure>
<p>通过 DataFrameReader 创建 DataFrame，经常用到的方法，支持多种数据源的读取，json，csv，parquet，text，JDBC，Kafka等等。在后面的 DataSource API 中会详细讲。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> reader = spark.read</span><br><span class="line">reader: org.apache.spark.sql.<span class="type">DataFrameReader</span> = org.apache.spark.sql.<span class="type">DataFrameReader</span>@<span class="number">2125</span>bb4e</span><br><span class="line"></span><br><span class="line">scala&gt; reader.json(<span class="string">&quot;file.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; reader.csv(<span class="string">&quot;file.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; reader.parquet(<span class="string">&quot;file.parquet&quot;</span>)</span><br></pre></td></tr></table></div></figure>


<p>DataFrame query，你可以像使用数据表一样，通过 SQL 和 Dataset API 来对 DataFrame 进行查询计算。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">improt org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> columns  = <span class="type">Seq</span>(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>, <span class="string">&quot;grade&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> a = df.select(colums.map(col(_)): _*) <span class="comment">// select 接受 Column 类型，所以需要做转换</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> b = df.selectExpr(columns: _*)  <span class="comment">// selectExpr 接受 Expression 类型</span></span><br><span class="line"></span><br><span class="line">a.filter($<span class="string">&quot;name&quot;</span>.equalTo(<span class="string">&quot;Time Machine&quot;</span>))</span><br><span class="line"></span><br><span class="line">a.groupBy(<span class="string">&quot;age&quot;</span>).count()</span><br><span class="line"></span><br><span class="line">a.withColumn(<span class="string">&quot;GradeString&quot;</span>, $<span class="string">&quot;grade&quot;</span>.toString)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> table = a.registerTempTable(<span class="string">&quot;people&quot;</span>) (<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sql = spark.sql(<span class="string">&quot;SELECT count(*) AS count FROM people&quot;</span>)</span><br></pre></td></tr></table></div></figure>



]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker常用命令</title>
    <url>/posts/29dc6fe8/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>本篇用来持续更新记录在使用 Docker 过程中经常遇到的命令以及一些小问题</p>
<a id="more"></a>


        <h3 id="compose-后台启动所有服务"   >
          <a href="#compose-后台启动所有服务" class="heading-link"><i class="fas fa-link"></i></a>compose 后台启动所有服务</h3>
      <figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">docker-compose up -d</span><br></pre></td></tr></table></div></figure>

        <h3 id="compose-重新构建依赖镜像"   >
          <a href="#compose-重新构建依赖镜像" class="heading-link"><i class="fas fa-link"></i></a>compose 重新构建依赖镜像</h3>
      <figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">docker-compose up --build</span><br></pre></td></tr></table></div></figure>

        <h3 id="快速检查容器基本信息"   >
          <a href="#快速检查容器基本信息" class="heading-link"><i class="fas fa-link"></i></a>快速检查容器基本信息</h3>
      <figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">docker inspect <span class="variable">$&#123;container_id&#125;</span> <span class="comment"># 关于 container 的查看，使用 docker ps -a</span></span><br></pre></td></tr></table></div></figure>

        <h3 id="查看容器日志"   >
          <a href="#查看容器日志" class="heading-link"><i class="fas fa-link"></i></a>查看容器日志</h3>
      <figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">log</span> <span class="variable">$&#123;container_id&#125;</span></span><br></pre></td></tr></table></div></figure>

        <h3 id="删除所有已经停止的容器"   >
          <a href="#删除所有已经停止的容器" class="heading-link"><i class="fas fa-link"></i></a>删除所有已经停止的容器</h3>
      <figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">docker rm `docker ps -a | grep Exited | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span>`</span><br></pre></td></tr></table></div></figure>

        <h3 id="删除所有构建过程中出错的镜像，这里就是简单删除了-TAG-为-None-的"   >
          <a href="#删除所有构建过程中出错的镜像，这里就是简单删除了-TAG-为-None-的" class="heading-link"><i class="fas fa-link"></i></a>删除所有构建过程中出错的镜像，这里就是简单删除了 TAG 为 None 的</h3>
      <figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">docker rmi `docker images | grep <span class="string">&#x27;&lt;none&gt;&#x27;</span> | awk <span class="string">&#x27;&#123;print $3&#125;&#x27;</span>` <span class="comment"># 注意只有删除了依赖镜像的相关容器后才能删除基础镜像</span></span><br></pre></td></tr></table></div></figure>
]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Doris 物化视图</title>
    <url>/posts/56052ea3/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>物化视图（Materialized View）本质是一种预计算，即把某些耗时的操作（例如JOIN、AGGREGATE）的结果保存下来，以便在查询时直接复用，从而避免这些耗时的操作，最终达到加速查询的目的。其作为一种检索加速技术，应用在许多 olap 引擎中，本文以 Doris 为例，主要介绍 Doris 的物化视图实现、应用场景以及如何查看是否命中物化视图、如果没有命中的话，原因又是哪些。</p>
<a id="more"></a>


        <h3 id="使用场景"   >
          <a href="#使用场景" class="heading-link"><i class="fas fa-link"></i></a>使用场景</h3>
      <p>物化视图一般在数据摄入的时候实时异步或者延时生成，会加剧写放大问题，并且占用额外的物理存储空间，会对集群造成一定的存储压力，数据摄入速度也会受到影响。因此一般在碰到比较严重的性能问题，并且物化视图加速比较明显的时候才会使用。另外物化视图常用于基于明细数据的固定维度查询上，这也是其和 rollup 预聚合的主要区别。</p>
<p>Doris 中的物化视图主要应用在以下两个场景中。</p>

        <h4 id="前缀索引"   >
          <a href="#前缀索引" class="heading-link"><i class="fas fa-link"></i></a>前缀索引</h4>
      <p>Doris 默认的索引机制只有前缀索引（类似于 LSM 中的稀疏索引），在数据摄入的时候会根据排序列来构建，并且最长只有 36 个字节，超过之后就会被截断。如果第一个字段就是 varchar  类型，大多数情况下单独以其它 key 列作为谓词来过滤查询的时候是不会命中前缀索引的，需要扫描所有的 tablet 分片，这时候在大数据量下，olap_scan_node 节点就会产生的一定的性能问题，尤其是后缀 key 列经常作为条件来过滤查询的场景。</p>
<p>这个时候就可以通过物化视图来解决，针对常用的  key 列查询构造基于不同排序列的物化视图，从而让查询能够更多的命中前缀索引，来减少 scan 节点的压力。这里举个例子</p>
<p>存在 base 表如下，有k1，k2 作为排序列，用于构造前缀索引。默认只有 k1 参与谓词过滤的条件时，才会命中前缀索引加速查询。</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="string">`temp.redbook_ecm_base`</span>(</span><br><span class="line">    <span class="string">`k1`</span> <span class="built_in">varchar</span>(<span class="number">10</span>),</span><br><span class="line">    <span class="string">`k2`</span> <span class="built_in">varchar</span>(<span class="number">10</span>),</span><br><span class="line">    <span class="string">`k3`</span> <span class="built_in">varchar</span>(<span class="number">10</span>),</span><br><span class="line">    <span class="string">`v1`</span> <span class="built_in">bigint</span></span><br><span class="line">) <span class="keyword">ENGINE</span>=OLAP</span><br><span class="line"><span class="keyword">DUPLICATE</span> <span class="keyword">KEY</span>(<span class="string">`k1`</span>, <span class="string">`k2`</span>)</span><br><span class="line"><span class="keyword">DISTRIBUTED</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(<span class="string">`k1`</span>) BUCKETS <span class="number">4</span></span><br><span class="line"></span><br></pre></td></tr></table></div></figure>
<p>如果 k3  作为经常过滤查询的维度的话，可以创建物化视图把 k3 列作为排序列，从而应用到前缀索引。</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">MATERIALIZED</span> <span class="keyword">VIEW</span> mv_1 <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> k3, k2, k1,v1</span><br><span class="line"><span class="keyword">FROM</span> tableA</span><br></pre></td></tr></table></div></figure>
<p>针对后缀列的过滤加速，除了采用改变排序列的物化视图外，还可以手动对后缀列添加 bitmap 或者 bloomfilter 索引，一般是低基维采用  bitmap，高基维采用 bloomfilter。</p>

        <h4 id="预聚合"   >
          <a href="#预聚合" class="heading-link"><i class="fas fa-link"></i></a>预聚合</h4>
      <p>举个例子，比如广告流量链路数据，特点是数据量大，涉及到了广告、策略、算法、落地页、转化等多个方向的维度。在不同的分析场景下，查询的维度频次有比较明显的差异，并且也确实需要组合多个方向维度的明细数据查询。像这种分析场景覆盖明细数据查询以及部分高频维度的场景，也比较适合于物化视图。</p>
<p>可以对高频维度建立对应的物化视图，比如某张 base 表存在 adpos、adunit、campaign、strategy、rank_mondel、page、order 等维度，而算法同学往往只关注相关模型带来的曝光成单转化。因此可以直接建立对应的物化视图</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">MATERIALIZED</span> <span class="keyword">VIEW</span> mv_1 <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line"> rank_mondel,</span><br><span class="line"> <span class="keyword">sum</span>(deliver),</span><br><span class="line"> <span class="keyword">sum</span>(click),</span><br><span class="line"> <span class="keyword">sum</span>(<span class="keyword">order</span>)</span><br><span class="line"><span class="keyword">FROM</span> tableA</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> rank_mondel;</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>
<p>在查询的时候，仍使用查询 base 表的 sql 语句即可。在执行计划解析时，会自动选择是否命中物化视图，并改写 sql 语句将查询打到选择的物化视图上去。</p>

        <h3 id="命中物化视图"   >
          <a href="#命中物化视图" class="heading-link"><i class="fas fa-link"></i></a>命中物化视图</h3>
      <p>建立了物化视图就一定会命中吗？答案是否定的，要想命中物化视图，必须保证查询的 key、value 列属于物化视图包含的 key、value 列的子集。</p>
<p>如果存在多个物化视图的话，又会选择哪个呢？之前只看过 Druid 的物化视图命中的源码，Doris 其实也是类似的。当存在多个物化视图的时候，命中会经历以下几个步骤</p>
<ul>
<li>查询元数据，获取该 base 表的所有物化视图信息。</li>
<li>比较查询的 key、value 列是否属于物化视图的子集，如果属于，则放入有序集合中，这里的有序会按照物化视图大小进行升序或者降序排序。</li>
<li>直接选择物理大小最小的物化视图进行查询。</li>
</ul>
<p>如何查看是否命中物化视图？一般是通过查看执行计划或者 profile 来实现。这里以 profile 为例。</p>
<ol>
<li>对于应用前缀索引的物化视图，着重关注 olap_scan_node 节点的耗时以及相关参数。一般 rollup 参数为物化视图名称时，则代表命中物化视图，此时可以在对比下命中物化视图前后扫描的 tablet 分片数以及行数是否降低。如果没有减少，可以考虑下是否存储是均匀划分的。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/materialized_view/olap_scan_node_1.png"></li>
</ol>
<p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/materialized_view/olap_scan_node_2.png"></p>
<ol start="2">
<li>对于预聚合的物化视图，着重关注 olap_scan_node 和 aggergate_node 节点的耗时以及相关参数。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/materialized_view/agg_node_1.png"><br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/materialized_view/agg_node_2.png"></li>
</ol>

        <h3 id="小结"   >
          <a href="#小结" class="heading-link"><i class="fas fa-link"></i></a>小结</h3>
      <p>物化视图适合于即需要大数据量明细数据查询又有高频维度聚合的场景，其经常作为性能优化手段出现，一般不预先设计。本质上是拿存储空间换查询效率，并且有写放大问题。因此需要把握好存储、摄入、查询三者间的平衡，往往在使用前需要做好基础的性能测试。</p>

        <h3 id="参考"   >
          <a href="#参考" class="heading-link"><i class="fas fa-link"></i></a>参考</h3>
      <p><span class="exturl"><a class="exturl__link"   href="https://docs.starrocks.io/zh-cn/latest/using_starrocks/Materialized_view" >物化视图</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://docs.starrocks.io/zh-cn/latest/table_design/Sort_key" >排序列和前缀索引</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Doris</tag>
        <tag>StarRocks</tag>
      </tags>
  </entry>
  <entry>
    <title>Elasticsearch 与 Hive 集成</title>
    <url>/posts/6614a01c/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>工作上存在将 Hive 上的数据刷到 ES 的场景，首先想到的是自己写程序读取 Hive 上的数据，经过业务逻辑处理在写回到 ES 上，不过请教了下，知道了 ES 本身已经可以和 Hive 集成。只需添加对应的 jar 包，在 hive 上建立与 ES 关联的外部表，即可使用 HQL 查询写入 ES 索引库。具体使用请见<span class="exturl"><a class="exturl__link"   href="https://www.elastic.co/guide/en/elasticsearch/hadoop/current/hive.html" >官方文档</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> ，本文只举个简单例子及介绍下主要的参数。</p>
<a id="more"></a>


        <h3 id="Elasticsearch-集成-Hive"   >
          <a href="#Elasticsearch-集成-Hive" class="heading-link"><i class="fas fa-link"></i></a>Elasticsearch 集成 Hive</h3>
      
        <h4 id="elasticsearch-hadoop-jar-包下载"   >
          <a href="#elasticsearch-hadoop-jar-包下载" class="heading-link"><i class="fas fa-link"></i></a>elasticsearch-hadoop jar 包下载</h4>
      <p>去 maven 中央仓库上去搜对应的 jar 包就行，这里只需要注意 jar 包的版本要和 ES  版本相对应。查看 ES 版本，只需要访问 <code>$&#123;es_node&#125;:9200</code> 即可。<span class="exturl"><a class="exturl__link"   href="https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-hadoop" >下载链接</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p>下载完成后，只需要进入 Hive ，输入以下命令即可，和添加 UDF jar 差不多。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">add jar <span class="variable">$&#123;your_path&#125;</span>/elasticsearch-hadoop-<span class="variable">$&#123;version&#125;</span>.jar</span><br></pre></td></tr></table></div></figure>

        <h4 id="建立-Hive-外部表"   >
          <a href="#建立-Hive-外部表" class="heading-link"><i class="fas fa-link"></i></a>建立 Hive 外部表</h4>
      <p>建表语句如下，这里需要注意 Hive 中的数据类型与 ES 中的对应关系，具体对应列表可以查看 <span class="exturl"><a class="exturl__link"   href="https://www.elastic.co/guide/en/elasticsearch/hadoop/current/hive.html" >Apache Hive integration</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> es_warehouse.tblleadsaccesslog(</span><br><span class="line">    batch_name <span class="keyword">STRING</span>,</span><br><span class="line">    channel <span class="keyword">STRING</span>,</span><br><span class="line">    course_id <span class="keyword">STRING</span>,</span><br><span class="line">    create_time <span class="keyword">STRING</span>,</span><br><span class="line">    op_date <span class="keyword">STRING</span>,</span><br><span class="line">    op_hour <span class="keyword">STRING</span>,</span><br><span class="line">    stu_uid <span class="keyword">STRING</span>,</span><br><span class="line">    <span class="keyword">id</span> <span class="keyword">STRING</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">&#x27;org.elasticsearch.hadoop.hive.EsStorageHandler&#x27;</span></span><br><span class="line">TBLPROPERTIES(</span><br><span class="line">  <span class="string">&#x27;COLUMN_STATS_ACCURATE&#x27;</span>=<span class="string">&#x27;false&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;es.batch.write.refresh&#x27;</span>=<span class="string">&#x27;false&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;es.index.auto.create&#x27;</span>=<span class="string">&#x27;false&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;es.input.use.sliced.partitions&#x27;</span>=<span class="string">&#x27;false&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;es.mapping.exclude&#x27;</span>=<span class="string">&#x27;doc_id&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;es.mapping.id&#x27;</span>=<span class="string">&#x27;id&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;es.net.http.auth.pass&#x27;</span>=<span class="string">&#x27;test&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;es.net.http.auth.user&#x27;</span>=<span class="string">&#x27;test&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;es.nodes&#x27;</span>=<span class="string">&#x27;172.29.236.154&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;es.nodes.wan.only&#x27;</span>=<span class="string">&#x27;true&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;es.port&#x27;</span>=<span class="string">&#x27;9200&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;es.resource&#x27;</span>=<span class="string">&#x27;ods_lec_leads_accesslog/_doc&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;es.scroll.size&#x27;</span>=<span class="string">&#x27;10000&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;es.write.operation&#x27;</span>=<span class="string">&#x27;upsert&#x27;</span></span><br><span class="line">);</span><br></pre></td></tr></table></div></figure>
<p>建表执行完成并且没有错误，就可以使用 HQL 直接查询关联的 ods_lec_leads_accesslog 的 ES 索引库。</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> es_warehouse.tblleadsaccesslog <span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></div></figure>
<p>也可以通过 HQL 直接插入数据到 ES。</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tblleadsaccesslog <span class="keyword">values</span>(<span class="string">&#x27;test&#x27;</span>, <span class="string">&#x27;test&#x27;</span>, <span class="string">&#x27;test&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;test&#x27;</span>, <span class="string">&#x27;1&#x27;</span>);</span><br></pre></td></tr></table></div></figure>
<p>查询对应的 ES 索引库，可以查询到 batch_name 为 test 的数据。</p>
<p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/008eGmZEly1gml9r54y5dj31ke0u0n51.jpg"></p>

        <h4 id="ES-相关属性"   >
          <a href="#ES-相关属性" class="heading-link"><i class="fas fa-link"></i></a>ES 相关属性</h4>
      <p>更详细的可以直接参考官方文档 <span class="exturl"><a class="exturl__link"   href="https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html" >Configuration</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<div class="table-container"><table>
<thead>
<tr>
<th>属性名</th>
<th>释义</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>es.resource</td>
<td>es 索引库/表</td>
<td><br>也可以分别指定读，写的索引库</br><br>es.resource.read</br> es.resource.write</td>
</tr>
<tr>
<td>es.nodes</td>
<td>es 节点</td>
<td></td>
</tr>
<tr>
<td>es.port</td>
<td>es 端口号</td>
<td></td>
</tr>
<tr>
<td>es.net.http.auth.pass</td>
<td>es 认证用户密码</td>
<td></td>
</tr>
<tr>
<td>es.net.http.auth.user</td>
<td>es 认证用户名</td>
<td></td>
</tr>
<tr>
<td>es.index.auto.create</td>
<td>是否自动创建不存在的索引</td>
<td></td>
</tr>
<tr>
<td>es.mapping.id document</td>
<td>id 对应的字段</td>
<td></td>
</tr>
<tr>
<td>es.batch.write.refresh</td>
<td>批量更新完成是否刷新索引库</td>
<td></td>
</tr>
<tr>
<td>es.write.operation</td>
<td>es 写时模式</td>
<td>index，create，update，upsert</td>
</tr>
<tr>
<td>es.query</td>
<td>默认读取 es 时的查询 DSL</td>
<td></td>
</tr>
<tr>
<td>es.mapping.names</td>
<td>hive 与 es 字段之间的对应关系</td>
<td></td>
</tr>
</tbody></table></div>
<p>配置属性都不太熟，尤其是刚使用 ES，还是建议先读一遍配置，不然就踩坑了。最近就踩了一个 case。在重新刷数据的时候，没有设置 <code>es.write.operation</code> 导致相同 doc_id 的数据被完全替换掉，而不是更新。这个参数默认是 index。</p>
<ul>
<li>index 新数据会追加，旧数据会直接覆盖</li>
<li>create 追加数据，如果数据已经存在，抛出异常</li>
<li>update 更新旧数据，如果旧数据不存在，抛出异常</li>
<li>upsert 追加新数据，更新旧数据</li>
</ul>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop之HDFS</title>
    <url>/posts/c508bdb8/</url>
    <content><![CDATA[
        <h3 id="HDFS"   >
          <a href="#HDFS" class="heading-link"><i class="fas fa-link"></i></a>HDFS</h3>
      <p>Hadoop 分布式文件存储系统。用于海量数据的存储，往往是静态数据，适合 OLAP 分析。</p>
<a id="more"></a>
<ul>
<li>典型的 Master/Slaver 架构，NameNode 负责管理元数据和文件系统命名空间，处理来自客户端的请求，依赖 ZK 实现 HA 高可用。多个 DateNode 负责数据的存储，并上报自己的 block 信息。</li>
<li>伸缩性好，易于拓展，很容易拓展多个 DataNode。</li>
<li>高效，并发访问读取，按 block 大小切片读取。</li>
<li>高容错性，多副本机制保证了一台机器挂掉，文件不会丢失。</li>
<li>一次写入，多次读取，适用于 OLAP 分析。</li>
</ul>

        <h3 id="HDFS-客户端-API-操作"   >
          <a href="#HDFS-客户端-API-操作" class="heading-link"><i class="fas fa-link"></i></a>HDFS 客户端 API 操作</h3>
      <p>Hdfs 命令和普通的 linux 命令操作没啥区别。不过多解释。关于 API 操作，就是一句话，根据配置获取文件系统对象。就像下面这样</p>
<ol>
<li>引入相关 maven 依赖<figure class="highlight xml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.9.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client--&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.9.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs --&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.9.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></div></figure></li>
<li>代码编写，这里要主要的是权限问题，不指定用户容易出问题。emmm，第一次碰到权限问题还是写 Spark 的时候，Spark 默认存 Hdfs，如果要存本地文件，需要 file:// 标识本地文件系统。<figure class="highlight java"><div class="table-container"><table><tr><td class="code"><pre><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop1:9000/&quot;</span>), configuration, <span class="string">&quot;root&quot;</span>);</span><br><span class="line">fs.mkdirs(<span class="keyword">new</span> Path(<span class="string">&quot;/test&quot;</span>));</span><br><span class="line">fs.close();</span><br></pre></td></tr></table></div></figure>

</li>
</ol>

        <h3 id="HDFS-读写机制"   >
          <a href="#HDFS-读写机制" class="heading-link"><i class="fas fa-link"></i></a>HDFS 读写机制</h3>
      <p>客户端与HDFS的读写其实也是一种网络通信。所以一开始的步骤肯定是互相问候<br>C: hello，我要读数据啦<br>N: 好的，数据在这个地方<br>C: hello，我要写数据了<br>N: 好的，我看看现在能不能写。可以写了<br>C: 那我写啦，这是我要写的数据，你看放在啥地方合适呢<br>N: 往这个地方写吧<br>C: 这个 D 挂了怎么办<br>N: 收到，我给你分配个新的</p>

        <h4 id="机架感知"   >
          <a href="#机架感知" class="heading-link"><i class="fas fa-link"></i></a>机架感知</h4>
      <p>副本是如何选择机器存储的，如果副本都在一个机器上，那岂不是就没意义了。副本存储规则如下</p>
<ul>
<li>随机选取一个 DataNode 存储第一个副本，往往根据 DataNode 的具体使用情况。</li>
<li>第二个副本存储在和第一个副本存储的DataNode相同机架的不同机器上。</li>
<li>第三个副本存储在不同机架的节点。</li>
</ul>

        <h4 id="HDFS-读数据流程"   >
          <a href="#HDFS-读数据流程" class="heading-link"><i class="fas fa-link"></i></a>HDFS 读数据流程</h4>
      <ol>
<li>客户端请求 NameNode 读取文件，NameNode 查找元数据信息，找到文件所在的 DataNode</li>
<li>计算节点距离，找到离该客户端最近的 DataNode</li>
<li>DataNode开始传输数据给客户端(从磁盘里面读取数据输入流，以Packet为单位来做校验)。</li>
<li>客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</li>
</ol>

        <h4 id="HDFS-写数据流程"   >
          <a href="#HDFS-写数据流程" class="heading-link"><i class="fas fa-link"></i></a>HDFS 写数据流程</h4>
      <ol>
<li>客户端请求上传文件</li>
<li>NameNode 检查该文件及目录结构是否已经存在，如果存在，则返回客户端异常。不存在，告诉客户端可以上传</li>
<li>客户端请求上传第一个 block 的存储位置</li>
<li>NmaeNode 返回相应的存储位置，</li>
<li>客户端请求上传第一个 block，存储位置串行化 pipeline，dn1-&gt;dn2-&gt;dn3。dn1 收到请求发给dn2，dn2 收到请求发给 dn3</li>
<li>dn1，dn2，dn3 返回客户端应答信息</li>
<li>DataNode 以 packet 为单位接收数据，dn1 接收到了转给 dn2，dn2 转给 dn3</li>
<li>重复 3~7 的步骤上传剩余的 block</li>
</ol>

        <h4 id="写过程发生错误怎么办"   >
          <a href="#写过程发生错误怎么办" class="heading-link"><i class="fas fa-link"></i></a>写过程发生错误怎么办</h4>
      <ol>
<li>pipeline 被关闭，已经到故障节点的 packet 会被重新加到 pipeline 前端，保证管道中剩余的正常节点接收的 packet 不会丢失</li>
<li>为已经正常存储在其他 DataNode 上的 Block 赋值一个新的 ID，并上报给 DataNode，保证故障节点恢复后的 DataNode 上删除上传失败后的不完整数据</li>
<li>pipeline 中删除故障节点信息，继续像其余正常节点上存储数据</li>
<li>namenode 监控到文件副本数不足时，会自动处理。</li>
</ol>

        <h4 id="NN-与-2NN"   >
          <a href="#NN-与-2NN" class="heading-link"><i class="fas fa-link"></i></a>NN 与 2NN</h4>
      <p>关于元数据节点与元数据辅助节点，在<span class="exturl"><a class="exturl__link"   href="https://flyraty.github.io/2020/04/08/Hadoop%E5%AD%A6%E4%B9%A0-Hadoop%E6%98%AF%E4%BB%80%E4%B9%88/" >Hadoop 是什么</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>中简单提了下，这里在重提下。</p>
<ul>
<li>NameNode 启动生成 fsimage 快照，后续数据文件改动记录到 edits 日志</li>
<li>2NN 定期询问 NmaeNode 是否需要 CheckPoint，如果需要，复制 fsimage，滚动 edits 日志文件。传输到 2NN 结点，进行 edits 日志与 fsimage 的合并。</li>
<li>2NN 合并完成，生成一个新的 fsimage，传输到 NN 节点，重命名替换NN结点上的 fsimage。<br>为什么这样做呢？NmaeNode 作为元数据管理结点，肯定不能经常重启，在运行很长一段时间后，edits 日志文件已经很大，此时重启，fsimage 与 edits 日志文件合并时间较长，影响使用。所以有了 2NN 用来定期 checkpoint 合并，也降低了由于 NameNode 结点挂掉导致元数据大量丢失的风险。</li>
</ul>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop 之 MapReduce</title>
    <url>/posts/f8e3216b/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>接触大数据以来，主要写 Spark SQL 和 HQL 来处理数据。关于 MR，除了公司有些数据导入工具是采用 MR 来写的，顺便了解了下，其他时候都没怎么用过。现在想想，未免有些跳的太快了，Spark ，Hive 中的思想有很多也是借鉴 MR 的。甚至最初始的 Hive 就是 SQL 版的 MR。<br>本篇主要用来介绍一些 MR 的自定义特性以及如何编写 MR 程序。我们只需要告诉框架做什么，而不用关心框架怎么做。</p>
<a id="more"></a>


        <h3 id="MR-中的-shuffle"   >
          <a href="#MR-中的-shuffle" class="heading-link"><i class="fas fa-link"></i></a>MR 中的 shuffle</h3>
      <p>shuffle 机制几乎贯穿在所有大数据处理框架中。MR 中的 shuffle 是什么样子的呢？<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlgy1gh6q17d25wj30yx096myd.jpg"></p>
<p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlgy1gh6q19vatij30zi0bcq4g.jpg"></p>
<ol>
<li>InputFormat 组件读取数据，split 逻辑切片（和 block 大小有关），划分出各个 MapTask 处理数据。</li>
<li>经过自定义的 map 逻辑处理完之后，context.write 进行数据收集，并进行分区。</li>
<li>数据开始经过环形缓冲区，缓冲区大小默认 100m，达到默认溢写比例 0.8 后，数据便会溢写到磁盘上，溢写过程中一定会做排序，多次溢写会产生多个文件。</li>
<li>合并溢写文件到一个磁盘文件，内部记录索引信息表明下游每个 reduce 对应数据的偏移量。</li>
<li>下游 reduceTask 拉取 Map 端处理后的数据，</li>
<li>拉取的数据同样会先放入内存缓冲区中，达到阈值后便会溢写到磁盘，最终会合并成一个文件并排序（归并排序）。</li>
<li>经过自定义的 reduce 逻辑处理后，交由 OutputFormat 组件输出最终结果文件。</li>
</ol>

        <h3 id="MR-程序的编写"   >
          <a href="#MR-程序的编写" class="heading-link"><i class="fas fa-link"></i></a>MR 程序的编写</h3>
      <p>MR 的抽象已经很好了，一般只需要编写 Mapper，Reducer，Driver 三个类，我们只需要确定每个阶段的输入输出类型即可。如果需要编写复杂的 pipeline，则需要编写多个 MR 程序串联，这也是 MR 相比后起的 Spark 等大数据框架处理慢的原因（MR 计算的中间结果都要落盘才能被下一个 MR 读取）。使用 Java 编写 MR 程序，都要引入以下依赖，这里使用的 Hadoop 版本是 2.9.2。</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span> </span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span> </span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.9.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span> </span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span> </span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.9.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span> </span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span> </span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.9.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></div></figure>
<p>lombok 可选，主要是有些很方便的注解</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.projectlombok<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>lombok<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.16.18<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></div></figure>

        <h4 id="简单的WordCount"   >
          <a href="#简单的WordCount" class="heading-link"><i class="fas fa-link"></i></a>简单的WordCount</h4>
      
        <h5 id="Mapper"   >
          <a href="#Mapper" class="heading-link"><i class="fas fa-link"></i></a>Mapper</h5>
      <p>Mapper&lt;&gt; 中的4个字段分别代表 Map 阶段的输入输出的 key，value 类型。Hadoop 本身已经封装了常见的序列化类型，比如 long-&gt;LongWritable，string-&gt;Text。</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WcMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    Text k = <span class="keyword">new</span> Text();</span><br><span class="line">    IntWritable v = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String line = value.toString();</span><br><span class="line">        k.set(line);</span><br><span class="line">        context.write(k, v);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></div></figure>

        <h5 id="Reducer"   >
          <a href="#Reducer" class="heading-link"><i class="fas fa-link"></i></a>Reducer</h5>
      <p>Reducer 接收 Map 阶段的输出，并对同一个 key 进行汇总统计。</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">public class WcReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    IntWritable v &#x3D; new IntWritable();</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        int sum &#x3D; 0;</span><br><span class="line">        for (IntWritable value: values) &#123;</span><br><span class="line">            sum +&#x3D; value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        v.set(sum);</span><br><span class="line">        context.write(key, v);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

        <h5 id="Driver"   >
          <a href="#Driver" class="heading-link"><i class="fas fa-link"></i></a>Driver</h5>
      <p>MR 程序的启动类，</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WcDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(WcDriver.class); /* 设置启动类的位置* /</span><br><span class="line">        job.setMapperClass(WcMapper.class); /* 设置 Mapper 类 */</span><br><span class="line">        job.setReducerClass(WcReducer.class); /*设置 Reducer 类*/</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text.class); /* Mapper 的输出 key 类型*/</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class); /*Mapper 的输出 value 类型*/</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text.class); /*Reducer 的输出 key 类型*/</span><br><span class="line">        job.setOutputValueClass(IntWritable.class); /*Reducer 的输出 value 类型*/</span><br><span class="line"></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>])); <span class="comment">/*MR 程序输入路径*/</span></span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>])); <span class="comment">/*MR 程序的输出路径*/</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

        <h4 id="自定义序列化类型"   >
          <a href="#自定义序列化类型" class="heading-link"><i class="fas fa-link"></i></a>自定义序列化类型</h4>
      <p>自带的类型往往不满足需求，比如需要统计本校毕业生 2018~2020 的平均薪资，此时 Map 的 value 不在是一个字段，是三年薪资的加和在平均。自定义序列化类型只需要实现 Writable 接口。这里引入了下 lombok 的 @Data 注解来自动生成 set，get 方法</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StuSalary</span> <span class="keyword">implements</span> <span class="title">Writable</span></span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> firstYearSalary;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> secondYearSalary;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> thirdYearSalary;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">StuSalary</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">StuSalary</span><span class="params">(<span class="keyword">long</span> firstYearSalary, <span class="keyword">long</span> secondYearSalary, thirdYearSalary)</span> </span>&#123; </span><br><span class="line">    	<span class="keyword">this</span>.firstYearSalary = firstYearSalary;</span><br><span class="line">    	<span class="keyword">this</span>.secondYearSalary = secondYearSalary;</span><br><span class="line">    	<span class="keyword">this</span>.thirdYearSalary = thirdYearSalary;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 序列化方法 */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        out.writeLong(firstYearSalary);</span><br><span class="line">        out.writeLong(secondYearSalary);</span><br><span class="line">        out.writeLong(thirdYearSalary);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 反序列化方法 */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.firstYearSalary = in.readLong();</span><br><span class="line">    	<span class="keyword">this</span>.secondYearSalary = in.readLong();</span><br><span class="line">    	<span class="keyword">this</span>.thirdYearSalary = in.readLong();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>此时，在 Mapper，Reducer 中就可以使用自定义的 StuSalary 类型的。</p>

        <h4 id="自定义分区"   >
          <a href="#自定义分区" class="heading-link"><i class="fas fa-link"></i></a>自定义分区</h4>
      <p>默认的分区是 HashParatitioner，保证相同的 key 数据进入到同一分区。有时候我们希望一组相同的 key 进入到同一分区，这时候就会用到自定义分区器。主要是实现 Partitioner 接口的 getParatition 方法</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">StuSalary</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text text, StuSalary stusalary, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">int</span> partition=<span class="number">0</span>;</span><br><span class="line">		<span class="keyword">final</span> String name = text.toString();</span><br><span class="line">		<span class="keyword">if</span>(name.startsWith(<span class="string">&quot;zhang&quot;</span>)) &#123; </span><br><span class="line">			partition=<span class="number">1</span>;</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span>(appkey.startsWith(<span class="string">&quot;li&quot;</span>)) &#123; </span><br><span class="line">			partition=<span class="number">2</span>;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		    partition=<span class="number">0</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> partition;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>如果想使用自定义分区器，需要在 Driver 中设置 <code>job.setPartitionerClass(CustomPartitioner.class);</code>。并且最好保证 reduceTask 数量和自定义分区数保持一致，否则可能出现空文件或者异常报错。这里三个分区，所以设置 <code>job.setNumReduceTasks(3);</code></p>

        <h4 id="自定义排序"   >
          <a href="#自定义排序" class="heading-link"><i class="fas fa-link"></i></a>自定义排序</h4>
      <p>上面的都是 value 类型为自定义类型，那么如果 key 也是自定义类型呢？MR 的 shuffle 阶段会按照 key 排序。所以要想使用自定义类型，必须实现对应的排序方法<br>。需要实现 WritableComparable 的 compareTo 方法。</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(StuSalary stu)</span> </span>&#123;</span><br><span class="line">	<span class="comment">/* 定义你自己的排序逻辑*/</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br></pre></td></tr></table></div></figure>]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop之Yarn</title>
    <url>/posts/f069b6af/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>YARN 作为最常见的资源调度管理器，它是如何工作的呢？</p>
<a id="more"></a>


        <h3 id="YRAN"   >
          <a href="#YRAN" class="heading-link"><i class="fas fa-link"></i></a>YRAN</h3>
      <p>在 <a href="https://timemachine.icu/posts/19243a02/">Hadoop 是什么</a> 中简单提到了 Yarn 的主要组件。</p>
<ul>
<li>ResourceManager -&gt; 接收处理客户端的请求，资源分配与调度，启动监控 ApplicationMaster，监控 NodeManager。</li>
<li>NodeManager -&gt; 单个节点上的资源管理，处理来自 ResourceManager 和 ApplicationMaster 的命令。</li>
<li>ApplicationMaster -&gt; 为应用程序申请资源并分配给内部任务。</li>
<li>Container -&gt; 运行 job 封装的容器，抽象出来部分 CPU，内存等资源供给使用。</li>
</ul>

        <h4 id="Yarn-任务提交"   >
          <a href="#Yarn-任务提交" class="heading-link"><i class="fas fa-link"></i></a>Yarn 任务提交</h4>
      <ol>
<li>客户端提交任务给 ResourceManager</li>
<li>RM 处理客户端请求并分配 app_id 和资源提交路径</li>
<li>客户端上传应用程序等信息到资源路径下，并请求 RM 分配 ApplicationMaster</li>
<li>RM 将请求添加到容量调度器中</li>
<li>NM 空闲后领取到 job 并创建 Container 和 ApplicationMaster</li>
<li>ApplicationMaster 从资源提交路径中下载应用程序相关信息到本地</li>
<li>ApplicationMaster 向 RM 申请运行多个 MapTask</li>
<li>RM 分配 MapTask 到其余的 NM 上</li>
<li>其余 NM 创建任务运行的容器</li>
<li>ApplicationMaster 向其余 NM 上发送程序启动的脚本并运行</li>
<li>MapTask 运行完毕后，ApplicationMaster 会像 RM 申请运行 ReduceTask</li>
<li>ReduceTask 拉取 MapTask 产生的中间数据</li>
<li>ReduceTask 运行完毕后，MR 会向 RM 申请注销自己</li>
</ol>

        <h4 id="Yarn-调度策略"   >
          <a href="#Yarn-调度策略" class="heading-link"><i class="fas fa-link"></i></a>Yarn 调度策略</h4>
      <p>具体配置可以通过修改 yarn-site.xml，<code>yarn.resourcemanager.scheduler.class</code></p>

        <h5 id="FIFO"   >
          <a href="#FIFO" class="heading-link"><i class="fas fa-link"></i></a>FIFO</h5>
      <p>先进先出策略，容易阻塞小任务</p>

        <h5 id="Capacity-Scheduler"   >
          <a href="#Capacity-Scheduler" class="heading-link"><i class="fas fa-link"></i></a>Capacity Scheduler</h5>
      <p>容量调度策略，多队列模式，每个队列占用集群一定比例的资源，队列内部又采用 FIFO。比如可以设置常驻任务队列，临时任务队列。</p>

        <h4 id="Fair-Scheduler"   >
          <a href="#Fair-Scheduler" class="heading-link"><i class="fas fa-link"></i></a>Fair Scheduler</h4>
      <p>公平调度器，CDH 默认使用策略。举个例子，假设有两个用户 A 和 B，他们分别拥有一个队列。当A启动一个 job 而 B 没有任务时，A 会获得全部集群资源;当B启动一个 job 后，A 的 job 会继续运行，不过一会儿之后两个任务会各自获得一半的集群资源。如果此时 B 再启动第二个 job 并且其它 job 还在运行，则它将会和 B 的第一个 job 共享 B 这个队列的资源，也就是 B 的第二个 job 会用四分之 一的集群资源，而 A 的 job 仍然用于集群一半的资源，结果就是资源最终在两个用户之间平等的共享。</p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop安装与基本配置</title>
    <url>/posts/2679c08d/</url>
    <content><![CDATA[
        <h3 id="Mac-上安装-Hadoop"   >
          <a href="#Mac-上安装-Hadoop" class="heading-link"><i class="fas fa-link"></i></a>Mac 上安装 Hadoop</h3>
      <ul>
<li><p>前提条件</p>
<ul>
<li><span class="exturl"><a class="exturl__link"   href="https://flyraty.github.io/posts/c032fe54/" >Java 安装</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
</ul>
</li>
<li><p>命令安装<br>执行以下命令，hadoop 会被安装到 /usr/local/Cellar/Hadoop/${HADOOP_VERSION}，这样默认安装的是 Hadoop 的最新版本，修改配置可以直接去安装目录下。</p>
<a id="more"></a>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">brew instll hadoop</span><br></pre></td></tr></table></div></figure></li>
<li><p>预编译包安装<br>官网下载Hadoop的预编译包，地址是 <span class="exturl"><a class="exturl__link"   href="https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-$%7BHADOOP_VERSION%7D/hadoop-$%7BHADOOP_VERSION%7D-src.tar.gz%EF%BC%8C%E6%8C%87%E5%AE%9A%E4%BD%A0%E6%83%B3%E4%B8%8B%E8%BD%BD%E7%9A%84%E7%89%88%E6%9C%AC%E5%B0%B1%E5%A5%BD%E4%BA%86%E3%80%82%E4%B8%8B%E8%BD%BD%E4%B8%8B%E6%9D%A5%E5%90%8E%EF%BC%8C%E7%9B%B4%E6%8E%A5%E8%A7%A3%E5%8E%8B%E7%BC%A9%EF%BC%8C%E6%94%BE%E5%88%B0%E4%BD%A0%E6%83%B3%E7%9A%84%E7%9B%AE%E5%BD%95%E3%80%82" >https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}-src.tar.gz，指定你想下载的版本就好了。下载下来后，直接解压缩，放到你想的目录。</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
</li>
</ul>

        <h3 id="Hadoop的三种运行模式"   >
          <a href="#Hadoop的三种运行模式" class="heading-link"><i class="fas fa-link"></i></a>Hadoop的三种运行模式</h3>
      <ul>
<li>Local (Standalone) Mode 独立运行模式，Hadoop 默认的配置，运行在单个 java 进程中，常用于调试</li>
<li>Pseudo-Distributed Mode 伪分布式模式，Hadoop 不同功能组件运行在不同进程，但是都运行在一个节点上</li>
<li>Fully-Distributed Mode 完全分布式模式，Hadoop 运行在多个节点上</li>
</ul>

        <h3 id="伪分布式启动-Hadoop"   >
          <a href="#伪分布式启动-Hadoop" class="heading-link"><i class="fas fa-link"></i></a>伪分布式启动 Hadoop</h3>
      
        <h4 id="修改配置文件"   >
          <a href="#修改配置文件" class="heading-link"><i class="fas fa-link"></i></a>修改配置文件</h4>
      <p>以下需要注意的是，如果配置文件不存在，可以从对应的 .template 模板文件复制或者重命名。</p>
<ol>
<li><p>${HADOOP_HOME}/etc/hadoop/core-site.xml，修改 Hadoop 的默认文件系统，配置 Hadoop 临时文件目录。默认在 /tmp/hadoop-${user.name}</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hadoop-2.7.2/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></div></figure></li>
<li><p>${HADOOP_HOME}/etc/hadoop/hdfs-site.xml，配置副本数</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></div></figure></li>
<li><p><span class="exturl"><a class="exturl__link"   href="https://flyraty.github.io/posts/4feffbd0/" >配置SSH</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
</li>
<li><p>${HADOOP_HOME}/etc/hadoop/mapred-site.xml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></div></figure></li>
<li><p>${HADOOP_HOME}/etc/hadoop/yarn-site.xml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></div></figure>

</li>
</ol>

        <h4 id="启动-HDFS"   >
          <a href="#启动-HDFS" class="heading-link"><i class="fas fa-link"></i></a>启动 HDFS</h4>
      <ol>
<li>格式化 NameNode，多次格式化的时候有坑，主要原因是 DataNode 不认识格式化后的 NameNode，后面详细说。如果不先格式化 NameNode，start-hdfs.sh 的时候就不会启动 NameNode。<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></div></figure></li>
<li>启动 HDFS<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">start-hdfs.sh</span><br></pre></td></tr></table></div></figure></li>
<li>jps 查看 HDFS 相关的进程是否启动成功。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1gefmo350qjj30lc05874n.jpg"></li>
</ol>
<ol start="4">
<li><p>访问 localhost:50070，启动成功会看到 HDFS 的 web ui，可以查看 HDFS 的状态和基本信息。</p>
</li>
<li><p>测试创建目录，上传文件。在 web ui 上可以查看文件。文件到底存在了 HDFS 的什么地方呢。和 hadoop.tmp.dir 的文件设置路径有关。<br>hadoop.tmp.dir/dfs/data 里。真实的文件目录层级比较深，比如我这个是在 tmp/dfs/data/current/BP-1094310977-192.168.102.5-1588517605135/current/finalized/subdir0/subdir0/blk_1073741825。相应的 NameNode 的数据在 /tmp/dfs/name/current 下面。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">hadoop fs -mkdir -p /user/<span class="built_in">test</span></span><br><span class="line">hadoop fs -put etc/hadoop/core-site.xml /user/<span class="built_in">test</span></span><br></pre></td></tr></table></div></figure>

</li>
</ol>

        <h4 id="启动-Yarn"   >
          <a href="#启动-Yarn" class="heading-link"><i class="fas fa-link"></i></a>启动 Yarn</h4>
      <ol>
<li>启动 YARN，jps 查看是否有相关进程。<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">start-yarn.sh</span><br></pre></td></tr></table></div></figure></li>
<li>打开 localhost:8088 可以看到 Yarn 的 web UI</li>
<li>测试运行 Map Reduce 程序，一开始运行了几次都不成功，UI 上查看日志显示 /bin/java 文件或者目录不存在。需要配置 hadoop-env.sh，yaer-env.sh，mapred-env.sh 中的 JAVA_HOME。<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/<span class="built_in">test</span>/input  output</span><br></pre></td></tr></table></div></figure>

</li>
</ol>

        <h3 id="Docker-搭建-Hadoop-集群（WIP）"   >
          <a href="#Docker-搭建-Hadoop-集群（WIP）" class="heading-link"><i class="fas fa-link"></i></a>Docker 搭建 Hadoop 集群（WIP）</h3>
      
        <h4 id="构建基础镜像，组装服务"   >
          <a href="#构建基础镜像，组装服务" class="heading-link"><i class="fas fa-link"></i></a>构建基础镜像，组装服务</h4>
      <p>一开始的想法很简单,想的就是 docker-compose 构建三个 centos 容器并组网。emmmm，其实没有考虑容易互联，通过 link 和 expose 开放端口的方式还没有尝试。这里先存下目前处理的东西</p>
<ol>
<li><p>编写 Dockerfile 构建基础镜像，安装 jdk，vim， curl， wget， ntp等搭建环境所需软件包。</p>
</li>
<li><p>根据基础镜像，docker-compose 组装服务</p>
</li>
</ol>

        <h4 id="集群配置分发"   >
          <a href="#集群配置分发" class="heading-link"><i class="fas fa-link"></i></a>集群配置分发</h4>
      <p>编写分发脚本，rsync 或者 scp，一般是 rsync，速度快，只同步差异。emmm，编写的脚本也比较简单，按规律循环遍历节点，执行 rsync，接收要分发的内容作为参数</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> [[ <span class="variable">$#</span> -eq 0 ]]; <span class="keyword">then</span></span><br><span class="line">   <span class="built_in">echo</span> <span class="string">&quot;no params&quot;</span>;</span><br><span class="line">   <span class="built_in">exit</span>;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">path=<span class="variable">$1</span></span><br><span class="line"></span><br><span class="line">file_name=`basename <span class="variable">$path</span>`</span><br><span class="line"></span><br><span class="line">dir=`<span class="built_in">cd</span> -P $(dirname <span class="variable">$&#123;file_name&#125;</span>); <span class="built_in">pwd</span>`</span><br><span class="line"></span><br><span class="line">user=`whoami`</span><br><span class="line"><span class="keyword">for</span>((host=1; host&lt;4; host++)); <span class="keyword">do</span></span><br><span class="line"><span class="built_in">echo</span> ------------------- hadoop<span class="variable">$host</span> --------------</span><br><span class="line">rsync -rvl <span class="variable">$dir</span><span class="variable">$file_name</span> <span class="variable">$user</span>@hadoop<span class="variable">$host</span>:<span class="variable">$dir</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></div></figure>

        <h4 id="SSH-配置"   >
          <a href="#SSH-配置" class="heading-link"><i class="fas fa-link"></i></a>SSH 配置</h4>
      <p>参考 <a href="">配置SSH</a></p>

        <h4 id="NTP-时间同步"   >
          <a href="#NTP-时间同步" class="heading-link"><i class="fas fa-link"></i></a>NTP 时间同步</h4>
      <p>设置一台服务器作为时间服务器，其他服务器定期从时间服务器同步时间，保证多台结点时间一致。参考 <span class="exturl"><a class="exturl__link"   href="https://blog.51cto.com/14259167/2427537" >ntp 时间服务器同步</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h4 id="启动整个集群"   >
          <a href="#启动整个集群" class="heading-link"><i class="fas fa-link"></i></a>启动整个集群</h4>
      <p>修改 Hadoop 配置文件，emmm，按照上面伪分布式搭建的修改就行，只不多 ResourceManager, default.FS 等需要替换成我们部署该进程的节点域名。并且编写 /etc/slaves用于指定该集群的子节点。所有修改完毕后，分发脚本。<br><code>start-dfs.sh</code> 启动 HDFS 集群，注意要在启动 NodeManager 的节点上执行<br><code>start-yarn.sh</code> 启动 yarn 集群，注意要在启动 ResourceManager 的节点上执行<br><code>mr-jobhistory-daemon.sh start historyserver</code> 启动历史服务器，主要要在 historyserver 的节点上执行</p>

        <h4 id="注意事项"   >
          <a href="#注意事项" class="heading-link"><i class="fas fa-link"></i></a>注意事项</h4>
      <ul>
<li>只有在第一次启动的时候，执行 hadoop namenode format，不要多次执行</li>
</ul>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop是什么</title>
    <url>/posts/19243a02/</url>
    <content><![CDATA[
        <h3 id="Hadoop"   >
          <a href="#Hadoop" class="heading-link"><i class="fas fa-link"></i></a>Hadoop</h3>
      <p>Hadoop 是海量数据分布式存储和计算框架，脱身于 Google 三大论文。现在我们常说的 Hadoop 往往指的是 Hadoop 生态圈。</p>
<a id="more"></a>
<p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gdnrjl629wj30gj0c9ta9.jpg"></p>

        <h3 id="Hadoop-1-x-与-2-x"   >
          <a href="#Hadoop-1-x-与-2-x" class="heading-link"><i class="fas fa-link"></i></a>Hadoop 1.x 与 2.x</h3>
      <p>Hadoop 1.x 中的 Map Reduce用于资源调度和分布式计算。Hadoop 2.x 引入了 YARN 用于资源调度，Map Reduce 只用于分布式计算。相当于对解耦合。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gdnrsp6wgej30is0d474q.jpg"></p>

        <h3 id="HDFS"   >
          <a href="#HDFS" class="heading-link"><i class="fas fa-link"></i></a>HDFS</h3>
      <p>HDFS 分布式文件存储系统。其主要组成分为 NameNode， DataNode 和 Secondary NameNode。<br>NameNode：元数据节点，元数据也就是表述数据的数据，比如文件的存放位置，大小等等。元数据一般驻留在内存中，所以大量的碎片文件可能会导致 NameNode 的 OOM。这是一个需要注意的地方。另外 NameNode 执行格式化，重启的时候，都要考虑清楚对 DataNode 的影响。<br>DataNode：数据节点，真正存放数据的节点。<br>Secondary NameNode：NameNode 的辅助接点，类似 CheckPoint 的作用。NmaeNode 启动时会生成一个系统快照，启动之后的文件改动信息会记录到日志当中。当重启时，会将上一次的快照和改动日志进行合并生成一个新的系统快照。通过这样来保证每次启动时系统快照都是最新的。但是 NameNode 不能经常重启，改动日志会变的比较大，如果下次重启 NameNode ，改动合并过程也可能导致重启过慢。这时候就需要 Secondary NameNode 了，它会定时查询改动日志，合并 NmaeNode 启动时的快照，在传回 NmaeNode。</p>

        <h3 id="YARN"   >
          <a href="#YARN" class="heading-link"><i class="fas fa-link"></i></a>YARN</h3>
      <p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gdnsgytadbg30ha0ap74w.gif"></p>
<p>上面这张图展现了 YARN Appliaction 的调度运行过程。</p>
<ul>
<li>客户端提交任务</li>
<li>Resource Manager，接收 Job 提交的请求，分配资源</li>
<li>NodeManager 监控该节点的整体资源状态，并像 Resource Manager 汇报。</li>
<li>启动 ApplicationMastr，ApplicationMastr 协调管理制定 Job 运行的 Container。</li>
</ul>

        <h3 id="后记"   >
          <a href="#后记" class="heading-link"><i class="fas fa-link"></i></a>后记</h3>
      <p>个人接触大数据已经半年多了，但是仅局限于 Spark SQL。其他东西用的还是蛮少的，甚至不知道东西该用在什么地方。Apache 这一套东西到底是怎么互相结合的，没有一个层次的概念。这里根据这几天看到的记录一下。<br>数据采集层：结构化数据， 半结构化数据，非结构化数据<br>数据传输层：Sqoop处理结构化数据，比如 MySQL 等，Flume日志收集，Kfaka 收集非结构化数据消息直接进行分析，比如视频，音频等。<br>数据存储层：HDFS，HBase等。<br>元数据管理层：atlas 管理整体的元数据，数据血缘追溯<br>数据分析计算层：Spark + Flink，离线计算和实时计算，而且提供了 MLLib 用于数据挖掘和机器学习。<br>任务调度层：Azkaban 任务调度<br>配置管理层：Zookeeper 管理整体配置，节点命名等。<br>可视化层：kylin，clickhouse 分析结果可视化，报表开发，还有 Doris</p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive的简单安装与配置</title>
    <url>/posts/1c5fdd95/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>WIP</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Iterm2 美化</title>
    <url>/posts/ec25acc0/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>工欲善其事，必先利其器。Iterm2 是 Mac 下的终端利器，支持标签变色，命令自动补全，智能选中，与 tmux 集成，大量易用的快捷键，通过配置 profile 可以快捷的登录到多个 remote 终端。</p>
<a id="more"></a>


        <h3 id="Iterm2-主题配置"   >
          <a href="#Iterm2-主题配置" class="heading-link"><i class="fas fa-link"></i></a>Iterm2 主题配置</h3>
      <p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1gileqar91oj31d90u0nph.jpg"></p>

        <h4 id="oh-my-zsh"   >
          <a href="#oh-my-zsh" class="heading-link"><i class="fas fa-link"></i></a>oh my zsh</h4>
      <p>on my zsh 通过提供开源的配置，只需要简单的修改配置文件就能增添插件，修改样式。安装如下</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">brew install zsh</span><br><span class="line">sh -c &quot;$(curl -fsSL https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;robbyrussell&#x2F;oh-my-zsh&#x2F;master&#x2F;tools&#x2F;install.sh)&quot;</span><br></pre></td></tr></table></div></figure>
<p>安装完成后，会在用户家目录下生成 <code>~/.zshrc</code>（zsh 配置文件） 和 <code>.oh-my-zsh</code>（主题和插件的存放目录）。</p>
<p>tips：如果发现切换到 zsh 后，少了一些环境变量，可以直接在 <code>~/.zshrc</code> 开头加入 <code>source ~/.bash_profile</code>。</p>

        <h4 id="常用插件"   >
          <a href="#常用插件" class="heading-link"><i class="fas fa-link"></i></a>常用插件</h4>
      <ul>
<li>Git 在主机名够显示 git 项目信息，比如分值，状态等</li>
<li>zsh-syntax-highlighting 高亮显示常见的命令，在命令输错时，会报红</li>
<li>zsh-autosuggestions 命令自动补全，输入命令时，会灰色显示出推荐命令，按右方向键即可补全。<br>有些插件需要安装，下载下来后直接放到 <code>~/.oh-my-zsh/custom/plugins</code> 即可，比如 <code>zsh-autosuggestions</code>。</li>
</ul>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git://github.com/zsh-users/zsh-autosuggestions  ~/.oh-my-zsh/custom/plugins</span><br></pre></td></tr></table></div></figure>
<p>然后修改 <code>~/.zshrc</code> 的 pluging 配置</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">plugins&#x3D;(git z zsh-syntax-highlighting zsh-autosuggestions)</span><br></pre></td></tr></table></div></figure>
<p>最后当然是重新 source 一下了</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">source ~&#x2F;.zshrc</span><br></pre></td></tr></table></div></figure>


        <h4 id="主题配置"   >
          <a href="#主题配置" class="heading-link"><i class="fas fa-link"></i></a>主题配置</h4>
      <p>可以在 <span class="exturl"><a class="exturl__link"   href="https://github.com/ohmyzsh/ohmyzsh/wiki/Themes" >oh-my-zsh 主题列表</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 里选用自己喜欢的终端样式，然后修改 <code>~/.zshrc</code>。</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">ZSH_THEME&#x3D;&quot;ys&quot;</span><br></pre></td></tr></table></div></figure>
<p>修改主题配色，这里用的是 <span class="exturl"><a class="exturl__link"   href="https://draculatheme.com/iterm/" >Dracula</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>，下载对应的主题文件，然后导入到 Iterm2 中。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1gilejtsgcej315x0u0b29.jpg"></p>

        <h4 id="背景配置"   >
          <a href="#背景配置" class="heading-link"><i class="fas fa-link"></i></a>背景配置</h4>
      <p>可以在 profile -&gt; window 中配置终端背景图片，然后自己调节一下终端透明度 Transparency<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1gilekuryhuj31fe0t4drb.jpg"></p>

        <h4 id="status-bar"   >
          <a href="#status-bar" class="heading-link"><i class="fas fa-link"></i></a>status bar</h4>
      <p>逼格比较高，在 profile -&gt; session 中启用 status bar 并配置。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1gileoqd2ifj31ek0teq9z.jpg"></p>
<p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1gileork71nj31a90u0ahm.jpg"> </p>

        <h3 id="sshpass-proilfe-快速登录远程终端"   >
          <a href="#sshpass-proilfe-快速登录远程终端" class="heading-link"><i class="fas fa-link"></i></a>sshpass + proilfe 快速登录远程终端</h3>
      <p>sshpass 用于在命令中直接提供服务器密码，而不用通过交互式输入。Iterm2  profile 中可以设置打开窗口时执行的 command。两者结合就可以实现快速登入远程服务器。</p>

        <h4 id="安装-sshpass"   >
          <a href="#安装-sshpass" class="heading-link"><i class="fas fa-link"></i></a>安装 sshpass</h4>
      <ul>
<li><span class="exturl"><a class="exturl__link"   href="https://sourceforge.net/projects/sshpass/files/" >sshpass 下载</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li>解压缩后，进入到 <code>sshpass</code> 目录</li>
<li>执行 <code>./configure</code></li>
<li>执行 <code>sudo make install</code></li>
<li>sshpass</li>
</ul>
<p>sshpass 的简单使用</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">&#x2F;usr&#x2F;local&#x2F;bin&#x2F;sshpass -p &#39;你的密码&#39; ssh user@host</span><br></pre></td></tr></table></div></figure>

        <h4 id="配置-profile"   >
          <a href="#配置-profile" class="heading-link"><i class="fas fa-link"></i></a>配置 profile</h4>
      <p>在 profile  command 中输入命令即可<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1gilf1rwxbkj318g0u0do6.jpg"></p>

        <h4 id="快速连接"   >
          <a href="#快速连接" class="heading-link"><i class="fas fa-link"></i></a>快速连接</h4>
      <p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1gilgch0xhlj316o0aakbg.jpg"></p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title>Java安装</title>
    <url>/posts/c032fe54/</url>
    <content><![CDATA[
        <h3 id="为什么-Java-8-仍是主流"   >
          <a href="#为什么-Java-8-仍是主流" class="heading-link"><i class="fas fa-link"></i></a>为什么 Java 8 仍是主流</h3>
      <p>如今 Java 已经出到了 14，为啥子大家还是在用 Java 8。你有没有为这个困惑过呢。其实接受新事物都有这样的规律，一是新事物有足够的吸引力，大家主动去追求。二是旧事物被强制扼杀，只能转向新事物。</p>
<a id="more"></a>

        <h3 id="安装-JDK"   >
          <a href="#安装-JDK" class="heading-link"><i class="fas fa-link"></i></a>安装 JDK</h3>
      <ul>
<li>命令安装<br>寻找你想安装的 Java 版本，并查看<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">brew search java</span><br><span class="line">brew search jdk</span><br><span class="line">brew info  java</span><br></pre></td></tr></table></div></figure>
如果没有你想安装的 Java 版本，可以更新一下 brew 仓库，或者利用 <span class="exturl"><a class="exturl__link"   href="https://segmentfault.com/a/1190000012826983" >brew tap</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 添加第三方库<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">brew tap homebrew/cask-versions</span><br></pre></td></tr></table></div></figure>
安装 JDK 8<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">brew cask install homebrew/cask-versions/adoptopenjdk8</span><br></pre></td></tr></table></div></figure>
官网下载<br><span class="exturl"><a class="exturl__link"   href="https://www.oracle.com/java/technologies/javase-downloads.html" >下载地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>，找到自己想要的版本，下载对应的 dmg 文件，跟着一步步走就好了。</li>
</ul>

        <h3 id="配置-JDK"   >
          <a href="#配置-JDK" class="heading-link"><i class="fas fa-link"></i></a>配置 JDK</h3>
      <p>这个见多了，大数据相关组件都要配置一个 JAVA_HOME，编写 ~/.bash_profile</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="comment"># JAVA</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=<span class="variable">$JAVA_HOME</span>/lib/tools.jar:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:.</span><br></pre></td></tr></table></div></figure>
<p>如果存在多个 Java 版本怎么办呢，可以编写 ~/.bash_profile 如下，这样的话，在终端下通过 jdk10 和 jdk8 命令便可以切换版本。不过，需要注意，这样切换并不会切换默认版本，只在当前终端下有用。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_8_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home</span><br><span class="line"><span class="built_in">export</span> JAVA_10_HOME=/Library/Java/JavaVirtualMachines/jdk-10.0.2.jdk/Contents/Home</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=<span class="variable">$JAVA_8_HOME</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=<span class="variable">$JAVA_HOME</span>/lib/tools.jar:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:.</span><br><span class="line"></span><br><span class="line"><span class="built_in">alias</span> jdk10=<span class="string">&quot;export JAVA_HOME=<span class="variable">$JAVA_10_HOME</span>&quot;</span></span><br><span class="line"><span class="built_in">alias</span> jdk8=<span class="string">&quot;export JAVA_HOME=<span class="variable">$JAVA_8_HOME</span>&quot;</span></span><br></pre></td></tr></table></div></figure>

        <h3 id="learn-by-the-way"   >
          <a href="#learn-by-the-way" class="heading-link"><i class="fas fa-link"></i></a>learn by the way</h3>
      <p>说实话，以前配置环境变量都是跟着模子一起配，没有细究过。这里面的 PATH $PATH $PATH: 到底是啥东西呢？<br>PATH - 可执行程序的搜索路径，通过 <code>echo $PATH</code> 可以查看。<br>$PATH: - 在当前环境变量下追加新的环境变量，一次多个的话可以用冒号分隔。</p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>MR Job Counters 分析</title>
    <url>/posts/d0259ab8/</url>
    <content><![CDATA[
        <h4 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h4>
      <p>MapReduce 的 Counters 对任务执行分析有一定帮助，本文主要记录其中参数的含义及一些使用。</p>
<a id="more"></a>]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Mac-Homebrew-常见问题</title>
    <url>/posts/c670d00d/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>Homebrew 是 Mac 下方便快捷的包管理器。但是有时候因为其版本迭代等，导致 <code>brew update</code> 执行后各种依赖报错或者 Warning。emmm，碰到好几次了，并且由于网上解决办法参差不齐，每次解决浪费了大量时间。遂记录下每次的解决方法。建议遇到问题去查看官方 issue</p>
<a id="more"></a>


        <h3 id="问题"   >
          <a href="#问题" class="heading-link"><i class="fas fa-link"></i></a>问题</h3>
      <p>建议遇到问题去查看官方 issue，你踩过的坑一般都已经有人踩过了。如果还不行，可以自己提 issue，找专业的人去解决效率会更高。<br>查询问题可以运用这几个命令，查看目前环境信息和 warning。</p>
<ul>
<li><code>brew config</code></li>
<li><code>brew doctor</code></li>
<li><code>brew update-reset</code></li>
<li><code>brew tap</code> -&gt; 重新关联相关源，一般删除对应源之后执行这个。<br>Homebrew 安装在 /usr/local 下面，其中 Taps 下面是一些核心库，比如 homebrew-core，homebrew-cask 等，本质上就是一个个的 git 仓库。</li>
</ul>

        <h4 id="Warning-Calling-cellar-in-a-bottle-block-is-deprecated-Use-brew-style-–fix-on-the-formula-to-update-the-style-or-use-sha256-with-a-cellar-argument-instead"   >
          <a href="#Warning-Calling-cellar-in-a-bottle-block-is-deprecated-Use-brew-style-–fix-on-the-formula-to-update-the-style-or-use-sha256-with-a-cellar-argument-instead" class="heading-link"><i class="fas fa-link"></i></a>Warning: Calling cellar in a bottle block is deprecated! Use brew style –fix on the formula to update the style or use sha256 with a cellar: argument instead.</h4>
      <p>在 homebrew-core 提了 issue 才解决的。可以参考 <span class="exturl"><a class="exturl__link"   href="https://github.com/Homebrew/homebrew-core/issues/77342" >issue#77342</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>。<br>在安装 grafana 的时候碰到的这个问题。<code>brew update</code> 也没报错，就是大量的 Warning。根据提示执行了 <code>brew style --fix</code>，显示 ruby 环境有点问题。<br>后面就是所有 brew 命令报这种错，并且更新包都不成功。<br>查看 <code>brew config </code>，homebrew-core 版本过低。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">HOMEBREW_VERSION: 3.1.7-36-g7c68b17</span><br><span class="line">ORIGIN: https://github.com/Homebrew/brew</span><br><span class="line">HEAD: 7c68b1738b3dce2885d0146f327eaaf96b6d0029</span><br><span class="line">Last commit: 2 days ago</span><br><span class="line">Core tap ORIGIN: https://github.com/Homebrew/homebrew-core</span><br><span class="line">Core tap HEAD: bf34b4a87af8acac55d95f133a8b56a627a28557</span><br><span class="line">Core tap last commit: 5 months ago</span><br><span class="line">Core tap branch: master</span><br><span class="line">HOMEBREW_PREFIX: /usr/<span class="built_in">local</span></span><br><span class="line">HOMEBREW_CASK_OPTS: `[]`</span><br><span class="line">HOMEBREW_DISPLAY: /private/tmp/com.apple.launchd.jiJkB9eSrz/org.macosforge.xquartz:0</span><br><span class="line">HOMEBREW_MAKE_JOBS: 8</span><br><span class="line">Homebrew Ruby: 2.6.3 =&gt; /System/Library/Frameworks/Ruby.framework/Versions/2.6/usr/bin/ruby</span><br><span class="line">CPU: octa-core 64-bit kabylake</span><br><span class="line">Clang: 12.0.5 build 1205</span><br><span class="line">Git: 2.29.2 =&gt; /usr/<span class="built_in">local</span>/bin/git</span><br><span class="line">Curl: 7.64.1 =&gt; /usr/bin/curl</span><br><span class="line">macOS: 11.2.3-x86_64</span><br><span class="line">CLT: 12.5.0.0.1.1617976050</span><br><span class="line">Xcode: 10.2.1</span><br></pre></td></tr></table></div></figure>
<p>第一次使用 <code>brew update-reset</code>，没有指定仓库，更新了 Taps 下所有仓库都到最新提交位置。显示成功，但是不知道为啥，单单 homebrew-core 没有 pull。<br>第二次先删除源，重新 <code>brew tap</code> 关联后成功。重新执行各种 brew 命令，不在报 Warning。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">rm -rf $(brew --repo homebrew/core)</span><br><span class="line">brew tap homebrew/core</span><br></pre></td></tr></table></div></figure>


]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title>Mac System Sleep Weak Failure</title>
    <url>/posts/dbfe9aba/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>最近更新了 mac 系统到 Catalina 10.15.6，然后碰到了一个鬼问题 System sleep weak failure。mac 在睡眠一段时间后，无法唤醒，然后就自动重启。一开始没在意，后来实在受不了每天都重启电脑，然后又重新打开所有应用，怎么办嘞，找办法解决。</p>
<a id="more"></a>


        <h3 id="Solve-System-sleep-weak-failure"   >
          <a href="#Solve-System-sleep-weak-failure" class="heading-link"><i class="fas fa-link"></i></a>Solve System sleep weak failure</h3>
      <p>看到最多的办法是重置，然而对我并没有啥用。这里也放上了，万一试试对你有用嘞。</p>
<ul>
<li><span class="exturl"><a class="exturl__link"   href="https://support.apple.com/zh-cn/HT204063" >重置 NVRAM</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><span class="exturl"><a class="exturl__link"   href="http://support.apple.com/zh-cn/HT201295" >重置 SMC</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br>最终的解决办法是通过 pmset 重置了 sleep 时间。pmset 用于电源管理相关的设置，在系统偏好设置-节能-电源可以看到其一些设置选项，但是 pmset 更加灵活。<br>首先 <code>pmset -g custom</code> 看下目前电脑电源相关设置<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1giay085zyhj30s411maeu.jpg"></li>
</ul>
<p>正常的话，应该是 sleep ≥ displaysleep ≥ disksleep。我这里 sleep 为 0 代表被禁用了（emmm，好像是我自己在节能里设置里不让电脑自动进入睡眠，设置之前也有 System sleep weak failure）。</p>
<ul>
<li>sleep -&gt; mac 闲置多长时间后进入睡眠</li>
<li>displaysleep -&gt; mac 闲置多长时间后显示器进入睡眠</li>
<li>disksleep -&gt; mac 闲置多长时间后硬盘进入睡眠<br>打开 sleep ，并按照正常顺序设置。 <code>sudo pmset -a sleep 15</code>。第二天早上，打开电脑终于没了 System sleep weak failure。问题解决。</li>
</ul>
<p>不过解决的方式有点迷糊，这东西产生的原因也不清楚，就当做一次记录吧。</p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title>Mac 上 VmwareFusion配置静态 IP</title>
    <url>/posts/f7cba8b0/</url>
    <content><![CDATA[
        <h3 id="背景"   >
          <a href="#背景" class="heading-link"><i class="fas fa-link"></i></a>背景</h3>
      <p>由于学习原因，在 mac 上下了 Vmware Fusion 构建虚拟机搭建集群。（emnn，我也想用 docker 呀，奈何水平不大够）。配置静态 IP 的时候，发现和 windows 上的一点也不一样，碰到了一些问题，遂记录下来。</p>
<a id="more"></a>


        <h3 id="Vmware-Fusion-配置静态IP"   >
          <a href="#Vmware-Fusion-配置静态IP" class="heading-link"><i class="fas fa-link"></i></a>Vmware Fusion 配置静态IP</h3>
      <ol>
<li><p>点击虚拟机窗口，修改网络适配器设置，改为 net 模式<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlgy1gg961uax3fj311m0oqage.jpg"></p>
</li>
<li><p>查看 Mac 本机的网络配置</p>
<ul>
<li><p>进入 Vmaware Fusion 的 vmnet8 目录，</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /Library/Preferences/VMware\ Fusion/vmnet8</span><br></pre></td></tr></table></div></figure>
<p><code>less nat.conf</code> 查看 nat.conf。其中的 NET gateway address 中的 ip 就是本机网关地址，netmask 是子网掩码。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlgy1gg96l82g99j31bc0s00wy.jpg"></p>
<p><code>less dhcpd.conf</code> 查看 dhcpd.conf。其中的 range 代表虚拟机允许选择的惊静态 ip 地址范围，我这里的范围就是 172.16.242.128 ~ 172.16.242.254<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlgy1gg96ptija2j30w60ac402.jpg"></p>
</li>
<li><p>获取 DNS，在 mac 系统偏好设置 -&gt; 网络 -&gt; 高级 -&gt; DNS<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlgy1gg96s7pdfij30zs0n6amb.jpg"></p>
</li>
</ul>
</li>
</ol>
<ol start="3">
<li><p>登录你装的虚拟机系统，修改 /etc/sysconfig/network-scripts 目录下的 ifcfg-en 开头的文件。修改如下，修改的内容主要有</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">BOOTPROTO&#x3D;static</span><br><span class="line">ONBOOT&#x3D;yes</span><br><span class="line">IPADDR&#x3D;172.16.242.130  -&gt; 你要设置的静态 IP</span><br><span class="line">GATEWAY&#x3D;172.16.242.2 -&gt; 上面第二步获取的本机网关地址</span><br><span class="line">NETMASK&#x3D;255.255.255.0 -&gt; 上面第二步获取的子网掩码</span><br><span class="line">DNS1&#x3D;210.22.84.3 -&gt; 上面第二步获取的 DNS，这里可以配置多个 DNS，比如下面在加个 DNS2</span><br></pre></td></tr></table></div></figure>
<p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlgy1gg96v7nn0kj30j80i4tax.jpg"></p>
</li>
<li><p>重启 network 服务， service network restart</p>
</li>
<li><p>至此，静态 IP 配置已经 OK 了。只要虚拟机开启，你就可以直接用 iTerm ssh 直连虚拟机，而不用进到 vmware fusion 打开的终端。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title>一些效率软件及有意思的东西</title>
    <url>/posts/7a64a955/</url>
    <content><![CDATA[
        <h4 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h4>
      <p>介绍一下目前用到的效率插件及一些有意思的东西。</p>
<a id="more"></a>


        <h4 id="插件列表"   >
          <a href="#插件列表" class="heading-link"><i class="fas fa-link"></i></a>插件列表</h4>
      <ul>
<li>工具网站汇总<ul>
<li><span class="exturl"><a class="exturl__link"   href="https://xclient.info/" >xclient.info</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> mac 破解</li>
<li><span class="exturl"><a class="exturl__link"   href="http://free.apprcn.com/" >反斗限免</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 限免 app</li>
<li><span class="exturl"><a class="exturl__link"   href="https://sbvpn.xyz/" >sbvpn.xyz</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 翻墙</li>
<li><span class="exturl"><a class="exturl__link"   href="https://github.com/ruanyf/weekly/blob/master/docs" >科技爱好者周刊</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>工具分享</li>
<li><span class="exturl"><a class="exturl__link"   href="https://wallhaven.cc/" >wallhaven.cc</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>  壁纸</li>
<li><span class="exturl"><a class="exturl__link"   href="https://simpleicons.org/" >simpleicons.org</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>  图标，矢量图</li>
</ul>
</li>
<li>Chrome<ul>
<li><span class="exturl"><a class="exturl__link"   href="https://chrome.google.com/webstore/detail/vimium/dbepggeogbaibhgnhhndojpepiihcmeb?page=1&hl=zh_CN&itemlang=sl" >Vimium</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 使用 vim 命令快速操作定位网页</li>
<li><span class="exturl"><a class="exturl__link"   href="https://www.octotree.io/" >Octotree</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> github 代码浏览</li>
<li><span class="exturl"><a class="exturl__link"   href="https://chrome.google.com/webstore/detail/%E5%B9%BF%E5%91%8A%E7%BB%88%E7%BB%93%E8%80%85/fpdnjdlbdmifoocedhkighhlbchbiikl" >ADBlock</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 广告终结者</li>
<li><span class="exturl"><a class="exturl__link"   href="https://github.com/tulios/json-viewer" >Json Viewer</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> Json 可视化 </li>
<li><span class="exturl"><a class="exturl__link"   href="https://chrome.google.com/webstore/detail/proxy-switchyomega/padekgcemlokbadohgkifijomclgjgif" >SwitchyOmega</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 代理插件</li>
<li><span class="exturl"><a class="exturl__link"   href="https://chrome.google.com/webstore/detail/google-translate" >Google</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 翻译 </li>
<li><span class="exturl"><a class="exturl__link"   href="https://chrome.google.com/webstore/detail/tampermonkey" >Tampermonkey</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>  油猴脚本管理，浏览器开挂</li>
</ul>
</li>
<li>Sublime Text<ul>
<li><span class="exturl"><a class="exturl__link"   href="https://packagecontrol.io/packages/A%20File%20Icon" >A File Icon</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>  常见文件格式图标</li>
<li><span class="exturl"><a class="exturl__link"   href="https://packagecontrol.io/packages/SqlBeautifier" >SqlBeautiful</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 格式化 SQL</li>
<li><span class="exturl"><a class="exturl__link"   href="https://packagecontrol.io/packages/SideBarEnhancements" >SideBarEnhancements</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 增强右键功能</li>
<li><span class="exturl"><a class="exturl__link"   href="https://packagecontrol.io/packages/Agila%20Theme" >Agila</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> agila theme</li>
<li><span class="exturl"><a class="exturl__link"   href="https://github.com/SublimeCodeIntel/SublimeCodeIntel" >SublimeCodeIntel</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>  代码跳转，自动提示</li>
</ul>
</li>
<li>Iterm2<ul>
<li><span class="exturl"><a class="exturl__link"   href="https://gist.github.com/arunoda/7790979" >sshpass</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 免密直连远程服务器</li>
<li><span class="exturl"><a class="exturl__link"   href="https://github.com/ohmyzsh/ohmyzsh" >ohmyzsh zsh</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 终端<ul>
<li><span class="exturl"><a class="exturl__link"   href="https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/git" >git</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 显示分支等 git 信息</li>
<li><span class="exturl"><a class="exturl__link"   href="https://github.com/wbingli/zsh-wakatime.git" >zsh-wakatime</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> wakatime hook</li>
<li><span class="exturl"><a class="exturl__link"   href="https://github.com/zsh-users/zsh-syntax-highlighting.git" >zsh-syntax-highlighting</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 命令高亮</li>
<li><span class="exturl"><a class="exturl__link"   href="https://github.com/zsh-users/zsh-autosuggestions" >zsh-autosuggestions</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 历史命令自动补全，不限于系统命令</li>
<li><span class="exturl"><a class="exturl__link"   href="https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/z" >z</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 历史浏览目录快速跳转，不必输入绝对路径</li>
</ul>
</li>
</ul>
</li>
<li>其他<ul>
<li><span class="exturl"><a class="exturl__link"   href="https://greasyfork.org/zh-CN/scripts/23635-%E7%99%BE%E5%BA%A6%E7%BD%91%E7%9B%98%E7%9B%B4%E6%8E%A5%E4%B8%8B%E8%BD%BD%E5%8A%A9%E6%89%8B" >ipic</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 免费图床</li>
<li>github repo release，像 shadowsocks，RDM，v2ray 都可以直接在 github 对应的项目组的 release 中找到。</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title>Mac重装系统找不到磁盘主盘，无法抹掉</title>
    <url>/posts/23b10311/</url>
    <content><![CDATA[
        <h4 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h4>
      <p>最近打算把自己 Mac 卖掉，重装系统碰到了个问题，搞了一天多才搞定，遂记录下。具体是在线重装系统进入到磁盘工具后，找不到主盘，只有一个不到 3G 的 disk0，无法抹掉主盘上的数据且重装系统的时候也识别不到主盘。和<span class="exturl"><a class="exturl__link"   href="https://www.jianshu.com/p/69346847efd0" >这个问题</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>比较类似，不过解决办法真是扯了，网上都是千篇一律，说不清楚，根本不能解决😑 。</p>
<a id="more"></a>
<p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/008eGmZEly1gnrhr55qq7j31400u0myu.jpg"><br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/008eGmZEly1gnrhr52lkxj31400u0dhc.jpg"></p>

        <h4 id="解决办法"   >
          <a href="#解决办法" class="heading-link"><i class="fas fa-link"></i></a>解决办法</h4>
      <p>这个问题的本质其实就是 Macintosh HD 识别不到，直接进入系统，打开磁盘工具查看，发现 Macintosh HD 是灰色的，显示未装载，运行急救不管用。Macintosh HD - 数据正常。</p>
<p>command + R 进入到恢复模式磁盘工具下，发现根本找不到 Macintosh HD。此处，还偶尔碰到一个 -5010F 错误。</p>
<p>改用 U 盘引导重装，在才进入到磁盘工具下，发现 Macintosh HD 为灰色，此处建议直接抹掉灰色显示的宗卷。抹掉后，再次进入到重新安装 OS 界面，此时已经可以识别出 Macintosh HD ，剩下的就是跟着提示安装了。</p>
<p>关于制作系统盘，可以直接查看官方支持文档 - <span class="exturl"><a class="exturl__link"   href="https://support.apple.com/zh-cn/HT201372" >如何创建可引导的 macOS 安装器</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title>MarkDown 文档常用的语法</title>
    <url>/posts/4092064b/</url>
    <content><![CDATA[
        <h3 id="背景"   >
          <a href="#背景" class="heading-link"><i class="fas fa-link"></i></a>背景</h3>
      <p>在使用 MarkDown 语法书写文档的过程中，经常遇到一些场景不会表达，每次都去搜索，太浪费时间。遂记录下来放在本篇文档中。不定期更新</p>
<a id="more"></a>

        <h3 id="文字加粗"   >
          <a href="#文字加粗" class="heading-link"><i class="fas fa-link"></i></a>文字加粗</h3>
      <p>使用 **(加粗文字)**，就像这样，你的名字</p>

        <h3 id="表格中代码换行"   >
          <a href="#表格中代码换行" class="heading-link"><i class="fas fa-link"></i></a>表格中代码换行</h3>
      <p>表格中书写代码示例的时候，由于没有换行，代码较短还可以，但是太长的话基本上就不知所云了。就像下面这样，虽然用 ; 做了分隔，但是看起来还是比较别扭。</p>
<div class="table-container"><table>
<thead>
<tr>
<th>算子</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td>map</td>
<td>case class Sentence(id: Long, text: String);val sentences = Seq(Sentence(0, “hello world”), Sentence(1, “witaj swiecie”)).toDS;sentences.map(s =&gt; s.text.length &gt; 12).show()</td>
</tr>
</tbody></table></div>
<p>这个时候可以采用 <br> 标签</p>
<div class="table-container"><table>
<thead>
<tr>
<th>算子</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td>map</td>
<td><code>case class Sentence(id: Long, text: String)</code><br><code>val sentences = Seq(Sentence(0, &quot;hello world&quot;), Sentence(1, &quot;witaj swiecie&quot;)).toDS</code><br><code>sentences.map(s =&gt; s.text.length &gt; 12).show()</code><br></td>
</tr>
</tbody></table></div>

        <h3 id="代码段折叠"   >
          <a href="#代码段折叠" class="heading-link"><i class="fas fa-link"></i></a>代码段折叠</h3>
      <p>使用 <code>&lt;details&gt;&lt;summary&gt;&lt;/summary&gt;&lt;/details&gt;</code> 实现。比如下面这样，<code>&lt;summary&gt;</code> 标签代表折叠代码块的摘要。 <code>&lt;summary&gt;</code> 标签下面的内容代表你要折叠的文本块或者代码块。</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">details</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">summary</span>&gt;</span>sales.csv<span class="tag">&lt;/<span class="name">summary</span>&gt;</span></span><br><span class="line">	tem, modelnumber, price, tax <span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">	Sneakers, MN009, 49.99, 1.11<span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">	Sneakers, MTG09, 139.99, 4.11<span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">	Shirt, MN089, 8.99, 1.44<span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">	Pants, N09, 39.99, 1.11<span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">	Sneakers, KN09, 49.99, 1.11<span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">	Shoes, BN009, 449.22, 4.31<span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">	Sneakers, dN099, 9.99, 1.22<span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">	Bananas, GG009, 4.99, 1.11<span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">details</span>&gt;</span></span><br></pre></td></tr></table></div></figure>
<p>最终实现的效果是这样子的</p>
<details>
    <summary>sales.csv</summary>
    tem, modelnumber, price, tax <br>
    Sneakers, MN009, 49.99, 1.11<br>
    Sneakers, MTG09, 139.99, 4.11<br>
    Shirt, MN089, 8.99, 1.44<br>
    Pants, N09, 39.99, 1.11<br>
    Sneakers, KN09, 49.99, 1.11<br>
    Shoes, BN009, 449.22, 4.31<br>
    Sneakers, dN099, 9.99, 1.22<br>
    Bananas, GG009, 4.99, 1.11<br>
</details>


        <h3 id="控制图片大小"   >
          <a href="#控制图片大小" class="heading-link"><i class="fas fa-link"></i></a>控制图片大小</h3>
      <p>通过 <code>&lt;img&gt;</code> 标签</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;https://spark.apache.org/docs/3.0.0-preview/img/AllJobsPageDetail1.png&quot;</span> <span class="attr">width</span>=<span class="string">&quot;10%&quot;</span> <span class="attr">height</span>=<span class="string">&quot;10%&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;https://spark.apache.org/docs/3.0.0-preview/img/AllJobsPageDetail1.png&quot;</span> <span class="attr">width</span>=<span class="string">&quot;30%&quot;</span> <span class="attr">height</span>=<span class="string">&quot;30%&quot;</span>&gt;</span></span><br></pre></td></tr></table></div></figure>
<p>可以比对下效果，第一张图片是第一个 img 标签<br><img src="https://spark.apache.org/docs/3.0.0-preview/img/AllJobsPageDetail1.png" width="10%" height="10%"></p>
<img src="https://spark.apache.org/docs/3.0.0-preview/img/AllJobsPageDetail1.png" width="30%" height="30%">


        <h3 id="任务列表"   >
          <a href="#任务列表" class="heading-link"><i class="fas fa-link"></i></a>任务列表</h3>
      <figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">- [] 已完成</span><br><span class="line">- [x] 未完成</span><br></pre></td></tr></table></div></figure>
<p>上述代码实现的效果如下，加 x 代表已经完成</p>
<ul>
<li>[] 已完成</li>
<li><input checked="" disabled="" type="checkbox"> 未完成</li>
</ul>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>MarkDown</tag>
      </tags>
  </entry>
  <entry>
    <title>OneData探索</title>
    <url>/posts/918e25c9/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>2021 年下半年主要做的是 IMP 实时/离线数据流的摄入以及涉及的各种 BI 报表工作。IMP 即内部营销平台，也可以叫作端内广告投放，作为最前置的业务，IMP 整个链路横跨广告投放、策略分流、落地页、微信导流、短信/PUSH、成单等多个垂直业务单元。随着业务需求的频繁迭代，之前构建的业务数仓暴露出来越来越多的问题，表、字段的命名不统一，同一业务不同表之间的逻辑耦合，相同指标不同口径实现的来回对数也对数据研发侧造成了很大困扰，由此本身产出的数据指标的置信性也开始受到挑战。</p>
<p>基于以上问题，2022 年开始做了一些离线业务数仓方向上的调研以及落地规划。目标是在支撑业务快速迭代开发的前提下，统一化字段业务口径，规范化离线数据开发，降低离线表的存储资源，去除逻辑的冗余开发，提高离线 ETL 的开发效率。</p>
<p>本文主要介绍个人基于 OneData 的一些看法，并举一些例子。</p>
<a id="more"></a>


        <h3 id="什么是-onedata"   >
          <a href="#什么是-onedata" class="heading-link"><i class="fas fa-link"></i></a>什么是 onedata</h3>
      <p>官方的解释是：阿里云 OneData 数据中台解决方案基于大数据存储和计算平台为载体，以 OneModel 统一数据构建及管理方法论为主干，OneID 核心商业要素资产化为核心，实现全域链接、标签萃取、立体画像，以数据资产管理为皮，数据应用服务为枝叶的松耦性整体解决方案。其数据服务理念根植于心，强调业务模式，在推进数字化转型中实现价值。具体如图所示：</p>
<p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/ali_oneData.png"></p>
<p>可以理解为 oneData 就是提供规范定义和开发标准，借助工具或者文档实现维度和指标的统一管理，在此基础上进行标准建模，对内对外提供统一的基础数据层，以期实现相同数据加工一次即可用的目的。</p>

        <h3 id="oneData-体系架构"   >
          <a href="#oneData-体系架构" class="heading-link"><i class="fas fa-link"></i></a>oneData 体系架构</h3>
      <p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/OneData%20%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84%20%281%29.png"></p>
<p>说是 oneData 的体系架构，更多是一种抽象出数据体系组成的方法，是实现 oneData 之路的第一步，一般分为以下三个步骤。</p>
<ol>
<li>业务板块划分：根据业务属性，将业务划分出几个相对独立的板块，使业务板块之间的指标或业务重叠性较小。</li>
<li>规范定义：结合行业的数据仓库建设经验和数据自身特点，设计出的一套数据规范命名体系，规范定义将会被用在模型设计中。</li>
<li>模型设计：以维度建模理论为基础，基于维度建模总线架构，构建一致性的维度和事实（进行规范定义），同时，在落地表模型时，基于自身业务特点，设计一套规范命名体系。</li>
</ol>
<p>在规范定义阶段会涉及到以下一些基础概念：</p>
<ul>
<li><p>数据域：也叫主题域，指面向业务分析，将业务过程或者维度进行抽象的集合。数据域是需要抽象提炼，并且长期维护和更新的，但不轻易变动。在划分数据域时，既能涵盖当前所有的业务需求，又能在新业务进入时无影响的包含进已有的数据域中和拓展新的数据域。数据域的概念是为了更好划分管理数仓，类似文件夹中的子文件夹的概念，如果没有数据域的概念，一个数仓下上千张表的划分和管理是也是比较困难的。</p>
</li>
<li><p>业务过程：指企业的业务活动事件，如广告位的请求，曝光，可见曝光，点击都是业务过程。需要注意业务过程是一个不可拆分的行为事件。</p>
</li>
<li><p>维度：维度是度量的环境，用来反映业务的一类属性。这类属性的集合构成一个维度，也可以称为实体对象。维度属于一个数据域，如时间维度（年/月/日等），用户维度（年级、性别等）。</p>
</li>
<li><p>维度属性：维度属性隶属于一个维度，如用户维度里的年级，性别都是独立的维度属性。</p>
</li>
<li><p>原子指标/度量：原子指标和度量含义相同，都是基于某一业务事件行为下的度量，是业务定义中不可在拆分的指标，如落地页曝光等。</p>
</li>
<li><p>修饰类型：是对修饰词的一种抽象划分，从属与某个业务域，如课程维下的课程学科等。与维度的划分有点类似。</p>
</li>
<li><p>修饰词：指除了统计维度以外指标的业务场景限定抽象。</p>
</li>
<li><p>时间周期：用来明确数据统计的时间范围和时间点，如最近一小时等。</p>
</li>
<li><p>派生指标：派生指标=时间周期+修饰词+原子指标。派生指标是有实际业务含义，可以直接取数据的指标，通过限定指标的组成要素来唯一精确定义每个指标。当维度，原子指标，时间周期/修饰词都确定的时候就可以唯一确定一个派生指标，同时给出具体数值。如最近一小时内 365 资源位的点击次数。</p>
</li>
</ul>
<p>在理解了上述概念之后，就可以根据业务需求调研的结果抽象出每个数据域下的层次结构，制定一些规范（一般包括字段命名规范，模型命名规范，数据流向规范），并以此规范进行后续数据模型的设计和优劣与否的判断标准。</p>

        <h3 id="规范定义"   >
          <a href="#规范定义" class="heading-link"><i class="fas fa-link"></i></a>规范定义</h3>
      
        <h4 id="字段命名规范"   >
          <a href="#字段命名规范" class="heading-link"><i class="fas fa-link"></i></a>字段命名规范</h4>
      <p>主要是词根表的建立，个人理解为词根是描述一件事物的最小单词，通过建立基础词根表，以不同词根的组合作为某个业务字段的命名来保证字段名称的规范和统一性。这里的词根也包含专业名词。</p>
<p>示例：</p>
<div class="table-container"><table>
<thead>
<tr>
<th>词根</th>
<th>含义</th>
<th>简写</th>
</tr>
</thead>
<tbody><tr>
<td>advert</td>
<td>广告</td>
<td>ad</td>
</tr>
<tr>
<td>postition</td>
<td>位置</td>
<td>pos</td>
</tr>
<tr>
<td>id</td>
<td>id</td>
<td></td>
</tr>
</tbody></table></div>
<p>广告位 id 就可以命名为 adpos_id。</p>

        <h4 id="指标定义规范"   >
          <a href="#指标定义规范" class="heading-link"><i class="fas fa-link"></i></a>指标定义规范</h4>
      <p>指标一般分为原子指标和派生指标。指标命名可以基于以上的字段的命名规范由不同词根组合而来。指标定义规范一般包含命名，口径，类型（sum 还是 count distinct ），从属的业务过程，以上三点在加上时间周期和修饰词就构建成了派生指标。在 BI 层通常还会配置复合指标，比如 ctr/cvr 等。</p>

        <h4 id="模型设计规范"   >
          <a href="#模型设计规范" class="heading-link"><i class="fas fa-link"></i></a>模型设计规范</h4>
      <p>模型分层<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/%E5%88%86%E5%B1%82.png" width="60%" height="50%"></p>
<ul>
<li>ODS：原始数据层，存放原始数据，数据保持原貌不做处理。</li>
<li>DW ：数据仓库层，是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合，是一个包含所有主题的通用集合。包含 DWD、DWS。</li>
<li>DWD：明细数据层，存储的都是以事实表为主，该层的事实表又叫明细事实表。DWD 层做数据的标准化，为后续的处理提供干净、统一、标准的数据。在 DWD 层的操作：<ul>
<li>过滤，去除掉丢失关键信息的数据、去除格式错误的数据、去除无用字段。</li>
<li>统一，打点数据不够规范，不同业务的打点逻辑可能不同，在 DWD 层做统一。如布尔类型，有用 0，1 数字标识，也有用 0，1 字符串标识，也有用 true，false 字符串标识别。如字符串空值有 “”,也有 null。如日期有秒级时间戳，也有毫秒时间戳。</li>
<li>映射，例如将字符串映射成枚举值。</li>
<li>按照数据域划分。</li>
</ul>
</li>
<li>DWS：数据服务层，按照业务划分，在该层按照不同的维度聚合计算指标，生成字段比较多的宽表，用于提供后续的业务查询，OLAP分析，数据分发等。</li>
<li>DIM：维表层，存储所有的维度信息。按照不同的维度主题划分。</li>
<li>ADS：数据应用层，面向实际的数据需求，以 DWD 或者 DWS 层的数据为基础，组成的各种统计报表。</li>
</ul>
<p>数据流向规范<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/shucangliuxiang.png" width="60%" height="50%"></p>
<ul>
<li>只有 dwd 和 dim 可以使用 ods 层数据。</li>
<li>dwd 之间不能同层调用，原因是不能跨数据域调用。</li>
<li>dws 之间可以互相调用，组合成不同数据域下的链路转化宽表。</li>
<li>dim 从 dwd 事实表中获取或者从 ods 层直接获取，不能直接将 ods 层表作为 dim 使用。</li>
<li>ads 层只能通过 dwd 和 dws 取数，并且要尽量避免直接从 dwd 取数，如果要取数的话，需要考虑是否可以建立 dws 宽表。</li>
<li>对于其他业务团队提供的 dwd 表，首先考虑建立 dws 表，再者同步成业务交集下的 dwd 事实表。</li>
</ul>

        <h4 id="模型命名规范"   >
          <a href="#模型命名规范" class="heading-link"><i class="fas fa-link"></i></a>模型命名规范</h4>
      <p>模型分层 + 业务板块 + 数据主题 + {可选的业务过程} + {可选的粒度} +  调度周期，比如 dwd_imp_ad_click_di</p>

        <h4 id="表文档规范"   >
          <a href="#表文档规范" class="heading-link"><i class="fas fa-link"></i></a>表文档规范</h4>
      <p>在平台基建不完善时，可以考虑结合 confluence 来对表文档进行集中管理，但是比较麻烦，这里不太建议。个人感觉的 oneData 各种规范的保证其实是强依赖于平台工具的，比如阿里的 dataworks。</p>

        <h4 id="模型设计"   >
          <a href="#模型设计" class="heading-link"><i class="fas fa-link"></i></a>模型设计</h4>
      <p>主要基于维度建模的思想来进行模型设计，主要考虑以下几个因素</p>
<ul>
<li>易用性，大多数数仓表不仅仅要提供给上下游研发使用，分析师和运营也有取数需求，表模型的设计要尽量便于他人理解。</li>
<li>拓展性，由于业务迭代添加字段时不用对下游业务使用造成太大的扰动。</li>
<li>低成本，合理的字段冗余可以提高 sql 的性能，避免多余的 join 操作。这里的低成本并非是简单的低存储，更多是存储成本和查询性能的平衡。</li>
<li>高效性，良好的查询性能，通常依赖于合理的分区和索引来实现，存储方面的话就是数据的分布。</li>
</ul>

        <h3 id="实施过程"   >
          <a href="#实施过程" class="heading-link"><i class="fas fa-link"></i></a>实施过程</h3>
      <p>阿里巴巴大数据之路第五章有对应的图，这里就不重新搞了，这里实施过程其实可以在抽象下，下面实践会介绍并举一些示例。</p>
<ol>
<li>业务需求调研。</li>
<li>依据 oneData 的体系架构进行数据域划分，抽象业务过程和维度。</li>
<li>明确规范定义<ul>
<li>指标规范（指标分层，是否要借助工具或者文档来进行指标的统一建立与管理）。</li>
<li>命名规范（字段命名规范，词根表，维度修饰词文档的维护）。</li>
<li>模型设计规范（模型分层，模型命名，数据流向等）</li>
<li>码值维护。</li>
</ul>
</li>
<li>构建总线矩阵，以全局整体的视角来看每个业务过程所涉及到的维度。</li>
<li>模型设计<ul>
<li>依据第二步抽象的维度和指标进行维表和事实表设计。</li>
<li>模型评审，保障交付。</li>
</ul>
</li>
</ol>

        <h3 id="一些问题"   >
          <a href="#一些问题" class="heading-link"><i class="fas fa-link"></i></a>一些问题</h3>
      <p>之前看 oneData 时的一些问题，其实应该也不算问题，oneData 作为一个方法论，只是起到总纲的作用，不能教条主义，要结合本身的业务场景来灵活调整，最终形成适合自己的 onedata。</p>
<ul>
<li>维度属性和修饰词的区别在哪里？既然已经作为修饰词了，那么就可以作为维度，为何还要区分成两个概念？</li>
<li>汇总表和事实表的区别在哪里？<br>只有汇总表会涉及到指标的概念，事实表仅仅是描述事件行为。</li>
<li>构建总线矩阵的意义在哪里？</li>
<li>oneData 的核心思想是什么？</li>
<li>维表有没有必要进行合并，什么场景下合并？<br>要理解主从维表的概念</li>
<li>ods 的维表为啥要同步到 dim 一份，仅仅是为了统一规范？</li>
<li>粒度和维度的区别在哪里？<br>粒度更偏向于描述事实的深度，比如小时粒度，天粒度，月粒度。而维度是描述事实的角度，更水平也更细。</li>
<li>针对前置业务板块用到了后续转化链路业务下的数据域中的表，是需要把后续链路业务板块划作一个数据域放到前置业务中去，还是做完全的业务隔离呢？</li>
<li>主题域和数据域的区别在哪里？</li>
<li>业务过程该怎么划分，要不要拆分到最细？这里分别对应的就是单事务事实表和多事务事实表。什么场景下该拆分到最细，什么场景下又该合并？</li>
<li>dwd，dws 层原则上不允许跨数据域，如果要跨数据域该怎么办？是新建数据域还是通过视图的方式提供？</li>
<li>oneDtata 整体的实施过程强依赖工具，尤其是规范定义部分，如果没有工具功能保证的话，不可避免的会出现规范定义问题，最常见的就是字段命名不统一。</li>
</ul>

        <h3 id="实践"   >
          <a href="#实践" class="heading-link"><i class="fas fa-link"></i></a>实践</h3>
      <p>看了 oneData 一段时间后，其实非常的晦涩难懂，总感觉是虚无缥缈，另外这个东西的设计与重构的周期是非常长的，并且还要考虑中间业务需求的变动造成的已确定表模型的更改以及双跑过程中的队列资源。基于以上问题，对以上 oneData 的实施过程在做细分。</p>
<p>下面的内容都会基于广告投放下的广告请求业务过程来做举例。</p>

        <h4 id="数据域及业务过程划分"   >
          <a href="#数据域及业务过程划分" class="heading-link"><i class="fas fa-link"></i></a>数据域及业务过程划分</h4>
      <p>上面已经介绍了数据域及业务过程的概念。对于涉及对个垂直业务单元的业务线来说，其涉及到的每个业务单元可以看做一个数据域，比如博主所做的 IMP ，就可以划分为广告投放，落地页，成单等等数据域。<br>这里以广告投放数据域为例，业务过程大致可以划分为内部与外部</p>
<ul>
<li>内部，运营侧在后台对广告计划，单元，创意的创建。</li>
<li>外部，用户产生的广告请求，广告曝光，广告可见曝光，广告点击。</li>
</ul>

        <h4 id="数仓规划"   >
          <a href="#数仓规划" class="heading-link"><i class="fas fa-link"></i></a>数仓规划</h4>
      
        <h5 id="构建总线矩阵"   >
          <a href="#构建总线矩阵" class="heading-link"><i class="fas fa-link"></i></a>构建总线矩阵</h5>
      <p>主要是明确该数据域下所涉及到的所有业务过程以及每个业务过程所涉及到的维度<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/zongxianjuzhen.png"></p>

        <h5 id="明确指标"   >
          <a href="#明确指标" class="heading-link"><i class="fas fa-link"></i></a>明确指标</h5>
      <p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/mingquezhibiao.png"></p>

        <h5 id="整体设计"   >
          <a href="#整体设计" class="heading-link"><i class="fas fa-link"></i></a>整体设计</h5>
      <p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/zhengtisheji.png"></p>

        <h4 id="数据标准"   >
          <a href="#数据标准" class="heading-link"><i class="fas fa-link"></i></a>数据标准</h4>
      
        <h5 id="定义字段标准"   >
          <a href="#定义字段标准" class="heading-link"><i class="fas fa-link"></i></a>定义字段标准</h5>
      <p>根据规范定义中的词根表来设计该数据域下每个业务过程所涉及到的字段以及业务含义。</p>

        <h5 id="定义枚举值"   >
          <a href="#定义枚举值" class="heading-link"><i class="fas fa-link"></i></a>定义枚举值</h5>
      <p>针对字段标准中字段，关联其对应的枚举值</p>

        <h5 id="定义度量"   >
          <a href="#定义度量" class="heading-link"><i class="fas fa-link"></i></a>定义度量</h5>
      
        <h4 id="数据指标"   >
          <a href="#数据指标" class="heading-link"><i class="fas fa-link"></i></a>数据指标</h4>
      
        <h5 id="定义原子指标"   >
          <a href="#定义原子指标" class="heading-link"><i class="fas fa-link"></i></a>定义原子指标</h5>
      
        <h5 id="定义修饰词"   >
          <a href="#定义修饰词" class="heading-link"><i class="fas fa-link"></i></a>定义修饰词</h5>
      
        <h5 id="定义时间周期"   >
          <a href="#定义时间周期" class="heading-link"><i class="fas fa-link"></i></a>定义时间周期</h5>
      
        <h5 id="构建派生指标"   >
          <a href="#构建派生指标" class="heading-link"><i class="fas fa-link"></i></a>构建派生指标</h5>
      
        <h4 id="模型设计-1"   >
          <a href="#模型设计-1" class="heading-link"><i class="fas fa-link"></i></a>模型设计</h4>
      
        <h3 id="总结"   >
          <a href="#总结" class="heading-link"><i class="fas fa-link"></i></a>总结</h3>
      <p>OneData 仅仅是一个思想，或者说是一种做事的通用方法。我们不应囿于这个个概念，而是应该结合具体的业务场景去思考如何落地，在实践中归纳出适合自己业务的 ‘OneData’。这里其实我把它归纳成了五步，然后没一步都需要详细的业务调研和评审环节，甚至工具来保证其强规范性</p>
<ul>
<li>数据域及业务过程划分</li>
<li>数仓规划</li>
<li>数据标准 </li>
<li>数据指标</li>
<li>模型设计</li>
<li>模型评审</li>
<li>资源预估</li>
</ul>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>OneData</tag>
      </tags>
  </entry>
  <entry>
    <title>Prometheus + Grafana 监控 - Kafka</title>
    <url>/posts/dc1a86bd/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>最近工作中越来越感受到监控对于查找问题的重要性，一个完备的链路监控对问题定位和趋势分析提效非常高。比如一条实时数据流，从数据采集到消费到入库各个阶段都有一些可观测性的指标（binlog 采集延迟，kafka-lag，读写 QPS，max-request-size，offset 趋势）。如果 kafka-lag 比较小并且 topic 写 QPS没打太高，但是数据有延迟，这里大概率就是上游采集的问题。<br>这里借用 prometheus 官网的话介绍监控的作用。</p>
<ul>
<li>长期趋势分析：通过对监控样本数据的持续收集和统计，对监控指标进行长期趋势分析。例如，通过对磁盘空间增长率的判断，我们可以提前预测在未来什么时间节点上需要对资源进行扩容。</li>
<li>对照分析：两个版本的系统运行资源使用情况的差异如何？在不同容量情况下系统的并发和负载变化如何？通过监控能够方便的对系统进行跟踪和比较。</li>
<li>告警：当系统出现或者即将出现故障时，监控系统需要迅速反应并通知管理员，从而能够对问题进行快速的处理或者提前预防问题的发生，避免出现对业务的影响。</li>
<li>故障分析与定位：当问题发生后，需要对问题进行调查和处理。通过对不同监控监控以及历史数据的分析，能够找到并解决根源问题。</li>
</ul>
<p>本系列主要用来记录工作中常见系统的监控实现，指标含义以及如何通过监控定位问题并在相关任务挂掉后如何和给下游业务一个较准确的预估恢复时间。大部分借助开源实现。</p>
<a id="more"></a>


        <h3 id="Prometheus"   >
          <a href="#Prometheus" class="heading-link"><i class="fas fa-link"></i></a>Prometheus</h3>
      
        <h4 id="安装"   >
          <a href="#安装" class="heading-link"><i class="fas fa-link"></i></a>安装</h4>
      <p>直接 brew 安装启动即可，也可以参考 <span class="exturl"><a class="exturl__link"   href="https://yunlzheng.gitbook.io/prometheus-book" >prometheus 中文文档</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 通过预编译包安装。</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">brew info prometheus</span><br><span class="line">brew install prometheus</span><br><span class="line">brew services start prometheus</span><br></pre></td></tr></table></div></figure>
<p>在 prometheus.yml 里面配置抓取的 job。 需要注意一下，如果是 mac os 的话，Prometheus 默认配置文件地址为 <code>/usr/local/etc/prometheus.yml</code>。</p>

        <h4 id="使用"   >
          <a href="#使用" class="heading-link"><i class="fas fa-link"></i></a>使用</h4>
      <p>安装完成后，可以打开 <code>http://localhost:9090/</code> 查看 Prometheus UI。可以在 Status-Targets 里面找到已经启动抓取的 exporter。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/008i3skNly1grc1gm1u0qj31h80ltn0f.jpg"></p>
<p>Prometheus 大概的流程如下。更多的信息可以查看官方文档。其中 PromQL 以及 metric 的格式需要重点了解下，以后查询 metrics，配置 Grafana DashBoard 都会用到。</p>
<ul>
<li>Prometheus server 定期从配置的 job 拉取 metrics。</li>
<li>Prometheus 将拉取的 metrics 信息存储到本地，记录新的时间序列。如果触发定义好的 alert.rules，AlterManager 会向用户发送报警。</li>
</ul>

        <h4 id="需要注意的地方"   >
          <a href="#需要注意的地方" class="heading-link"><i class="fas fa-link"></i></a>需要注意的地方</h4>
      <p>使用 brew service 无法启动服务的话，可以去查看 xx.plist 文件中的 log 路径找具体错误。我这里就是碰到 9000 端口被其他代理服务占用，然后 prometheus 一直在 error 状态。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">$ brew services list | grep prometheus</span><br><span class="line">Name          Status  User   Plist</span><br><span class="line">prometheus    started zaoshu /Users/zaoshu/Library/LaunchAgents/homebrew.mxcl.prometheus.plist</span><br></pre></td></tr></table></div></figure>


        <h3 id="Grafana"   >
          <a href="#Grafana" class="heading-link"><i class="fas fa-link"></i></a>Grafana</h3>
      
        <h4 id="安装-1"   >
          <a href="#安装-1" class="heading-link"><i class="fas fa-link"></i></a>安装</h4>
      <figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">brew info grafana</span><br><span class="line">brew install grafana</span><br><span class="line">brew services start grafana</span><br></pre></td></tr></table></div></figure>

        <h4 id="使用-1"   >
          <a href="#使用-1" class="heading-link"><i class="fas fa-link"></i></a>使用</h4>
      <p>打开 <code>http://localhost:3000/</code>，查看 Grafana UI，默认账户名密码是 admin/admin。使用流程和普通写程序差不多，全套 CURD。</p>
<ol>
<li>配置 DataSource</li>
<li>配置 DashBoard，直接 load 模板或者自己通过 PromQL 配置。</li>
<li>配置告警规则</li>
</ol>

        <h3 id="kafka-exporter"   >
          <a href="#kafka-exporter" class="heading-link"><i class="fas fa-link"></i></a>kafka_exporter</h3>
      <p>kafka_exporter 用来收集 Topic，Broker，ConsumerGroup 的相关信息，可以无缝对接 Prometheus 和 Grafana，使用起来比较方便。</p>

        <h4 id="安装-2"   >
          <a href="#安装-2" class="heading-link"><i class="fas fa-link"></i></a>安装</h4>
      <p>查看对应的 github 项目 <span class="exturl"><a class="exturl__link"   href="https://github.com/danielqsj/kafka_exporter" >kafka_exporter</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>。在 release 页面中下载适合 mac os 的包（kafka_exporter-1.3.1.darwin-amd64.tar.gz<br>）。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">wget https://github.com/danielqsj/kafka_exporter/releases/download/v1.3.1/kafka_exporter-1.3.1.darwin-amd64.tar.gz -P /usr/<span class="built_in">local</span>/Cellar/kafka_exporter &amp;&amp; <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/Cellar/kafka_exporter</span><br><span class="line">tar -xzvf kafka_exporter-1.3.1.darwin-amd64.tar.gz </span><br><span class="line"></span><br></pre></td></tr></table></div></figure>

        <h4 id="使用-2"   >
          <a href="#使用-2" class="heading-link"><i class="fas fa-link"></i></a>使用</h4>
      <p>启动 kafka 集群，执行启动脚本。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">nohup kafka_exporter --kafka.server=localhost:9092 &gt;&gt; kafka_exporter.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></div></figure>
<p>修改 prometheus.yml，加上 kafka_exporter 的 job。默认端口是 9308。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">global:</span><br><span class="line">  scrape_interval: 15s</span><br><span class="line"></span><br><span class="line">scrape_configs:</span><br><span class="line">  - job_name: <span class="string">&quot;prometheus&quot;</span></span><br><span class="line">    static_configs:</span><br><span class="line">    - targets: [<span class="string">&quot;localhost:9095&quot;</span>]</span><br><span class="line">  - job_name: <span class="string">&quot;kafka_exporter&quot;</span></span><br><span class="line">    static_configs:</span><br><span class="line">    - targets: [<span class="string">&quot;localhost:9308&quot;</span>]</span><br><span class="line">  - job_name: <span class="string">&quot;canal&quot;</span></span><br><span class="line">    static_configs:</span><br><span class="line">    - targets: [<span class="string">&quot;localhost:11112&quot;</span>]</span><br></pre></td></tr></table></div></figure>
<p>查看 prometheus ui，可以观察到已经启动抓取的 job。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/008i3skNly1gsof789howj31h80hgtc5.jpg"><br>访问上图中的 Endpoint 地址（默认是 <span class="exturl"><a class="exturl__link"   href="http://localhost:9308/metrics%EF%BC%89%EF%BC%8C%E5%8F%AF%E4%BB%A5%E6%9F%A5%E7%9C%8B%E5%B7%B2%E7%BB%8F%E6%8A%93%E5%8F%96%E5%88%B0%E7%9A%84%E6%8C%87%E6%A0%87%E3%80%82" >http://localhost:9308/metrics），可以查看已经抓取到的指标。</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/008i3skNly1gsofhrydl2j30r30p5gu0.jpg"></p>

        <h4 id="指标列表"   >
          <a href="#指标列表" class="heading-link"><i class="fas fa-link"></i></a>指标列表</h4>
      <p>参考 <span class="exturl"><a class="exturl__link"   href="https://github.com/danielqsj/kafka_exporter" >kafka_exporter</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 的 Readme。</p>
<div class="table-container"><table>
<thead>
<tr>
<th>Metrics</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>kafka_brokers</td>
<td>kafka 集群的 broker 数量</td>
</tr>
<tr>
<td>kafka_topic_partitions</td>
<td>kafka topic 的分区数</td>
</tr>
<tr>
<td>kafka_topic_partition_current_offset</td>
<td>kafka topic 分区当前的 offset</td>
</tr>
<tr>
<td>kafka_topic_partition_oldest_offset</td>
<td>kafka topic 分区最旧的 offset</td>
</tr>
<tr>
<td>kafka_topic_partition_in_sync_replica</td>
<td>处于同步过程中的 topic/partition 数</td>
</tr>
<tr>
<td>kafka_topic_partition_leader</td>
<td>topic/partition leader 的 broker id</td>
</tr>
<tr>
<td>kafka_topic_partition_leader_is_preferred</td>
<td>topic/partition 是否使用 preferred broker</td>
</tr>
<tr>
<td>kafka_topic_partition_replicas</td>
<td>topic/partition 的副本数</td>
</tr>
<tr>
<td>kafka_topic_partition_under_replicated_partition</td>
<td>partition 是否处于 replicated</td>
</tr>
<tr>
<td>kafka_consumergroup_current_offset</td>
<td>kakfa topic 消费者组的 offset</td>
</tr>
<tr>
<td>kafka_consumergroup_lag</td>
<td>kakfa-lag 消费延迟</td>
</tr>
</tbody></table></div>

        <h4 id="遗留问题"   >
          <a href="#遗留问题" class="heading-link"><i class="fas fa-link"></i></a>遗留问题</h4>
      <p>如何使用 kafka_exporter 监控多集群？启动多个 kafka_exporter 实例还是有其他方式。</p>

        <h3 id="Grafana-看板配置"   >
          <a href="#Grafana-看板配置" class="heading-link"><i class="fas fa-link"></i></a>Grafana 看板配置</h3>
      <p>关于 kafka，我们比较关注的是集群 broker 数量，topic 可用分区数，读写 QPS，partition offset，kafka-lag，吞吐量。<br>如果是第一次接触 Grafana 的话，建议直接导入模板，然后在模板基础上模仿修改最终达到自己想要的效果。<span class="exturl"><a class="exturl__link"   href="https://grafana.com/grafana/dashboards" >Grafana DashBoard</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>。这里使用的模板是<br>7589。<br>tips：在使用 prometheus 作为数据源的时候，grafana 的 query 配置其实就是 promQL+指标+。比如（sum(kafka_topic_partitions{instance=<del>“$instance”,topic=</del>“$topic”})） </p>
<ul>
<li><p>创建 DashBoard，配置数据源，导入模板。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/kvw9u.png"><br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/j2f4h.png"></p>
</li>
<li><p>添加 consumergroup variable，方便查看每个消费者的 lag 和 offset 趋势。<br>参考模板的几个 variable 的配置。job -&gt; prometheus job。instance -&gt; kafka_exporter 实例。topic -&gt; kafka 主题。consumergroup -&gt; 消费者组。关于一些变量命名可以直接访问 <span class="exturl"><a class="exturl__link"   href="http://localhost:9308/metrics" >http://localhost:9308/metrics</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 查看。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/4jj0i.png"></p>
</li>
<li><p>添加 broker，partition，吞吐量的 stat panal。<br>broker，partition 的指标 kafka_exporter 本身已经抓取到。<br>吞吐量其实就是 kafka_topic_partition_current_offset 的增长和 kafka_consumergroup_current_offset 的增长，这里使用 rate 来计算平均增长速率。</p>
</li>
<li><p>配置 kafka-lag panal 和报警。<br>使用 kafka_consumergroup_lag。<br>配置 Alert 的时候会报错 <code>template variables are not supported in alert queries</code>，这是因为 alert 不支持带变量的 metrics。添加 query，设置一个常量，并且设置为不可见即可。然后使用该不可见指标配置报警规则。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/xvle5.png"><br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/lfzmz.png"></p>
</li>
<li><p>配置 offset 趋势。</p>
</li>
<li><p>配置 topic 的读写 QPS。<br>和吞吐量的算法一样。</p>
</li>
<li><p>配置 partition offset。</p>
</li>
</ul>
<p>看板配置完成后，可以往 kafka topic 写入数据并启动 kafka-console-consumer 观察。因为我本地配置了 canal，所以直接 create_time + 1 触发了 binlog 到指定 topic。最后整体看板如下<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/wiyza.png"><br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/4vt7b.png"></p>

        <h3 id="监控分析"   >
          <a href="#监控分析" class="heading-link"><i class="fas fa-link"></i></a>监控分析</h3>
      <p>kafka 广泛应用于实时作业中，所以对其的监控报警是必要的。上面配置的看板对于问题查看分析有什么作用呢？下面举几个典型的例子。</p>
<ul>
<li><p>kakfa topic 消费堆积<br>kafka-lag 是比较重要的性能指标。一般实时流任务的延迟，首先考虑到的就是消费堆积，此时需要查看是否刷数场景或者业务高峰期造成的短期堆积，是否需要临时加大消费者资源防止对下游业务使用造成影响。</p>
</li>
<li><p>kafka-lag 恢复时长估计<br>可以观察 consumergroup offset 的上升趋势，计算处理速度。</p>
</li>
<li><p>binlog 端到端延迟判断<br>需要结合 binlog 监控。如果没有 kafka-lag 且写 QPS 正常，一般是 canal 采集的问题。如果存在 kafka-lag 并且写入 QPS 比往常高很多，就要考虑刷数场景造成的堆积，进而产生 binlog 端到端延迟。</p>
</li>
<li><p>topic 分区是否均匀<br>观察 partition offset 和分 partition 写入 QPS。如果某个分区写入数据量很少，那么需要考虑分区 key 是否合理。不合理的分区对下游消费也会产生性能影响。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据运维</category>
      </categories>
      <tags>
        <tag>Grafana</tag>
        <tag>Prometheus</tag>
        <tag>kafka_exporter</tag>
      </tags>
  </entry>
  <entry>
    <title>Row &amp;&amp; Column - Compose &quot;Tabular Data Set&quot;</title>
    <url>/posts/5d48686a/</url>
    <content><![CDATA[
        <h2 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h2>
      <p>Dataset ，DataFrame 在我们眼中的直观的呈现形式就是一张表格。那么我们该如何处理一张表格的行列呢？Spark SQL 中的 Row， Column 类型将为我们解答这个问题。</p>
<a id="more"></a>


        <h2 id="Row"   >
          <a href="#Row" class="heading-link"><i class="fas fa-link"></i></a>Row</h2>
      <p>Row 可以被看作集合 collection，只是带有可选的 schema （当对 Dataset 使用 toDF 或者 DataFrameReader 读取数据源时，schema 会通过 RowEncoder 分配给 Row）。所以 Row 具有集合的特性，我们可以通过索引来获取集合中的字段。下面是 Row 的一些方法示例</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> row = <span class="type">Row</span>(<span class="number">1</span>, <span class="string">&quot;hello&quot;</span>)</span><br><span class="line">row: org.apache.spark.sql.<span class="type">Row</span> = [<span class="number">1</span>,hello]</span><br><span class="line"></span><br><span class="line">scala&gt; row(<span class="number">1</span>)</span><br><span class="line">res13: <span class="type">Any</span> = hello</span><br><span class="line"></span><br><span class="line">scala&gt; row.get(<span class="number">1</span>)</span><br><span class="line">res8: <span class="type">Any</span> = hello</span><br><span class="line"></span><br><span class="line">scala&gt; row.schema</span><br><span class="line">res9: org.apache.spark.sql.types.<span class="type">StructType</span> = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">Row</span>.empty</span><br><span class="line">res14: org.apache.spark.sql.<span class="type">Row</span> = []</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">Row</span>.fromSeq(<span class="type">Seq</span>(<span class="number">1</span>, <span class="string">&quot;hello&quot;</span>))</span><br><span class="line">res15: org.apache.spark.sql.<span class="type">Row</span> = [<span class="number">1</span>,hello]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">Row</span>.fromTuple((<span class="number">0</span>, <span class="string">&quot;hello&quot;</span>))</span><br><span class="line">res16: org.apache.spark.sql.<span class="type">Row</span> = [<span class="number">0</span>,hello]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">Row</span>.merge(<span class="type">Row</span>(<span class="number">1</span>), <span class="type">Row</span>(<span class="string">&quot;hello&quot;</span>))</span><br><span class="line">res17: org.apache.spark.sql.<span class="type">Row</span> = [<span class="number">1</span>,hello]</span><br></pre></td></tr></table></div></figure>
<p>row 通过索引获取的是 Any 类型。可以通过 getAs 方法指定类型</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">row.getAS[<span class="type">Int</span>](<span class="number">0</span>)</span><br></pre></td></tr></table></div></figure>

        <h2 id="Column"   >
          <a href="#Column" class="heading-link"><i class="fas fa-link"></i></a>Column</h2>
      <p>通过 Column 来操作数据集中的每列数据。在讲 SparkSession 的时候我们有提到过 implicits object。其中有 stringtocolumn 的隐式转换方法。因此我们可以直接通过 $ 来创建一个column。这也是我们在 Spark SQL 程序中经常操作列的写法。Spark SQL 内置的 col 函数也是转换 Column 的。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; $<span class="string">&quot;name&quot;</span></span><br><span class="line">res17: org.apache.spark.sql.<span class="type">ColumnName</span> = name</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line">scala&gt; col(<span class="string">&quot;name&quot;</span>)</span><br><span class="line">res18: org.apache.spark.sql.<span class="type">Column</span> = name</span><br></pre></td></tr></table></div></figure>
<p>$ 符的使用有时候还是有限制的，比如我们字段中存在 $。这时候 <code>$&quot;$ip&quot;</code>，<code>selectExpr(&quot;$ip&quot;)</code>  就会出现问题。这时候可能就需要 <code>col</code> 来操作列。</p>
<p>下面介绍一下与 Column 相关的一些常见方法 （日常使用中，我们往往需要 UDF 和 transform。Columns 和 filter 经常会结合使用用来过滤数据集）。</p>
<div class="table-container"><table>
<thead>
<tr>
<th>method</th>
<th>description</th>
<th>example</th>
</tr>
</thead>
<tbody><tr>
<td>as</td>
<td>指定该列的类型</td>
<td><code>$&quot;id&quot;.as[Int]</code></td>
</tr>
<tr>
<td>cast</td>
<td>指定该列的类型</td>
<td><code>$&quot;id&quot;.cast(&quot;string&quot;)</code></td>
</tr>
<tr>
<td>alias</td>
<td>重命名该列</td>
<td><code>$&quot;id&quot;.alias(&quot;ID&quot;)</code></td>
</tr>
<tr>
<td>withColumn</td>
<td>增加一列</td>
<td><br>``spark</br><br><code>.range(5)</code></br><br><code>withColumn(&quot;id_&quot;，$&quot;id&quot; - 1)</code></br></td>
</tr>
<tr>
<td>like</td>
<td>类似于 SQL 中的 like</td>
<td><code>df(&quot;id&quot;) like 0</code></td>
</tr>
<tr>
<td>over</td>
<td>和 Window 结合使用，Window agg 后面会讲解到</td>
<td><br><code>val window: WindowSpec =  ......</code></br> <br><code>$&quot;id&quot; over window</code></br></td>
</tr>
<tr>
<td>isin</td>
<td>isin</td>
<td><code>$&quot;id&quot; is in (channel: _*)</code></td>
</tr>
<tr>
<td>isInCollections</td>
<td>是否该列值在指定集合中</td>
<td></td>
</tr>
<tr>
<td>asc /  desc</td>
<td></td>
<td><code>spark.range(5).sort($&quot;id&quot;.desc).show()</code></td>
</tr>
</tbody></table></div>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>SSH端口转发</title>
    <url>/posts/4feffbd0/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>端口转发在工作中经常使用，比如转发客户服务器上的 4040 端口到本地用来查看 Spark UI。前几天，想把本地端口流量转发到远程机器上，有点被绕晕了，遂总结整理一下</p>
<a id="more"></a>


        <h3 id="为什么需要端口转发"   >
          <a href="#为什么需要端口转发" class="heading-link"><i class="fas fa-link"></i></a>为什么需要端口转发</h3>
      
        <h4 id="本地端口转发"   >
          <a href="#本地端口转发" class="heading-link"><i class="fas fa-link"></i></a>本地端口转发</h4>
      <p>将发送到本地端口的请求转发到目标端口。常用的场景是网络端口受限情况下，访问目标主机上的服务。<br>比如现在远程 B 主机上存在 8000 端口 web 服务，想在本地直接访问，那么可以使用以下命令。代表将发往本地 8000 端口的请求全部发往目标主机 8000 端口。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">ssh -L 8000:127.0.0.1:8000 hostB</span><br></pre></td></tr></table></div></figure>

        <h4 id="远程端口转发"   >
          <a href="#远程端口转发" class="heading-link"><i class="fas fa-link"></i></a>远程端口转发</h4>
      <p>将发送到远程端口的请求转发到本地，并在待登录主机上访问 host 主机不能直接访问 188.188.188.188:10080 服务</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">ssh -R 0.0.0.0:10080:188.188.188.188:10080 host</span><br></pre></td></tr></table></div></figure>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>SSH配置</title>
    <url>/posts/9a4d2557/</url>
    <content><![CDATA[
        <h3 id="什么是-SSH"   >
          <a href="#什么是-SSH" class="heading-link"><i class="fas fa-link"></i></a>什么是 SSH</h3>
      <p>ssh 是一种网络协议，用于计算机之间的加密登录。大致流程如下</p>
<a id="more"></a>
<p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1gefb36prmvj30ll0dm3z7.jpg"></p>

        <h3 id="配置-ssh"   >
          <a href="#配置-ssh" class="heading-link"><i class="fas fa-link"></i></a>配置 ssh</h3>
      <p>这个应该挺熟悉的了，在使用 github 的时候，为了避免每次提交推送输入密码，我们应该都已经配置过了。在提一下过程，以下是 Mac 的。</p>
<ol>
<li>ssh-keygen -t rsa 以 RSA 算法生成秘钥</li>
<li>ssh-copy-id -i id_rsa.pub user@host 上传公钥到要免密登录的服务器</li>
<li>ssh-add -K id_rsa</li>
<li>系统偏好设置 -&gt; 共享 -&gt; 勾选远程登录</li>
<li>如果需要 ssh localhost，也需要复制公钥到自己的 authorized_keys，cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</li>
</ol>

        <h3 id="多台机器配置-SSH-免密登录"   >
          <a href="#多台机器配置-SSH-免密登录" class="heading-link"><i class="fas fa-link"></i></a>多台机器配置 SSH 免密登录</h3>
      <p>比如有三台机器 hadoop1 hadoop2 hadoop3。在每台机器上执行以下命令</p>
<ul>
<li>ssh-keygen -t rsa -P “”</li>
<li>cd .ssh</li>
<li>cat id_rsa.pub &gt;&gt; authorized_keys</li>
</ul>
<p>分别登录 hadoop2 hadoop3，执行以下命令</p>
<ul>
<li>ssh-copy-id -i id_rsa.pub hadoop1</li>
</ul>
<p>这时候 hadoop1 上的 authorized_keys 就是全的了，在从 hadoop1 上分发到其他机器</p>
<ul>
<li>scp /root/.ssh/authorized_keys hadoop2:/root/.ssh/</li>
<li>scp /root/.ssh/authorized_keys hadoop3:/root/.ssh/<br>至此三台机器间已经可以免密登录</li>
</ul>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala实用指南-从Java到Scala</title>
    <url>/posts/1f41a4db/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>接触 Scala 小半年，主要用来写 Spark SQL。不得不感叹这东西的学习曲线，入门简单，深入难，好多姿势不懂什么意思，往往写不出 Scala 的特性。现在基本就是拿来当简洁版的 Java 来用，囿于这种想法，常常觉得为啥别人写的 Scala 这么炫，姿势这么多。但是其实忽略了简洁也正是 Scala 相比 Java 的一个优点。<br>本文主要介绍 Scala 的简洁性，用比 java 更少的代码量来达到同样的效果甚至更好，更容易让人理解。 不深究其中的一些特性，仅仅展现，让我们知道可以这样做。</p>
<a id="more"></a>


        <h3 id="Scala-简洁的Java"   >
          <a href="#Scala-简洁的Java" class="heading-link"><i class="fas fa-link"></i></a>Scala-简洁的Java</h3>
      
        <h4 id="减少样板代码"   >
          <a href="#减少样板代码" class="heading-link"><i class="fas fa-link"></i></a>减少样板代码</h4>
      <p>以简单的 for 循环代码为例，分别用 Java 和 Scala 实现</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Greetings</span> </span>&#123;</span><br><span class="line"> 	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"> 		<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; <span class="number">4</span>; i++) &#123;</span><br><span class="line"> 			System.out.print(i + <span class="string">&quot;,&quot;</span>);</span><br><span class="line"> 		&#125;</span><br><span class="line"> 		System.out.println(<span class="string">&quot;Scala Rocks!!!&quot;</span>); </span><br><span class="line"> &#125;</span><br></pre></td></tr></table></div></figure>
<p>可以看下用 Scala 改写后的代码。</p>
<ul>
<li>去掉了多余的代码结尾分号，降低了代码噪声。</li>
<li>简单的 print 输出。 去除了没有什么实际意义的 Greetings 类。</li>
<li>函数式代码风格的引用使得我们可以一行书写 for 循环。</li>
<li>去除了变量类型的显式声明</li>
</ul>
<p>在这里，你可能有很多疑问，这些问题我们可以先放一放，尽管先熟悉 Scala 的代码风格结构，后面文章会解释。</p>
<ul>
<li>为什么调用方法不用 .？比如 to foreach 的调用。</li>
<li>为什么函数可以作为方法的参数？</li>
<li>为什么不需要显示声明变量类型？</li>
</ul>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Greeting</span></span>&#123;</span><br><span class="line">	<span class="number">1</span> to <span class="number">3</span> foreach(i =&gt; print(<span class="string">s&quot;<span class="subst">$i</span>,&quot;</span>))</span><br><span class="line">	println(<span class="string">&quot;Scala Rocks!!!&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>


        <h4 id="不可变性"   >
          <a href="#不可变性" class="heading-link"><i class="fas fa-link"></i></a>不可变性</h4>
      <p>Scala 中用 var 或者 val 来声明变量。val 声明的变量是不可变的，相当于 final。不可变是 FP 的重要特性，减少了代码副作用，状态不会改变，只是迭代转换变成了新的状态。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a = <span class="number">1</span></span><br><span class="line">a: <span class="type">Int</span> = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> a = <span class="number">2</span></span><br><span class="line">a: <span class="type">Int</span> = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">scala&gt; a = <span class="number">3</span></span><br><span class="line">&lt;console&gt;:<span class="number">12</span>: error: reassignment to <span class="keyword">val</span></span><br><span class="line">       a = <span class="number">3</span></span><br></pre></td></tr></table></div></figure>
<p>咦？不是说了 val 定义的是不可变的吗，怎么 a 变了？这是 shadow 机制，其实就是 a 的引用发生了变化。直接修改 a = 3 就不一样了，会造成编译错误。<br>像上边的 for 循环中的 i 值其实也并没有跟着变化，在每次循环过程中，我们都创建一个不同的名为 i 的 val 变量。我们不会<br>在循环中不经意地改变变量 i 的值，因为变量 i 是不可变的。</p>
<p>####v类型推断<br>在 Scala 中不必每次都显式声明类型。Scala 自己会根据上下文信息，推断出变量或者函数的返回值信息。可以自己 scala REPL 中试下。<br>最后也可以看到，scala 允许去除 return 关键字。返回值及类型和函数方法中的最后一行有关。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">a</span></span>() = <span class="number">1</span></span><br><span class="line">a: ()<span class="type">Int</span></span><br><span class="line"></span><br><span class="line">scala&gt; a</span><br><span class="line">res0: <span class="type">Int</span> = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">a</span></span>(x:<span class="type">Int</span>) = println(x)</span><br><span class="line">a: (x: <span class="type">Int</span>)<span class="type">Unit</span></span><br><span class="line"></span><br><span class="line">scala&gt; a(<span class="number">1</span>)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">a</span></span>() = &#123;<span class="number">1</span>;<span class="string">&quot;1&quot;</span>&#125;</span><br><span class="line">&lt;console&gt;:<span class="number">11</span>: warning: a pure expression does nothing in statement position; multiline expressions might require enclosing parentheses</span><br><span class="line">       <span class="function"><span class="keyword">def</span> <span class="title">a</span></span>() = &#123;<span class="number">1</span>;<span class="string">&quot;1&quot;</span>&#125;</span><br><span class="line">                  ^</span><br><span class="line">a: ()<span class="type">String</span></span><br><span class="line"></span><br><span class="line">scala&gt; a</span><br><span class="line">res3: <span class="type">String</span> = <span class="number">1</span></span><br></pre></td></tr></table></div></figure>

        <h4 id="高阶函数"   >
          <a href="#高阶函数" class="heading-link"><i class="fas fa-link"></i></a>高阶函数</h4>
      <p>最常见的 map，reduce，fold，filter。接收函数值作为参数，这些高阶函数其实是提高了代码的复用性。像对列表中的元素做操作，不必再每次以命令式的风格循环代码计算，而是只需要用高阶函数接收一个操作函数，告诉编译器我要做什么，而不是怎么做。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> seq = <span class="number">1</span> until <span class="number">5</span> toArray</span><br><span class="line">seq: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; seq.map(_*<span class="number">2</span>)</span><br><span class="line">res4: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; seq.reduce(_+_)</span><br><span class="line">res5: <span class="type">Int</span> = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">scala&gt; seq.filter(i =&gt; i % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">res6: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; seq.fold(<span class="number">0</span>)(<span class="type">Math</span>.max)</span><br><span class="line">res7: <span class="type">Int</span> = <span class="number">4</span></span><br></pre></td></tr></table></div></figure>

        <h4 id="元组和多重赋值"   >
          <a href="#元组和多重赋值" class="heading-link"><i class="fas fa-link"></i></a>元组和多重赋值</h4>
      <p>Scala 中函数允许返回多个值，其实是个元组。而 Java 中如果需要返回多个值，还要构造额外的类</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPersonInfo</span></span>(primaryKey: <span class="type">Int</span>) = &#123;</span><br><span class="line"> (<span class="string">&quot;Venkat&quot;</span>, <span class="string">&quot;Subramaniam&quot;</span>, <span class="string">&quot;venkats@agiledeveloper.com&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> (firstName, lastName, emailAddress) = getPersonInfo(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> info = getPersonInfo(<span class="number">1</span>)</span><br><span class="line">println(info._1)</span><br><span class="line">pringln(info._2)</span><br></pre></td></tr></table></div></figure>

        <h4 id="隐式类型转换，隐式参数"   >
          <a href="#隐式类型转换，隐式参数" class="heading-link"><i class="fas fa-link"></i></a>隐式类型转换，隐式参数</h4>
      <p>Scala 中的隐式转换是个很神奇的东西，有时候会让人感觉莫名其妙，但是却很强大。Scala 中可以直接调用 2.toString，但是 Int 类型并没有这样的方法，这里其实是 Int 自动转换成了 RichInt。这里先只说下隐式参数和隐式类型转换。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 隐式参数, 如果参数没有定义且被 implicit 修饰，会在当前作用域内查找相同类型的隐式参数</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">implicitParams</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">foo</span></span>(amount:<span class="type">Float</span>)(<span class="keyword">implicit</span> rate: <span class="type">Float</span>) = println(amount * rate)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 隐式类型转换 编译器在当前作用域查找类型转换方法，对数据类型进行转换。</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">implicitTypeTra</span> </span>&#123;</span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">doubleToInt</span></span>(i: <span class="type">Double</span>) = i.toInt</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">typeTra</span></span>(i: <span class="type">Int</span>) = println(i)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> implicitParams._</span><br><span class="line"><span class="keyword">import</span> implicitTypeTra._</span><br><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> rat = <span class="number">0.3</span>f</span><br><span class="line">foo(<span class="number">10</span>)</span><br><span class="line">typeTra(<span class="number">3.5</span>)</span><br></pre></td></tr></table></div></figure>
<p>看下上面的例子，并没有出现编译错，隐式转换为我们自动解决了。emnn，某种意义上可以说隐式转换是一种编译器自我修复机制。通过定义某些隐式转换，我们也减少了代码量。</p>

        <h4 id="操作符重载"   >
          <a href="#操作符重载" class="heading-link"><i class="fas fa-link"></i></a>操作符重载</h4>
      <p>Scala 中没有绝对意义的操作符，+ - * / 都是方法，因此都可以重写以支持自定义类型的操作。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Complex</span>(<span class="params">val real: <span class="type">Int</span>, val imaginary: <span class="type">Int</span></span>) </span>&#123;</span><br><span class="line"> 	<span class="function"><span class="keyword">def</span> <span class="title">+</span></span>(operand: <span class="type">Complex</span>): <span class="type">Complex</span> = &#123;</span><br><span class="line">  		<span class="keyword">new</span> <span class="type">Complex</span>(real + operand.real, imaginary + operand.imaginary)</span><br><span class="line"> 	&#125;</span><br><span class="line"> 	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = &#123;</span><br><span class="line"> 		<span class="keyword">val</span> sign = <span class="keyword">if</span> (imaginary &lt; <span class="number">0</span>) <span class="string">&quot;&quot;</span> <span class="keyword">else</span> <span class="string">&quot;+&quot;</span></span><br><span class="line"> 		<span class="string">s&quot;<span class="subst">$real</span><span class="subst">$sign</span><span class="subst">$&#123;imaginary&#125;</span>i&quot;</span></span><br><span class="line"> 	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

        <h4 id="权限控制"   >
          <a href="#权限控制" class="heading-link"><i class="fas fa-link"></i></a>权限控制</h4>
      <p>在不使用任何访问修饰符的情况下，Scala 默认认为类、字段和方法都是公开的。不用在额外写 public 关键字。</p>
]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>Schema - Describe Structure of Data</title>
    <url>/posts/4374198/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>Schema 描述并规范数据的结构组成。在 Spark SQL 中，你所处理的每个 df， ds 都有自己的 schema。</p>
<a id="more"></a>


        <h3 id="Schema"   >
          <a href="#Schema" class="heading-link"><i class="fas fa-link"></i></a>Schema</h3>
      <p>Schema 可以是隐式的（在执行过程中推断出 schema），也可以是显式的（指定 schema 并在编译期检查）。Schema 用 StructType 和 StructField 声明。你可以使用 printTreeString 或者 prettyJson 来输出更美观的 schema。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> schemaUntyped = <span class="keyword">new</span> <span class="type">StructType</span>().add(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;int&quot;</span>)</span><br><span class="line">schemaUntyped: org.apache.spark.sql.types.<span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(a,<span class="type">IntegerType</span>,<span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; schemaUntyped.printTreeString</span><br><span class="line">root</span><br><span class="line"> |-- a: integer (nullable = <span class="literal">true</span>)</span><br><span class="line"><span class="type">Spark</span> <span class="number">2.</span>x 中，使用 <span class="type">Encoder</span> 来解析描述 <span class="type">Dataset</span> 的 schema</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.sql.<span class="type">Encoders</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">Encoders</span>.<span class="type">INT</span>.schema.printTreeString</span><br><span class="line">root</span><br><span class="line"> |-- value: integer (nullable = <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">Encoders</span>.product[(<span class="type">String</span>, java.sql.<span class="type">Timestamp</span>)].schema.printTreeString</span><br><span class="line">root</span><br><span class="line">|-- _1: string (nullable = <span class="literal">true</span>)</span><br><span class="line">|-- _2: timestamp (nullable = <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">id: <span class="type">Long</span>, name: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">Encoders</span>.<span class="title">product</span>[<span class="type">Person</span>].<span class="title">schema</span>.<span class="title">printTreeString</span></span></span><br><span class="line"><span class="class"><span class="title">root</span></span></span><br><span class="line"><span class="class"> <span class="title">|--</span> <span class="title">id</span></span>: long (nullable = <span class="literal">false</span>)</span><br><span class="line"> |-- name: string (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></div></figure>

<p>上面提到每个 df 和 ds 都有自己的 schema，不管是隐式还是显式的。可以使用 printSchema 来得到。printSchema 在我们调试处理一些比较复杂的数据结构时非常有用。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = <span class="type">Seq</span>((<span class="number">1</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">4</span>)).toDF(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;num&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [id: int, num: int]</span><br><span class="line"></span><br><span class="line">scala&gt; df.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- id: integer (nullable = <span class="literal">false</span>)</span><br><span class="line"> |-- num: integer (nullable = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; df.schema</span><br><span class="line">res5: org.apache.spark.sql.types.<span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(id,<span class="type">IntegerType</span>,<span class="literal">false</span>), <span class="type">StructField</span>(num,<span class="type">IntegerType</span>,<span class="literal">false</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; df.schema(<span class="string">&quot;id&quot;</span>).dataType</span><br><span class="line">res6: org.apache.spark.sql.types.<span class="type">DataType</span> = <span class="type">IntegerType</span></span><br></pre></td></tr></table></div></figure>

        <h3 id="StructType"   >
          <a href="#StructType" class="heading-link"><i class="fas fa-link"></i></a>StructType</h3>
      <p>StructType 是用来定义声明 schema 的数据类型。可看作是 StructField 的集合，与 DDL 数据库定义语言可以互相转换。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; df.schema foreach println</span><br><span class="line"><span class="type">StructField</span>(id,<span class="type">IntegerType</span>,<span class="literal">false</span>)</span><br><span class="line"><span class="type">StructField</span>(num,<span class="type">IntegerType</span>,<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; df.schema.toDDL</span><br><span class="line">res9: <span class="type">String</span> = `id` <span class="type">INT</span>,`num` <span class="type">INT</span></span><br><span class="line"></span><br><span class="line">scala&gt; df.schema.sql</span><br><span class="line">res10: <span class="type">String</span> = <span class="type">STRUCT</span>&lt;`id`: <span class="type">INT</span>, `num`: <span class="type">INT</span>&gt;</span><br></pre></td></tr></table></div></figure>
<p>我们可以通过 schema 的 add 方法来创建一层或者多层嵌套的 schema。在日常工作中，处理一些无表头类 csv 格式文件时，我们可以通过 map 快速生成统一类型的 schema。如果你要硬指定类型的话，需要确保数据质量足够好，同一字段不会存在类型不一致的现象，否则根据 schema 读取出来的数据会出现大量 Null。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> cols = <span class="type">Seq</span>(<span class="string">&quot;session_id&quot;</span>, <span class="string">&quot;original_id&quot;</span>, <span class="string">&quot;time&quot;</span>, <span class="string">&quot;timestamp&quot;</span>, <span class="string">&quot;$ip&quot;</span>, <span class="string">&quot;computer_id&quot;</span>, <span class="string">&quot;distinct_id&quot;</span>, <span class="string">&quot;click&quot;</span>, <span class="string">&quot;value&quot;</span>, <span class="string">&quot;$url&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> inferSchema = <span class="type">StructType</span>(cols.map(<span class="type">StructField</span>(_, <span class="type">StringType</span>)))</span><br></pre></td></tr></table></div></figure>

        <h3 id="StructField"   >
          <a href="#StructField" class="heading-link"><i class="fas fa-link"></i></a>StructField</h3>
      <p>StructField 用来描述每列的类型，一般由列名，数据类型，nullable，comment（默认为空） 组成。同样与 DDL 数据库定义语言可以互相转换</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; df.schema(<span class="string">&quot;id&quot;</span>)</span><br><span class="line">res11: org.apache.spark.sql.types.<span class="type">StructField</span> = <span class="type">StructField</span>(id,<span class="type">IntegerType</span>,<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; df.schema(<span class="string">&quot;id&quot;</span>).getComment</span><br><span class="line">res12: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.sql.types.<span class="type">MetadataBuilder</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.<span class="type">MetadataBuilder</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> metadata = <span class="keyword">new</span> <span class="type">MetadataBuilder</span>().putString(<span class="string">&quot;comment&quot;</span>, <span class="string">&quot;id&quot;</span>).build</span><br><span class="line">metadata: org.apache.spark.sql.types.<span class="type">Metadata</span> = &#123;<span class="string">&quot;comment&quot;</span>:<span class="string">&quot;id&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">LongType</span>, <span class="type">StructField</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">LongType</span>, <span class="type">StructField</span>&#125;</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> f = <span class="keyword">new</span> <span class="type">StructField</span>(name = <span class="string">&quot;id&quot;</span>, dataType = <span class="type">LongType</span>, nullable = <span class="literal">false</span>, metadata)</span><br><span class="line">f: org.apache.spark.sql.types.<span class="type">StructField</span> = <span class="type">StructField</span>(id,<span class="type">LongType</span>,<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; f.toDDL</span><br><span class="line">res13: <span class="type">String</span> = `id` <span class="type">BIGINT</span> <span class="type">COMMENT</span> <span class="symbol">&#x27;i</span>d&#x27;</span><br></pre></td></tr></table></div></figure>

]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkSession - The Entry Point to Spark SQL</title>
    <url>/posts/f3423d06/</url>
    <content><![CDATA[
        <h2 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h2>
      <p>根据 Spark 的架构，我们知道 driver 端通过 SparkContext 实例来控制程序的运行。在 Spark 2.X 里，提供了 SparkContext 的上层 SparkSession，两者之间可以互相转化。可以说，我们开发 Spark SQL 应用程序首先就要创建 SparkSession。</p>
<a id="more"></a>


        <h2 id="SparkSession"   >
          <a href="#SparkSession" class="heading-link"><i class="fas fa-link"></i></a>SparkSession</h2>
      <p>我们通过 SparkSession.Builder 来创建 SparkSession 实例。一旦 SparkSession 被创建，我们就可以使用  SparkSession 来创建 DataSet 来进行后续数据的计算。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apcahe.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">	.builder()</span><br><span class="line">	.appName(<span class="string">&quot;My First Processor&quot;</span>)</span><br><span class="line">    .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">	.enableHiveSupport() </span><br><span class="line">	.config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&quot;target/spark-warehouse&quot;</span>)</span><br><span class="line">	.withExtensions &#123; extensions =&gt;</span><br><span class="line">    extensions.injectResolutionRule &#123; session =&gt;</span><br><span class="line">      ...</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  .getOrCreate</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = <span class="type">Seq</span>((<span class="string">&quot;Mike&quot;</span>, <span class="number">11</span>,), (<span class="string">&quot;Jam&quot;</span>, <span class="number">23</span>)).toDF(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line"></span><br><span class="line">df.show()</span><br></pre></td></tr></table></div></figure>
<p>通过 SparkSession，我们可以调用其 creatDataFrame 等函数，DataFrameReader API 等加载任意数据源，后面会有单独的章节讲 DataFrameReader。</p>
<p>tip：SparkSession.builder.withExtensions 可以用于新增自定义规则，像 OptimizerRule，ParseRule。 场景：数据查询平台中枢每天接受大量的 Sql 请求，可以通过自定义 Check 规则来过滤掉每个 session 提交的不合理请求。</p>
<p>spark-shell 会自动为我们创建变量名为 spark 的 SparkSession 对象。供我们调用各种 API 进行调试与学习。并且自动引入了隐式转换。现在打开 spark-shell，输入以下代码试一下</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">spark.range(<span class="number">5</span>).show()</span><br><span class="line"><span class="keyword">val</span> a = <span class="type">Seq</span>((<span class="string">&quot;Mike&quot;</span>, <span class="number">11</span>,), (<span class="string">&quot;Jam&quot;</span>, <span class="number">23</span>)).toDF(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br></pre></td></tr></table></div></figure>
<p>spark-shell 默认 enableHiveSupport() ，使用 hive metastore 进行元数据管理。可以通过以下命令使用内存进行元数据管理。涉及到的对象是 Spark.catalog。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">spark-shell --conf spark.sql.catalogImplementation=<span class="keyword">in</span>-memory</span><br></pre></td></tr></table></div></figure>
<p>下面看下 SparkSession 常见的 API</p>
<div class="table-container"><table>
<thead>
<tr>
<th>Method</th>
<th>Description</th>
<th>example</th>
</tr>
</thead>
<tbody><tr>
<td>builder</td>
<td>创建 SparkSession</td>
<td><br><code>val spark = SparkSession</code></br><br><code>.builder.</code></br><br><code>.master(&quot;local[*]&quot;)</code></br><br><code>.getOrCreate</code></br></td>
</tr>
<tr>
<td>catalog</td>
<td>元数据管理</td>
<td>spark.catalog</td>
</tr>
<tr>
<td>createDataFrame</td>
<td>通过 RDD 和 schema 创建 DF，重载方法，接受的参数都不一样</td>
<td><br><code>import org.apache.spark.sql.types._</code></br><br><code>import org.apache.spark.sql.Row</code></br><br><code>val schema = new StructType().add($&quot;id&quot;.int)</code></br><br><code>val rdd = spark.sparkContext.parallelize(Seq(Row(1), Row(2)))</code></br><br><code>val df = spark.createDataFrame(rdd, schema)</code></br></td>
</tr>
<tr>
<td>emptyDataFrame</td>
<td>空 DF</td>
<td><code>spark.emptyDataFrame.show()</code></td>
</tr>
<tr>
<td>implicits</td>
<td>引入隐式转换</td>
<td><br><code>import spark.implicits._</code></br><br><code>$&quot;id&quot;.toString</code></br></td>
</tr>
<tr>
<td>newSession</td>
<td>新建一个 SparkSession</td>
<td></td>
</tr>
<tr>
<td>read    创建 DataFrameReader</td>
<td><code>spark.read.json(input_path)</code></td>
<td></td>
</tr>
<tr>
<td>range    创建 DataSet[java.lang.Long]</td>
<td><code>spark.range(5).show()</code></td>
<td></td>
</tr>
<tr>
<td>sql    执行 sql 语句</td>
<td><code>spark.sql(&quot;show tables&quot;)</code></td>
<td></td>
</tr>
<tr>
<td>stop    停止关联的 SparkContext</td>
<td><code>spark.stop()</code></td>
<td></td>
</tr>
</tbody></table></div>

        <h2 id="Builder-API"   >
          <a href="#Builder-API" class="heading-link"><i class="fas fa-link"></i></a>Builder API</h2>
      <p>Builder API 用来创建 SparkSession。这里介绍经常用到的方法。</p>
<div class="table-container"><table>
<thead>
<tr>
<th>Method</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>appName</td>
<td>应用程序的名称</td>
</tr>
<tr>
<td>config</td>
<td>设置配置项，可以设置 core 个数等</td>
</tr>
<tr>
<td>enableHiveSupport</td>
<td>使用 hive metastore 作为 catalog</td>
</tr>
<tr>
<td>master</td>
<td>spark master，比如 local[*]，YARN</td>
</tr>
<tr>
<td>getOrCreate</td>
<td>获取一个SparkSession 示例，如果获取不到就创建</td>
</tr>
<tr>
<td>withExtensions</td>
<td>自定义规则拓展</td>
</tr>
</tbody></table></div>

        <h2 id="Implicits-Object"   >
          <a href="#Implicits-Object" class="heading-link"><i class="fas fa-link"></i></a>Implicits Object</h2>
      <p>隐式对象提供了将 scala 对象，比如 Seq，String，转换成 DataSet，DataFrame，Coulmn 等 spark 数据结构的方法。简而言之，就是为我们提供便捷。</p>
<div class="table-container"><table>
<thead>
<tr>
<th>Method</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>localSeqToDatasetHolder</td>
<td>scala 的 <code>Seq[T]  =&gt; DataSet[T]</code></td>
</tr>
<tr>
<td>Encoders：Spark SQL 用来序列化/反序列化的一个类。主要用于 DataSet。本质上每次调用toDS() 函数的时候都调用了 Encoder</td>
<td></td>
</tr>
<tr>
<td>StringToColumn</td>
<td><code>$&quot;name&quot; =&gt; Column</code>，这也是我们经常看到 Spark 代码里操作列的写法。在后面的 Column 章节会详细讲到</td>
</tr>
<tr>
<td>rddToDatasetHolder</td>
<td><code>rdd =&gt; dataset</code></td>
</tr>
<tr>
<td>symbolToColumn</td>
<td><code>symbol =&gt; Column</code></td>
</tr>
</tbody></table></div>
<p>implicits object 定义在 SparkSession Object 里面，所以创建 SparkSession 后才可以引用。像下面的这些方法都是通过隐式转换实现的</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">=&gt;</span> <span class="title">Dataset</span>[<span class="type">T</span>]</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">import</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">sql</span>.<span class="title">SparkSession</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">spark</span> </span>= <span class="type">SparkSession</span>.builder.master(<span class="string">&quot;local[*]&quot;</span>).getOrCreate</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df1 = <span class="type">Seq</span>(<span class="string">&quot;implicits&quot;</span>).toDS</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df2 = <span class="type">Seq</span>(<span class="string">&quot;implicits&quot;</span>).toDF</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df3 = <span class="type">Seq</span>(<span class="string">&quot;implicits&quot;</span>).toDF(<span class="string">&quot;text&quot;</span>)</span><br><span class="line"></span><br><span class="line">$<span class="string">&quot;name&quot;</span>.toString</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd = spark.sparkContext..parallelize(<span class="type">Seq</span>(<span class="type">Row</span>(<span class="number">1</span>), <span class="type">Row</span>(<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">rdd.toDS</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">People</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">people</span> </span>= <span class="type">Seq</span>(<span class="type">People</span>(<span class="string">&quot;Mike&quot;</span>, <span class="number">21</span>), <span class="type">People</span>(<span class="string">&quot;Jam&quot;</span>, <span class="number">20</span>)).toDS</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Shuffle</title>
    <url>/posts/baf74228/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>在学习很多大数据处理框架时，我们都会听到 Shuffle 。那么 Shuffle 到底是什么？为什么需要 Shuffle 的存在呢？</p>
<a id="more"></a>


        <h3 id="Shuffle-是什么"   >
          <a href="#Shuffle-是什么" class="heading-link"><i class="fas fa-link"></i></a>Shuffle 是什么</h3>
      <p>我们常理解的 shuffle 就是洗牌过程，将有规则的牌打乱成无规则的。但是在数据处理过程中，shuffle 是为了将无规则的数据洗成有规则的（将分布在不同节点上的数据按照一定的规则汇聚到一起）。<br>对于 MR 来讲，shuffle 连接了 map 阶段和 reduce 阶段。<br>对于 Spark 来讲，可以理解为 shuffle 连接了不同的 stage 阶段。</p>

        <h3 id="为什么需要-Shuffle"   >
          <a href="#为什么需要-Shuffle" class="heading-link"><i class="fas fa-link"></i></a>为什么需要 Shuffle</h3>
      <p>对于 MR 来讲，受限于其编程模型，reduce 阶段处理的必须是相同 key 的数据，所以必然需要 shuffle。<br>对于 Spark 来讲，其依靠 DAG 描述计算逻辑，其中没有宽依赖时，不会进行 shuffle。遇到宽依赖时，会划分 stage，从而需要 shuffle。这也是 Spark 相比 MR 更快的原因之一（因为 shuffle 阶段涉及大量的磁盘读写和网络 IO，直接影响着整个程序的性能和吞吐量）。</p>

        <h3 id="Spark-Shuffle"   >
          <a href="#Spark-Shuffle" class="heading-link"><i class="fas fa-link"></i></a>Spark Shuffle</h3>
      <p>Spark shuffle 的发展主要经历了以下几个阶段。</p>
<ul>
<li>HashShuffle。</li>
<li>引入 File Consolidation 机制，优化了 HashShuffle。</li>
<li>SortMergeShuffle + bypass 机制。</li>
<li>Unsafe Shuffle。</li>
<li>合并 Unsafe Shuffle 到 SortMergeShuffle。</li>
<li>Spark 2.0 后，完全去除 HashShuffle。</li>
</ul>
<p>shuffle 过程其实主要分为两个阶段，shuffle read 和 shuffle write。</p>
<ul>
<li>shuffle write 其实就是 map -&gt; partition -&gt; (sort) -&gt; (combine)对数据按照指定规则进行分区排序预聚合，为下游 reduce task 处理做准备。</li>
<li>shuffle read 其实就是 featch -&gt; merge -&gt; reduce ，对上游 map task 生成的数据进行拉取处理。我们可以从以下四个问题的角度来了解 shuffle read。<ul>
<li>什么时候 fetch 数据？</li>
<li>以什么样的方式 fetch 数据，是边 fetch 边处理还是像 MR 一样等待一批数据准备好后在 fetch?</li>
<li>fetch 的数据存放在哪里？</li>
<li>下游 reduce task 如何获取 fetch 的数据的存放位置？<br>下面就是讨论各种 shuffle 机制下是如何进行 shuffle read 和 shuffle  write 的。以下讨论的假设前提是每个 executor 拥有一个 cpu core，每个 cpu core 上运行一个 task</li>
</ul>
</li>
</ul>

        <h4 id="HashShuffle"   >
          <a href="#HashShuffle" class="heading-link"><i class="fas fa-link"></i></a>HashShuffle</h4>
      <p>未经优化的 HashShuffle 流程如图<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/0081Kckwgy1gky7w8fltij31nf0tkdln.jpg"></p>
<ul>
<li><p>shuffle write<br>首先对数据按照 key 进行分区，相同 key 的数据会先保存在内存中，达到一定阈值后，溢写到磁盘文件，每个上游 task 相同 key 的数据一定会落到相同的磁盘文件中。<br>每个上游 map task 都要为下游所有的 reduce task 生成处理后的文件。所以产生的中间结果文件数量达到 M*R。碎片文件过多，对磁盘 IO 影响比较大。 </p>
</li>
<li><p>shuffle read<br>为了迎合 stage 的概念，可以理解为在上一个 stage task 都结束后，开始 shuffle  read。在此过程中 reduce task 边 fetch  边处理数据，底层使用类  hash map - appendonly map 进行数据 fetch 处理结果的保存，阈值达到 70%  spill 到文件。部分算子会选用 extrendappendonly map，本质是 appendonly map + sort 压实。</p>
</li>
</ul>

        <h4 id="File-Consolidation-优化后的-HashShuffle"   >
          <a href="#File-Consolidation-优化后的-HashShuffle" class="heading-link"><i class="fas fa-link"></i></a>File Consolidation 优化后的 HashShuffle</h4>
      <p>引入 File Consolidation 优化后的 HashShuffle 后的流程如图<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/0081Kckwgy1gky7x21hdvj31io0r0jva.jpg"></p>
<ul>
<li><p>shuffle write<br>在图中也可以看出来，一个 cpu core 上多次运行的 task 最终只会生成一个磁盘文件。类似于组的概念。最终生成的磁盘文件数量为 core * R。在 executor 具有多个 cpu core 的时候可以提升很多性能。可以设置一下参数打开此机制</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">spark.shuffle.consolidateFiles &#x3D; true</span><br></pre></td></tr></table></div></figure>
</li>
<li><p>shuffle read<br>shuffle read 过程没有太大改变</p>
</li>
</ul>

        <h4 id="SortMergeShuffle"   >
          <a href="#SortMergeShuffle" class="heading-link"><i class="fas fa-link"></i></a>SortMergeShuffle</h4>
      <p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/0081Kckwgy1gky8ithp1fj319a0ts41t.jpg"></p>
<ul>
<li>shuffle write<br>在该模式下， 数据会先写入到内存数据结构中，边 map 边聚合，达到阈值后便会将数据溢写到磁盘并清空该数据结构。在写磁盘前，会先对数据进行排序，并分批写入磁盘文件，写入过程中，数据会先放进内存缓冲区，缓冲区满后在刷到磁盘，虽然减少了磁盘 IO ，但是此时最多可能会同时开 M*R 个buffer 缓冲区，对内存的压力也是比较大的。一般一个 shuffle map task 会发生多次溢写生成多个磁盘文件，最后会 merge 成一个磁盘文件，并生成一个文件段索引文件，为下游 reduce task 的拉取提供帮助。<br>生成磁盘文件数量就是 shuffle map task 的数量</li>
<li>shuffle read<br>shuffle read 过程没有太大改变</li>
</ul>

        <h4 id="bypass-机制"   >
          <a href="#bypass-机制" class="heading-link"><i class="fas fa-link"></i></a>bypass 机制</h4>
      <p>bypass 机制起作用下的 sortmerge shuffle。本质上和 hashshuffle 差不多，只是在 shuffle write 后 多了一层 merge。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/0081Kckwgy1gky8p4vzwyj312k0o0jub.jpg"><br>当满足以下条件是，会舍弃掉 sort 阶段 </p>
<ul>
<li><p>shuffle map task 的数量小于 spark.shuffle.sort.bypassMergeThreshold</p>
</li>
<li><p>非聚合类的 shuffle 算子</p>
</li>
<li><p>shuffle write<br>shuffle map task 对数据按照 key hash 进行分区，并 spill 到文件。最终对每个 map task 生成的多个文件进行一次 merge。</p>
</li>
</ul>

        <h4 id="Spark-Shuffle-参数"   >
          <a href="#Spark-Shuffle-参数" class="heading-link"><i class="fas fa-link"></i></a>Spark Shuffle 参数</h4>
      <p>通过上面也可以看到 shuffle 阶段涉及到大量的 spill 和开 buffer。如何减少 spill 次数降低磁盘 IO 呢，首先想到的就是调节 buffer 的大小，在内存资源充足的情况下，增大这个参数</p>
<ul>
<li>spark.shuffle.file.buffer - spark write task 溢写到磁盘文件时的内存缓冲区大小</li>
<li>spark.reducer.maxSizeInFlight - spark read task 拉取数据时的 buffer 缓冲区大小</li>
<li>spark.shuffle.memoryFraction - Executor 内存中，分配给 shuffle read task 进行聚合操作的内存比例，默认是 20%。</li>
</ul>

        <h4 id="总结"   >
          <a href="#总结" class="heading-link"><i class="fas fa-link"></i></a>总结</h4>
      <p>很多文章说 Spark 内存计算是相比 MR 来说更快的原因之一，其实看过 shuffle 之后，会发现这个说法是不严谨的。程序只要运行就必须加载到内存中，MR 也是如此。Spark 更快速的原因是基于 DAG 构造了一个数据 pipeline，中间结果会优先放到内存，内存放不下了就会自动下放到磁盘，并且具体到各个算子，其会灵活的使用各种数据结构来优化内存的使用，减少 spill 到磁盘的个数。<br>以上只是批处理中的 shuffle，流式 shuffle 又是什么样子的呢，这是一个值得思考学习的问题。 </p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark小知识点</title>
    <url>/posts/e220000e/</url>
    <content><![CDATA[
        <h4 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h4>
      <p>工作中碰到的一些 Spark 的问题</p>
<a id="more"></a>


        <h4 id="spark-load-impala"   >
          <a href="#spark-load-impala" class="heading-link"><i class="fas fa-link"></i></a>spark load impala</h4>
      <p>采用 jdbc format，不过需要注意，ImpalaJDBC41.jar 的版本。在 Maven 中央仓库中的 ImpalaJDBC41 的依赖都是不管用的。因此需要自己去下载并放入 libs 目录 <span class="exturl"><a class="exturl__link"   href="https://www.cloudera.com/downloads/connectors/impala/jdbc/2-6-4.html" >下载地址</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>。如果采用 sbt 打包管理的话，在 build.sbt 中加入以下内容用于标识本地依赖 jar 的路径。</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">unmanagedBase :&#x3D; baseDirectory.value &#x2F; &quot;libs&quot;</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>
<p>连接 impala 代码示例，这里加了一些参数，<code>numPartitions,partitionColumn,lowerBound,upperBound</code> 用于调节 load 数据时的并发度。不加这些参数的时候。默认只有一个 task 在加载数据，大数据量的情况下相对较慢。建议 jdbc format 的数据源都调节一下这些参数，以达到一个理想的并发度。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">spark.read</span><br><span class="line">  .format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;url&quot;</span>, <span class="string">s&quot;<span class="subst">$impala_url</span>&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;driver&quot;</span>, <span class="string">&quot;com.cloudera.impala.jdbc41.Driver&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;users&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;numPartitions&quot;</span>, <span class="number">32</span>)</span><br><span class="line">  .option(<span class="string">&quot;partitionColumn&quot;</span>, <span class="string">&quot;id&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;lowerBound&quot;</span>, <span class="string">&quot;1&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;upperBound&quot;</span>, <span class="string">&quot;100000&quot;</span>)</span><br><span class="line">  .load()</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>


        <h4 id="spark-非等值-join-的执行计划"   >
          <a href="#spark-非等值-join-的执行计划" class="heading-link"><i class="fas fa-link"></i></a>spark 非等值 join 的执行计划</h4>
      <p>默认使用 BNLJ，及 BrocastNestLoopJoin，执行效率会非常低。如果查询条件比较少，此时可以想办法转换为等值连接来优化。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; array.join(b, array_contains($<span class="string">&quot;id_array&quot;</span>, $<span class="string">&quot;id&quot;</span>)).explain(<span class="literal">true</span>)</span><br><span class="line">== <span class="type">Parsed</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line"><span class="type">Join</span> <span class="type">Inner</span>, array_contains(id_array#<span class="number">7</span>, id#<span class="number">19</span>)</span><br><span class="line">:- <span class="type">Project</span> [id_string#<span class="number">5</span>, split(id_string#<span class="number">5</span>,  ) <span class="type">AS</span> id_array#<span class="number">7</span>]</span><br><span class="line">:  +- <span class="type">Project</span> [value#<span class="number">3</span> <span class="type">AS</span> id_string#<span class="number">5</span>]</span><br><span class="line">:     +- <span class="type">LocalRelation</span> [value#<span class="number">3</span>]</span><br><span class="line">+- <span class="type">Project</span> [cast(id#<span class="number">0</span>L as string) <span class="type">AS</span> id#<span class="number">19</span>]</span><br><span class="line">   +- <span class="type">Range</span> (<span class="number">0</span>, <span class="number">5</span>, step=<span class="number">1</span>, splits=<span class="type">Some</span>(<span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">== <span class="type">Analyzed</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line">id_string: string, id_array: array&lt;string&gt;, id: string</span><br><span class="line"><span class="type">Join</span> <span class="type">Inner</span>, array_contains(id_array#<span class="number">7</span>, id#<span class="number">19</span>)</span><br><span class="line">:- <span class="type">Project</span> [id_string#<span class="number">5</span>, split(id_string#<span class="number">5</span>,  ) <span class="type">AS</span> id_array#<span class="number">7</span>]</span><br><span class="line">:  +- <span class="type">Project</span> [value#<span class="number">3</span> <span class="type">AS</span> id_string#<span class="number">5</span>]</span><br><span class="line">:     +- <span class="type">LocalRelation</span> [value#<span class="number">3</span>]</span><br><span class="line">+- <span class="type">Project</span> [cast(id#<span class="number">0</span>L as string) <span class="type">AS</span> id#<span class="number">19</span>]</span><br><span class="line">   +- <span class="type">Range</span> (<span class="number">0</span>, <span class="number">5</span>, step=<span class="number">1</span>, splits=<span class="type">Some</span>(<span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">== <span class="type">Optimized</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line"><span class="type">Join</span> <span class="type">Inner</span>, array_contains(id_array#<span class="number">7</span>, id#<span class="number">19</span>)</span><br><span class="line">:- <span class="type">LocalRelation</span> [id_string#<span class="number">5</span>, id_array#<span class="number">7</span>]</span><br><span class="line">+- <span class="type">Project</span> [cast(id#<span class="number">0</span>L as string) <span class="type">AS</span> id#<span class="number">19</span>]</span><br><span class="line">   +- <span class="type">Range</span> (<span class="number">0</span>, <span class="number">5</span>, step=<span class="number">1</span>, splits=<span class="type">Some</span>(<span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">== <span class="type">Physical</span> <span class="type">Plan</span> ==</span><br><span class="line"><span class="type">BroadcastNestedLoopJoin</span> <span class="type">BuildLeft</span>, <span class="type">Inner</span>, array_contains(id_array#<span class="number">7</span>, id#<span class="number">19</span>)</span><br><span class="line">:- <span class="type">BroadcastExchange</span> <span class="type">IdentityBroadcastMode</span></span><br><span class="line">:  +- <span class="type">LocalTableScan</span> [id_string#<span class="number">5</span>, id_array#<span class="number">7</span>]</span><br><span class="line">+- *(<span class="number">1</span>) <span class="type">Project</span> [cast(id#<span class="number">0</span>L as string) <span class="type">AS</span> id#<span class="number">19</span>]</span><br><span class="line">   +- *(<span class="number">1</span>) <span class="type">Range</span> (<span class="number">0</span>, <span class="number">5</span>, step=<span class="number">1</span>, splits=<span class="number">8</span>)</span><br></pre></td></tr></table></div></figure>


        <h4 id="Spark-SQL-处理多层嵌套数据"   >
          <a href="#Spark-SQL-处理多层嵌套数据" class="heading-link"><i class="fas fa-link"></i></a>Spark SQL 处理多层嵌套数据</h4>
      <p>写代码要以尽量简洁的形式表达出想要的意思，避免代码的冗余。如果要取多层嵌套数据中的某些字段做处理，比如像下面的的 schema，是大量的 getFiled 还是采用其他的办法呢。</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- ak: string (nullable &#x3D; true)</span><br><span class="line"> |-- pl: string (nullable &#x3D; true)</span><br><span class="line"> |-- usr: struct (nullable &#x3D; true)</span><br><span class="line"> |    |-- did: string (nullable &#x3D; true)</span><br><span class="line"> |-- ut: string (nullable &#x3D; true)</span><br><span class="line"> |-- ip: string (nullable &#x3D; true)</span><br><span class="line"> |-- st: long (nullable &#x3D; true)</span><br><span class="line"> |-- ua: string (nullable &#x3D; true)</span><br><span class="line"> |-- data: struct (nullable &#x3D; true)</span><br><span class="line"> |    |-- dt: string (nullable &#x3D; true)</span><br><span class="line"> |    |-- pr: struct (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $an: string (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $br: string (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $cn: string (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $cr: string (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $ct: long (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $cuid: string (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $dru: long (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $dv: string (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $eid: string (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $imei: string (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $jail: long (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $lang: string (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $mkr: string (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $mnet: string (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $net: string (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $os: string (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $ov: string (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $private: long (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $ps: string (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $rs: string (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $sc: long (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $sid: long (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $ss_name: string (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $tz: long (nullable &#x3D; true)</span><br><span class="line"> |    |    |-- $vn: string (nullable &#x3D; true)</span><br></pre></td></tr></table></div></figure>
<p>在字段具有相似特征的情况下可以用以下方法处理，得到所有列的集合在做后续处理。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"> </span><br><span class="line"><span class="keyword">val</span> pr = $<span class="string">&quot;data&quot;</span>.getFiled(<span class="string">&quot;pr&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> prDF = testDataDF.select(<span class="string">&quot;data.pr.*&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> prColumns = prDF.columns.filter(_.startsWith(<span class="string">&quot;$&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> custom_columns = prColumns.map(x =&gt; pr.getField(x).alias(x)).toSeq</span><br></pre></td></tr></table></div></figure>


        <h4 id="Spark-UDF-WrappedArray"   >
          <a href="#Spark-UDF-WrappedArray" class="heading-link"><i class="fas fa-link"></i></a>Spark UDF WrappedArray</h4>
      <p>UDF 在接收 Array 作为参数的时候，类型其实是 WrappedArray。</p>

        <h4 id="Spark-UDF-接受常量作为参数"   >
          <a href="#Spark-UDF-接受常量作为参数" class="heading-link"><i class="fas fa-link"></i></a>Spark UDF 接受常量作为参数</h4>
      <p>使用 lit 或者 typedlit 包装一下，成为一个常量 Column 在处理。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> getScreen = udf((t: <span class="type">String</span>, index: <span class="type">Int</span>) =&gt;</span><br><span class="line">    t <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> t <span class="keyword">if</span> !t.contains(<span class="string">&quot;*&quot;</span>) =&gt; <span class="string">&quot;&quot;</span></span><br><span class="line">      <span class="keyword">case</span> t <span class="keyword">if</span> t.split(<span class="string">&quot;\\*&quot;</span>).length.equals(<span class="number">2</span>) =&gt; t.split(<span class="string">&quot;\\*&quot;</span>)(index)</span><br><span class="line">      <span class="keyword">case</span> _ =&gt; <span class="string">&quot;&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">  )</span><br><span class="line">getScreen($<span class="string">&quot;Screen&quot;</span>, lit(<span class="number">0</span>))</span><br></pre></td></tr></table></div></figure>


        <h4 id="Spark-SQL-lit-与-typedLit"   >
          <a href="#Spark-SQL-lit-与-typedLit" class="heading-link"><i class="fas fa-link"></i></a>Spark SQL lit 与 typedLit</h4>
      <p>typedLit 用于包装 scala 的数据结构类型，比如 Seq，Map 等。<br>lit 用于包装简单数据类型，比如 int，string 等。</p>

        <h4 id="cast-强转类型的坑"   >
          <a href="#cast-强转类型的坑" class="heading-link"><i class="fas fa-link"></i></a>cast 强转类型的坑</h4>
      <p>针对数据质量较差的数据做类型转换时，建议使用 UDF 。因为 cast 转换失败就为 null，容易丢数据。可以查看 <code>org.apache.spark.sql.catalyst.expressions.Cast.scala</code></p>

        <h4 id="spark-Ambiguous-reference-to-fields-StructField"   >
          <a href="#spark-Ambiguous-reference-to-fields-StructField" class="heading-link"><i class="fas fa-link"></i></a>spark Ambiguous reference to fields StructField</h4>
      <p>spark 默认大小写不敏感，这就导致如果你的数据中存在 name 和 Name，就会认为是相同的列，从而抛出此错误，此时可以开启大小写敏感</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">    .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    .appName(<span class="string">&quot;zhugeProcessor&quot;</span>)</span><br><span class="line">    .config(<span class="string">&quot;spark.sql.caseSensitive&quot;</span>, <span class="literal">true</span>)</span><br><span class="line">    .getOrCreate()</span><br></pre></td></tr></table></div></figure>


]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark简介</title>
    <url>/posts/aaf17d15/</url>
    <content><![CDATA[
        <h2 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h2>
      <p>Apache Spark 是一个快速的通用集群计算系统。它提供了Java， Scala， Python ，R 四种编程语言的 API 编程接口和基于 DAG 图执行的优化引擎。它还支持一系列丰富的高级工具：处理结构化数据的 Spark SQL，用于机器学习的 MLlib，控制图、并行图操作和计算的一组算法和工具的集合 GraphX，数据流处理 Spark Streaming。</p>
<a id="more"></a>

<p>本栏目主要讲解的是 Spark SQL。Spark SQL 是用来处理结构化数据的 Spark 模块。与底层的 RDD API 不同的是，Spark SQL 提供的接口为 Spark 提供了更多的数据结构和计算信息。在内部，Spark SQL 会根据这些额外的信息来执行额外的优化。我们可以通过 SQL 和 DataSet API 来与 Spark SQL 进行交互。当计算结果时，会使用相同的 SQL 执行引擎，而与你用来实现计算的 API/语言无关。这种统一意味着开发人员可以很容易地在不同的 API 之间来回切换，基于这些 Spark 提供了非常自然的转换表达式。</p>
<p>我个人是在 2019 年 3 月份开始接触 Spark，说实话 Spark 非常容易上手，其接口简洁易懂，尤其是和 Scala 的结合，有时候每个数据处理单元不超过十行代码就可以完成对 T 级别数据的处理，并且速度非常高效。抛却编程语言，只要你会 SQL ，就可以将你的分析任务转换为 Spark 作业来处理。这点对除程序员外的分析人员也是非常友好的。当然其也有限制使用的场景，这些在本栏目下面都会讲到</p>

        <h2 id="基本理念"   >
          <a href="#基本理念" class="heading-link"><i class="fas fa-link"></i></a>基本理念</h2>
      
        <h3 id="RDD"   >
          <a href="#RDD" class="heading-link"><i class="fas fa-link"></i></a>RDD</h3>
      <p>弹性分布式数据集。分布在集群各个节点上可以并行处理的数据集。其实我们可以按照词的语义来分析想一下 RDD 为什么是弹性的？RDD 是如何分布式的？RDD的数据集到底指的是什么？借用一下官方文档的原话 The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel.</p>

        <h3 id="DataSet-DataFrame"   >
          <a href="#DataSet-DataFrame" class="heading-link"><i class="fas fa-link"></i></a>DataSet/DataFrame</h3>
      <p>Spark 2.X 抽象出来的上层数据结构。本质上也是并行的数据集合。提供了更多的数据操作方法。个人理解为 DataFrame 是一种特殊 DataSet，DataFrame = DataSet[Row]。而DataSet 往往指的是 DataSet[T]，T 可以是自定义的数据类型，比如 case class。</p>

        <h3 id="Task"   >
          <a href="#Task" class="heading-link"><i class="fas fa-link"></i></a>Task</h3>
      <p>具体执行的任务，分为 ShuffleMapTask 和 ResultTask。分别类似于 MapReduce 中的 Map 和 Reduce 任务。</p>

        <h3 id="Job"   >
          <a href="#Job" class="heading-link"><i class="fas fa-link"></i></a>Job</h3>
      <p>用户提交的 Spark 作业，由一个或者多个 taks 组成</p>

        <h3 id="Stage"   >
          <a href="#Stage" class="heading-link"><i class="fas fa-link"></i></a>Stage</h3>
      <p>Job 分成的阶段，一个 Spark 作业常被分为一个或者多个 Stage。 Stage 基于 RDD 的 DAG 依赖关系图进行划分。调度器从 DAG 图末端出发，遇到 ShuffleDependecy 就断开。遇到 NarrowDependecy 就加入到当前 Stage。</p>

        <h3 id="Partition"   >
          <a href="#Partition" class="heading-link"><i class="fas fa-link"></i></a>Partition</h3>
      <p>数据分区，即一个 RDD 可以被划分成多少个分区</p>

        <h3 id="Shuffle"   >
          <a href="#Shuffle" class="heading-link"><i class="fas fa-link"></i></a>Shuffle</h3>
      <p>有些运算需要将各节点上的同一类数据汇集到某一节点进行计算，把这些分布在不同节点的数据按照一定的规则汇集到一起的过程称为 Shuffle。后面会有单独的文章讲 Shuffle。</p>

        <h3 id="NarrowDependency"   >
          <a href="#NarrowDependency" class="heading-link"><i class="fas fa-link"></i></a>NarrowDependency</h3>
      <p>窄依赖，即子RDD依赖于父RDD中固定的Partition。NarrowDependency 分为 OneToOneDependency 和 RangeDependency两种。</p>

        <h3 id="ShuffleDependency"   >
          <a href="#ShuffleDependency" class="heading-link"><i class="fas fa-link"></i></a>ShuffleDependency</h3>
      <p>宽依赖，shuffle 依赖，即子 RDD 对父 RDD 中的所有 Partition 都有依赖。</p>

        <h3 id="DAG"   >
          <a href="#DAG" class="heading-link"><i class="fas fa-link"></i></a>DAG</h3>
      <p>有向无环图，RDD 之间的依赖关系</p>
<p>tip: Job， Stage，Task，DAG 都可以在 Spark Application UI 上看到。</p>

        <h2 id="使用场景"   >
          <a href="#使用场景" class="heading-link"><i class="fas fa-link"></i></a>使用场景</h2>
      <p>Spark 是 基于内存迭代的计算框架，数据集操作越频繁，操作的数据量越大越适合于 Spark。当然其对内存要求也是比较高的，因此比较重。Spark 操作小数据集并且计算密度比较大时，处理的非常慢，因此不太适合这种场景。我使用 Spark 处理过几 M 大小的碎片文件，竟然需要几十 s 甚至 1min。</p>
<p>RDD 只读并且是写操作粗粒度的，因此不支持异步细粒度更新的应用数据处理。</p>
<p>Spark 不适用于内存 hold 不住的场景，虽然在内存不足时，由于 RDD 是弹性的，其会自动下放到磁盘，但是偶尔也会出现 OOM 的问题，降低性能。</p>
<p>Spark 不适合高实时统计分析，Spark 2.X 的 Structured Streaming 也在快速发展，Continuous Processing还处在试验阶段。但是其受限于 RDD 以及其他原因，还是无法媲美 Flink  在高实时数据流方面的处理。</p>

        <h2 id="架构"   >
          <a href="#架构" class="heading-link"><i class="fas fa-link"></i></a>架构</h2>
      <p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd3wm9z12rj30gk07ydg6.jpg"></p>
<p>Cluster Manager：在 standalone 模式中即为 Master 主节点，控制整个集群，监控 worker。在 YARN 模式中为资源管理器。</p>
<p>Worker节点：从节点，负责控制计算节点，启动 Executor 或者 Driver。</p>
<p>Driver： 运行 Application 的 main() 函数。</p>
<p>Executor：执行器，某个 Application 运行在 worker node 上的一个进程。运行 task 和 管理运行过程中的数据存储</p>
<p>Spark 应用程序作为独立的进程集运行在集群上，通过 Driver  Program 中的 SparkContext 对象来进行调度。一旦连接上 Cluster Manager（YARN，Spark 自带的 Standalone Cluster），Spark 就会在对应的 Worker 上启动 executor 进程用于计算和存储应用程序运行所需要的数据。接着你的应用程序代码会被发送到各个 executor 。SparkContext 会调度生成 task 在 executor 进程中执行。</p>

        <h2 id="Spark-编程模型"   >
          <a href="#Spark-编程模型" class="heading-link"><i class="fas fa-link"></i></a>Spark 编程模型</h2>
      <p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd3wn0othhj310e0gqn9d.jpg"></p>
<p>用户使用 SparkContext 提供的 API（常用的有 textFile、sequenceFile、runJob、stop、等）编写 Driver application 程序。此外 SQLContext、HiveContext 及 StreamingContext 对 SparkContext进行封装，并提供了 SQL、Hive 及流式计算相关的 API。Spark 2.X 提供了更为方便的 SparkSession（DataFrameReader、DataFrameWriter）。</p>
<p>使用 SparkContext 提交的用户应用程序，首先会使用 BlockManager 和 BroadcastManager 将任务的 Hadoop 配置进行广播。然后由 DAGScheduler 将任务转换为 RDD 并组织成 DAG，DAG 还将被划分为不同的 Stage。最后由T askScheduler 借助 ActorSystem 将任务提交给集群管理器（Cluster Manager）。</p>
<p>集群管理器（ClusterManager）给任务分配资源，即将具体任务分配到 Worker 上，Worker 创建 Executor 来处理 task 的运行。Standalone、YARN、Mesos、EC2等都可以作为 Spark 的集群管理器。</p>

        <h2 id="Spark-计算模型"   >
          <a href="#Spark-计算模型" class="heading-link"><i class="fas fa-link"></i></a>Spark 计算模型</h2>
      <p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd3wn9j0tpj310a0dg10o.jpg"></p>
<p>Spark 是基于内存迭代计算的框架，底层数据抽象是 RDD，通过算子来对 RDD 进行转换计算，得到目标结果。</p>

        <h2 id="Spark-运行流程及特点"   >
          <a href="#Spark-运行流程及特点" class="heading-link"><i class="fas fa-link"></i></a>Spark 运行流程及特点</h2>
      
        <h3 id="运行流程"   >
          <a href="#运行流程" class="heading-link"><i class="fas fa-link"></i></a>运行流程</h3>
      <p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd3wnqk3zoj30g40cc0w6.jpg"></p>
<p>启动 SparkContext</p>
<p>SparkContext 像资源管理器申请 worker Executor资源并启动 StandaloneExecutorbackend</p>
<p>Executor 向 SparkContext 申请 Task</p>
<p>SparkContext 将应用程序分发给 Executor</p>
<p>SparkContext 构建成 DAG 图，将 DAG 图分解成 Stage、将 Taskset 发送给 Task Scheduler，最后由 Task Scheduler 将 Task 发送给 Executor 运行</p>
<p>task 执行完毕后释放资源</p>

        <h3 id="特点"   >
          <a href="#特点" class="heading-link"><i class="fas fa-link"></i></a>特点</h3>
      <p>每个应用程序都运行在自己的 executor 进程里面，在 executor 里面又以多线程来运行各种 task。这样做的好处很明显，每个 driver  都只需调度自己的 task。来自不同应用程序产生的 task 都在在不同的 JVM 中运行。当然，这也意味着，不同 SparkContext  实例之间不能共享数据，只能先存储在外部存储系统，然后在做交互。</p>
<p>Spark 与应用哪种集群管理器无关，只要可以获取 executor 进程并且 driver 和 executor 端可以保持通信，即使运行在还支持其他应用程序的的资源管理器（YARN 等）也是非常容易的</p>
<p>在应用程序的一个生命周期内， driver 和 executor 端必须可以通信，否则无法分发 task及运行信息交换。这就要求 driver 端和 各个 worker 节点是网络连通的，并且 driver 端要尽量靠近 worker 端，因为 Spark Application 运行过程中 SparkContext 和 Executor 之间有大量的信息交换。</p>

        <h2 id="RDD-运行流程"   >
          <a href="#RDD-运行流程" class="heading-link"><i class="fas fa-link"></i></a>RDD 运行流程</h2>
      <p>RDD 在 Spark 中运行的大致流程</p>
<p>1.创建 RDD 对象</p>
<p>2.DAGScheduler 模块介入运算，计算 RDD 之间的依赖关系，生成 DAG</p>
<p>3.根据 DAG 划分 Stage，上面的基本概念里也简单提到了 stage 的划分。</p>
<p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd3wobuz5jj30gh094n22.jpg"></p>
<p>基于上面的例子来划分 stage，在遇到 shuffle 依赖时，DAG 会断开依赖关系，前面的操作划分为一个 Stage，后面的继续按照这样来划分。例子中总共涉及到 RDD 的 4 次转换，action 算子collect 不会触发 RDD 的转换生成。所以在这里 groupByKey 操作涉及到 shuffle。由此 shuffle 之前的操作会作为一个 stage 来处理。最终划分的 stage 结果如下</p>
<p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd3woz9wg5j30ex0690uu.jpg"></p>
<p>Task Scheduler 会接受各个 stage 阶段划分的 task 并分发到 executor 中运行。一个 stage 阶段 task 的运行必须等待上一个 stage 阶段的 task 全部运行完。因为下一阶段的第一个转换一定是重新组织数据的，所以必须等当前阶段所有结果数据都计算出来了才能继续。</p>

        <h2 id="Spark-3-X-展望"   >
          <a href="#Spark-3-X-展望" class="heading-link"><i class="fas fa-link"></i></a>Spark 3.X 展望</h2>
      <p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd3wq5tungj30u00gy751.jpg"></p>
<p>Spark 将支持 Stage 级别的资源控制和调度。目前 Spark 支持对 executor 资源的控制，但是往往一个 spark 作业分为几个 Stage 阶段，每个阶段用到的资源也是不一样的。支持 Stage 细粒度界别的资源控制有助于我们更好的控制集群资源。详情见 SPARK-27495</p>
<p>Spark Continuous Processing 模块</p>
<p>Spark Delta Lake </p>
<p>Spark SQL 对事物的处理，支持 CURD。</p>

        <h2 id="本文参考"   >
          <a href="#本文参考" class="heading-link"><i class="fas fa-link"></i></a>本文参考</h2>
      <p><span class="exturl"><a class="exturl__link"   href="https://juejin.im/post/6844903558622494733" >Spark 简介</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Sublime常用命令和插件</title>
    <url>/posts/12d67bff/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>鉴于每次电脑重装都要重新折腾一遍，遂创建一个<span class="exturl"><a class="exturl__link"   href="https://flyraty.github.io/categories/%E5%B7%A5%E5%85%B7/" >工具分类</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>用于记录常见软件的配置安装及使用，本篇主要介绍 Sublime Text.<br>Sublime Text 是一款流行的代码编辑器软件，也是HTML和散文先进的文本编辑器，可运行在Linux，Windows 和 Mac OS X。也是许多程序员喜欢使用的一款文本编辑器软件。同样推荐<span class="exturl"><a class="exturl__link"   href="https://www.sublimetextcn.com/" >官方文档</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<a id="more"></a>


        <h3 id="更换主题"   >
          <a href="#更换主题" class="heading-link"><i class="fas fa-link"></i></a>更换主题</h3>
      <ul>
<li>安装 a file icon ，添加对应文件图片，改善视觉效果。 <code>Command + Shift + p</code> 搜索 Package Control ，安装对应插件即可。</li>
<li>安装 Agila 主题，同样的打开 Package Control 搜索安装。</li>
<li>修改自定义配置文件 Sublime Text -&gt; Preferences -&gt; settings。左边是默认配置文件，右边是自定义的。关于 Agila 主题，可以直接参考<span class="exturl"><a class="exturl__link"   href="https://github.com/arvi/Agila-Theme" >文档</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>，我这里的配置文件如下<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">&quot;color_scheme&quot;: &quot;Packages&#x2F;Agila Theme&#x2F;Agila Origin Oceanic Next.tmTheme&quot;,</span><br><span class="line">&quot;ignored_packages&quot;:</span><br><span class="line">	[</span><br><span class="line">		&quot;Vintage&quot;,</span><br><span class="line">		&quot;zzz A File Icon zzz&quot;</span><br><span class="line">	],</span><br><span class="line">&quot;theme&quot;: &quot;Agila Origin.sublime-theme&quot;,</span><br></pre></td></tr></table></div></figure>

</li>
</ul>

        <h3 id="Sql​Beautifier"   >
          <a href="#Sql​Beautifier" class="heading-link"><i class="fas fa-link"></i></a>Sql​Beautifier</h3>
      <p>主要格式化 SQL，打开 Package Control 搜索安装即可。<code>Command + k  &amp;&amp; Command + f</code> 格式化 SQL 文件。</p>

        <h3 id="多行编辑模式"   >
          <a href="#多行编辑模式" class="heading-link"><i class="fas fa-link"></i></a>多行编辑模式</h3>
      <p>最常使用的是 Alt + 多选切换到多行编辑模式 → 加上 “” → <code>Command + J</code> 合并多行生成列表</p>

        <h3 id="设置文件修改自动保存"   >
          <a href="#设置文件修改自动保存" class="heading-link"><i class="fas fa-link"></i></a>设置文件修改自动保存</h3>
      <p>在自定义配置文件中加入 <code>&quot;save_on_focus_lost&quot;: true</code></p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Sublime Text</tag>
      </tags>
  </entry>
  <entry>
    <title>TimeRnageJoin（WIP）</title>
    <url>/posts/7073e6ec/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p><span class="exturl"><a class="exturl__link"   href="https://rahulpedduri.github.io/2017/10/23/time-range-join.html" >TimeRangeJoin</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>WIP</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformations - Transform Your Dataset</title>
    <url>/posts/e0f17242/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>由前面提到的 Spark 计算模型，我们可以知道 Spark 的数据计算在本质上是 RDD 的迭代转换。本文要讲的就是涉及到转换操作的转换算子 transformations 。通过这些转换算子，你就可以完成定义在数据集上的各种计算了，就和 SQL 一样。</p>
<a id="more"></a>


        <h3 id="Typed-Transformations"   >
          <a href="#Typed-Transformations" class="heading-link"><i class="fas fa-link"></i></a>Typed Transformations</h3>
      <p>这里的 Typed 是指什么呢，可以理解为强类型，即 transform  xxx to typed。这类转换算子返回 Dataset[T]，KeyValueGroupDataset。到底 Typed Transformations 和 UnTyped Transformations 有啥区别，我也有点迷糊，这里需要去读下源码。以看下下面的对 Typed Transformations 的解释。</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">Typed transformations are part of the Dataset API for transforming a Dataset with an Encoder (except the RowEncoder).</span><br></pre></td></tr></table></div></figure>
<p>这里介绍一些常见的 Typed Transformations。你会发现这个普通的 SQL 书写并没有什么区别。只是 Spark 为你提供了更简洁的 API，并且做了一些内部优化。</p>
<div class="table-container"><table>
<thead>
<tr>
<th>Method</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody><tr>
<td>alias</td>
<td>重命名列</td>
<td><code>$&quot;id&quot;.alias(&quot;ID&quot;)</code></td>
</tr>
<tr>
<td>as</td>
<td>强转类型</td>
<td><code>spark.range(5).as[String]</code></td>
</tr>
<tr>
<td>coalesce</td>
<td>hint，重分区，shuffle = false</td>
<td><code>spark.range(10000).coalesce(2).explain(true)</code></td>
</tr>
<tr>
<td>repartition</td>
<td>hint，重分区，shuffle = true</td>
<td><code>spark.range(10000).repartition(1).explain(true)</code></td>
</tr>
<tr>
<td>distinct</td>
<td>整体去重</td>
<td><code>val ds = Seq(1, 2, 3, 2, 5).toDS.distinct()</code></td>
</tr>
<tr>
<td>dropDuplicates</td>
<td>按照一列或者多列去重</td>
<td><code>val ds = Seq(1, 2, 3, 2, 5).toDS.dropDuplicates(&quot;id&quot;)</code></td>
</tr>
<tr>
<td>except</td>
<td>差集</td>
<td><br><code>val ds1 = Seq(1, 2, 3, 2, 5).toDS</code></br><br><code>val ds2  = Seq(1, 2, 6, 7, 8).toDS</code></br><br><code>ds1.except(ds2).show()</code></br></td>
</tr>
<tr>
<td>intersect</td>
<td>交集     <code>ds1.intersect(ds2).show()</code></td>
<td></td>
</tr>
<tr>
<td>union</td>
<td>并集</td>
<td><code>ds1.union(ds2).show()</code></td>
</tr>
<tr>
<td>unionByname</td>
<td>并集，比较严格，遇到列名重复，以及列缺少会报错</td>
<td></td>
</tr>
<tr>
<td>filter</td>
<td>过滤</td>
<td><code>ds1.filter($&quot;value&quot; === 2)</code></td>
</tr>
<tr>
<td>where</td>
<td>过滤，看源码其实就是 filter</td>
<td><code>ds1.where($&quot;value&quot; === 2)</code></td>
</tr>
<tr>
<td>map</td>
<td>转换每行的内容</td>
<td><br><code>case class Sentence(id: Long, text: String)</code></br><br><code>val sentences = Seq(Sentence(0, &quot;hello world&quot;), Sentence(1, &quot;witaj swiecie&quot;)).toDS</code></br><br><code>val sentences = Seq(Sentence(0, &quot;hello world&quot;), Sentence(1, &quot;witaj swiecie&quot;)).toDS</code></br><br><code>sentences.map(s =&gt; s.text.length &gt; 12).show()</code></br></td>
</tr>
<tr>
<td>mapPartitions</td>
<td>map对每个元素做操作，mapPartitions 针对每个分区，对分区里的元素做操作</td>
<td><code>sentences.mapPartitions(it =&gt; it.map(s =&gt; s.text.length &gt; 12)).show()</code></td>
</tr>
<tr>
<td>flatmap</td>
<td>map + 打平嵌套元素</td>
<td>sentences.flatMap(s =&gt; s.text.split(“\s+”)).show()</td>
</tr>
<tr>
<td>limit</td>
<td></td>
<td><code>sentences.limit(1)</code></td>
</tr>
<tr>
<td>groupByKey</td>
<td>比 groupBy 更加灵活，灵活在可以自由组合列？返回 KeyValueGroupDataset。但是后面可支持的聚合函数可能比较少</td>
<td><br><code>ds.groupBy(&quot;id&quot;)</code></br><br><code>ds.groupByKey(row =&gt; &#123;row.getString(0)&#125;)</code></br></td>
</tr>
<tr>
<td>select</td>
<td>SQL 中的 select</td>
<td></td>
</tr>
<tr>
<td>sort</td>
<td>排序</td>
<td></td>
</tr>
<tr>
<td>transform</td>
<td>高阶函数，可把多个操作链接起来。后面会拿出来单独拿出来讲</td>
<td><br>def transformInt(columns: Seq[String])(df: DataFrame) = {</br><br>  var dfi = df</br> <br>  for (column &lt;- columns) {</br><br>    dfi = dfi.withColumn(column, col(s”$column”).cast(“int”))</br><br>  }</br><br>  dfi</br><br>}</br><br>df.transform(transformInt(Seq(“id”, “revenue”)))</br></td>
</tr>
</tbody></table></div>

        <h3 id="UnTyped-Transformations"   >
          <a href="#UnTyped-Transformations" class="heading-link"><i class="fas fa-link"></i></a>UnTyped Transformations</h3>
      <p>弱类型的转换算子把 Dataset 转换为一些弱类型的数据结构，比如 DataFrame，RelationalGroupedDataset等，即 transform xxx to untyped。</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">Untyped transformations are part of the Dataset API for transforming a Dataset to a DataFrame, a Column, a RelationalGroupedDataset, a DataFrameNaFunctions or a DataFrameStatFunctions (and hence untyped).</span><br></pre></td></tr></table></div></figure>
<p>这里介绍常见的 UnTyped Transformations，emnn，其实就是常见的 SQL Operator</p>
<div class="table-container"><table>
<thead>
<tr>
<th>Method</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody><tr>
<td>agg</td>
<td>聚合函数</td>
<td><br><code>val ds = xxx</code></br><br><code>ds.groupBy(&quot;name&quot;).agg(sum(&quot;score&quot;), min(&quot;id&quot;))</code></br></td>
</tr>
<tr>
<td>grouoBy</td>
<td>分组</td>
<td><code>ds.groupBy(&quot;name&quot;).count()</code></td>
</tr>
<tr>
<td>cube</td>
<td>多维分析</td>
<td></td>
</tr>
<tr>
<td>drop</td>
<td>删除某列</td>
<td></td>
</tr>
<tr>
<td>join</td>
<td>连接查询，SQL 中的 join</td>
<td></td>
</tr>
<tr>
<td>na</td>
<td>DataFramnNaFunctions，常用于替换空值</td>
<td><code>spark.range(5).na</code></td>
</tr>
<tr>
<td>rollup</td>
<td>多维分析，分析的维度组合生成不一样</td>
<td></td>
</tr>
<tr>
<td>select</td>
<td>SQL 中的 select，和 Typed Transformations 的 select 接收的参数类型不一样，一个是  TypedColumn</td>
<td></td>
</tr>
<tr>
<td>selectExpr</td>
<td>接收 Expression 类型</td>
<td></td>
</tr>
<tr>
<td>withColumn</td>
<td>根据其他类添加一列或者修改本列，当你有很多 withColumn 时，你可以选择使用 transform 来将很多操作 chain 起来</td>
<td><br><code>spark.range(5).withColumn(&quot;idPlus&quot;, $&quot;id&quot; + 1)</code></br><br><code>spark.range(5).withColumn(&quot;id&quot;, $&quot;id&quot; + 1)</code></br></td>
</tr>
<tr>
<td>withColumnRenamed</td>
<td>重命名列，alias也是不错的选择</td>
<td><code>spark.range(5).withColumnRenamed(&quot;id&quot;, &quot;ID&quot;)</code></td>
</tr>
</tbody></table></div>
<p>na 可以用来做填充替换。比入我们读取的 csv 文件中，有的列既有有值的，又有空字符串，还有为空的，或者其他乱七八遭的数据，为了方便后续的统计计算，我们肯定要替换成统一值代表一个统一的含义。</p>
<p>tip：spark na.fill 当接收 Map 作为参数时，Map 的 value 类型只能为 Int, Long, Float, Double, String, Boolean.</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> cols = <span class="type">Seq</span>(<span class="string">&quot;distinct_id&quot;</span>, <span class="string">&quot;time&quot;</span>, <span class="string">&quot;$ip&quot;</span>, <span class="string">&quot;$browser&quot;</span>, <span class="string">&quot;$url&quot;</span>, <span class="string">&quot;$referrer&quot;</span>)</span><br><span class="line">spark.read.csv(input_path).na.replace(cols, <span class="type">Map</span>(<span class="string">&quot;-&quot;</span> -&gt; <span class="string">&quot;&quot;</span>))</span><br><span class="line"></span><br><span class="line">spark.read.csv(input_path).na.fill(<span class="number">-1</span>)</span><br></pre></td></tr></table></div></figure>



]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>UDAF - User Defined Aggregate  Functions</title>
    <url>/posts/fb27551c/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>UDF 是基于列的自定义处理函数。UDAF 是基于多行的自定义处理函数。UDAF 用于 Untyped Dataset，Aggregator 用于处理自定义 Scala 对象构成的数据集的聚合。本文主要以三个例子来实现简单的 UDAF 和 Aggregator。</p>
<a id="more"></a>

        <h3 id="UDAF"   >
          <a href="#UDAF" class="heading-link"><i class="fas fa-link"></i></a>UDAF</h3>
      <p>UDAF 的定义继承 UserDefinedAggregateFunction。必须实现以下几个函数</p>
<div class="table-container"><table>
<thead>
<tr>
<th>方法</th>
<th>释义</th>
</tr>
</thead>
<tbody><tr>
<td>inputSchema</td>
<td>输入的数据类型</td>
</tr>
<tr>
<td>bufferSchema</td>
<td>中间计算结果类型</td>
</tr>
<tr>
<td>dataType</td>
<td>聚合函数最终返回的类型</td>
</tr>
<tr>
<td>deterministic</td>
<td>相同输入是否返回相同输出，true</td>
</tr>
<tr>
<td>initialize</td>
<td>初始化中间结果 buffer</td>
</tr>
<tr>
<td>update</td>
<td>分区内计算结果，更新 buffer</td>
</tr>
<tr>
<td>merge</td>
<td>分区之间合并计算结果</td>
</tr>
<tr>
<td>evaluate</td>
<td>从 buffer 中获取最终结果</td>
</tr>
</tbody></table></div>
<p>只需要创建 UDAF 对应的实例，便可以在 sql 或者 agg 中使用。比如 val myCount = new MyCountUDAF。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>, <span class="type">Aggregator</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Encoders</span>, <span class="type">Encoder</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCountUDAF</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">StructType</span>().add(<span class="string">&quot;id&quot;</span>, <span class="type">LongType</span>, nullable = <span class="literal">true</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">StructType</span>().add(<span class="string">&quot;count&quot;</span>, <span class="type">LongType</span>, nullable = <span class="literal">true</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">LongType</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = buffer(<span class="number">0</span>) = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, row: <span class="type">Row</span>): <span class="type">Unit</span> = buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + row.getLong(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = buffer.getLong(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyAverageUDAF</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span></span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">StructType</span>().add(<span class="string">&quot;inputColumn&quot;</span>, <span class="type">LongType</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">StructType</span>()</span><br><span class="line">      .add(<span class="string">&quot;sum&quot;</span>, <span class="type">LongType</span>)</span><br><span class="line">      .add(<span class="string">&quot;count&quot;</span>, <span class="type">LongType</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer(<span class="number">0</span>) = <span class="number">0</span>L</span><br><span class="line">    buffer(<span class="number">1</span>) = <span class="number">0</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + input.getLong(<span class="number">0</span>)</span><br><span class="line">    buffer(<span class="number">1</span>) = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer1(<span class="number">0</span>) = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>)</span><br><span class="line">    buffer1(<span class="number">1</span>) = buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Double</span> = buffer.getLong(<span class="number">0</span>) / buffer.getLong(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> myCount = <span class="keyword">new</span> <span class="type">MyCountUDAF</span></span><br><span class="line"><span class="keyword">val</span> myAverage = <span class="keyword">new</span> <span class="type">MyAverageUDAF</span></span><br><span class="line">spark</span><br><span class="line">    .range(start = <span class="number">0</span>, end = <span class="number">4</span>, step = <span class="number">1</span>, numPartitions = <span class="number">2</span>)</span><br><span class="line">    .withColumn(<span class="string">&quot;group&quot;</span>, $<span class="string">&quot;id&quot;</span> % <span class="number">2</span>)</span><br><span class="line">    .groupBy(<span class="string">&quot;group&quot;</span>)</span><br><span class="line">    .agg(myCount.distinct($<span class="string">&quot;id&quot;</span>) as <span class="string">&quot;count&quot;</span>)</span><br><span class="line">    .show()</span><br><span class="line">spark</span><br><span class="line">    .range(start = <span class="number">0</span>, end = <span class="number">4</span>, step = <span class="number">1</span>, numPartitions = <span class="number">2</span>)</span><br><span class="line">    .agg(myAverage($<span class="string">&quot;id&quot;</span>))</span><br><span class="line">    .show()</span><br></pre></td></tr></table></div></figure>


        <h3 id="Aggregator"   >
          <a href="#Aggregator" class="heading-link"><i class="fas fa-link"></i></a>Aggregator</h3>
      <p>自定义的 Aggregator 继承自 org.apache.spark.sql.expressions.Aggregator。必须实现一下几个方法。</p>
<div class="table-container"><table>
<thead>
<tr>
<th>方法</th>
<th>释义</th>
</tr>
</thead>
<tbody><tr>
<td>zero</td>
<td>计算结果初始值</td>
</tr>
<tr>
<td>reduce</td>
<td>分区内计算结果</td>
</tr>
<tr>
<td>merge</td>
<td>分区间合并计算结果</td>
</tr>
<tr>
<td>finish</td>
<td>计算最终结果并制定结果类型</td>
</tr>
<tr>
<td>outputEncoder</td>
<td>结果类型指定编码器</td>
</tr>
<tr>
<td>bufferEncoder</td>
<td>中间结果类型指定编码器</td>
</tr>
</tbody></table></div>
<p>需要注意的是，在使用 Aggreagtor 的时候，需要 toColumn 来生成 TypedColumn 用于计算。就跟 groupBykey 中的一样，需要使用 typed.avg 来标记。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span>(<span class="params">name:<span class="type">String</span>, salary:<span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Average</span>(<span class="params">var count:<span class="type">Long</span>, var sum:<span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">MyAverageAggregator</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">Employee</span>, <span class="type">Average</span>, <span class="type">Double</span>]</span>&#123;</span><br><span class="line">  <span class="comment">// 初始化类型buffer</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Average</span> = <span class="type">Average</span>(<span class="number">0</span>L, <span class="number">0</span>L)</span><br><span class="line">  <span class="comment">// 计算聚合中间结果</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(b: <span class="type">Average</span>, a: <span class="type">Employee</span>): <span class="type">Average</span> = &#123;</span><br><span class="line">    b.count += <span class="number">1</span></span><br><span class="line">    b.sum += a.salary</span><br><span class="line">    b</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 合并中间结果</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">Average</span>, b2: <span class="type">Average</span>): <span class="type">Average</span> = &#123;</span><br><span class="line">    b1.sum += b2.sum</span><br><span class="line">    b1.count += b2.count</span><br><span class="line">    b1</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 计算最终结果并确定类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">Average</span>): <span class="type">Double</span> = reduction.sum.toDouble / reduction.count</span><br><span class="line">  <span class="comment">// 中间值类型指定编码器</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Average</span>] = <span class="type">Encoders</span>.product</span><br><span class="line">  <span class="comment">// 结果类型指定编码器</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = <span class="type">Encoders</span>.scalaDouble</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> employee = <span class="type">Seq</span>(</span><br><span class="line">      <span class="type">Employee</span>(<span class="string">&quot;Tom&quot;</span>, <span class="number">2674</span>),</span><br><span class="line">      <span class="type">Employee</span>(<span class="string">&quot;Ton&quot;</span>, <span class="number">3400</span>),</span><br><span class="line">      <span class="type">Employee</span>(<span class="string">&quot;Top&quot;</span>, <span class="number">4218</span>),</span><br><span class="line">      <span class="type">Employee</span>(<span class="string">&quot;Tos&quot;</span>, <span class="number">1652</span>)</span><br><span class="line">    )</span><br><span class="line">employee.toDS().select(myAveragor.toColumn.name(<span class="string">&quot;average_salary&quot;</span>)).show() <span class="comment">// 因为这里是Dataset[Employee]类型数据做聚合,所以toDS</span></span><br></pre></td></tr></table></div></figure>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>UDF - User  Defined  Functions</title>
    <url>/posts/d35f6739/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>Spark 本身提供的算子可以满足我们大多数的需求，并且我们可以组合各种算子，但是计算处理逻辑往往是复杂的。有些转换逻辑需要我们自定义函数才可以实现，这些自定义函数就是 UDF。UDF 是基于列的函数，拓展了 Spark SQL DSL，用于转换数据集。</p>
<a id="more"></a>


        <h3 id="UDF"   >
          <a href="#UDF" class="heading-link"><i class="fas fa-link"></i></a>UDF</h3>
      
        <h4 id="声明-UDF"   >
          <a href="#声明-UDF" class="heading-link"><i class="fas fa-link"></i></a>声明 UDF</h4>
      <p>定义 UDF 和写其他的函数并没有什么本质的区别。</p>
<p>UDF 的声明常用的方式是使用 <code>org.apache.spark.sql.functions.udf</code> 。看了下源码里面的注释，udf 接收的函数的参数最多只能有十个。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> upper = udf((s:<span class="type">String</span>) =&gt; s.toUpperCase)</span><br><span class="line">upper: org.apache.spark.sql.expressions.<span class="type">UserDefinedFunction</span> = <span class="type">UserDefinedFunction</span>(&lt;function1&gt;,<span class="type">StringType</span>,<span class="type">Some</span>(<span class="type">List</span>(<span class="type">StringType</span>)))</span><br></pre></td></tr></table></div></figure>
<p>使用 SparkContext 来注册，注意在使用你注册的 UDF 时，这里使用了 selectExpr，Expression 会自动由 Spark 去解析。而 select 的参数是 column 类型，这里使用 select 会报错。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; spark.udf.register(<span class="string">&quot;myUpper&quot;</span>, (input: <span class="type">String</span>) =&gt; input.toUpperCase)</span><br><span class="line">res0: org.apache.spark.sql.expressions.<span class="type">UserDefinedFunction</span> = <span class="type">UserDefinedFunction</span>(&lt;function1&gt;,<span class="type">StringType</span>,<span class="type">Some</span>(<span class="type">List</span>(<span class="type">StringType</span>)))</span><br><span class="line">scala&gt; <span class="type">Seq</span>(<span class="string">&quot;mike&quot;</span>).toDF(<span class="string">&quot;name&quot;</span>).selectExpr(<span class="string">&quot;myUpper(name)&quot;</span>).show()</span><br><span class="line">+-----------------+</span><br><span class="line">|<span class="type">UDF</span>:myUpper(name)|</span><br><span class="line">+-----------------+</span><br><span class="line">|             <span class="type">MIKE</span>|</span><br><span class="line">+-----------------+</span><br></pre></td></tr></table></div></figure>
<p>使用 UDF 也很简单，将他当成 org.apache.spark.functions._ 中的函数一样调用就可以。UDF 常出现的问题其实是是类型错误，要避免 Any 类型的出现。</p>

        <h4 id="UDF-是个黑盒？"   >
          <a href="#UDF-是个黑盒？" class="heading-link"><i class="fas fa-link"></i></a>UDF 是个黑盒？</h4>
      <p>Spark 不会尝试去优化 UDF。这里可以简单对比一下使用 UDF 与不使用 UDF 的执行计划。可以看到使用了 UDF filter 算子并没有下推。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="type">People</span>(<span class="string">&quot;Mike&quot;</span>, <span class="number">18</span>), <span class="type">People</span>(<span class="string">&quot;Mary&quot;</span>, <span class="number">19</span>)).toDS</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">People</span>] = [name: string, age: int]</span><br><span class="line"></span><br><span class="line">scala&gt; ds.show()</span><br><span class="line">+----+---+</span><br><span class="line">|name|age|</span><br><span class="line">+----+---+</span><br><span class="line">|<span class="type">Mike</span>| <span class="number">18</span>|</span><br><span class="line">|<span class="type">Mary</span>| <span class="number">19</span>|</span><br><span class="line">+----+---+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; ds.filter($<span class="string">&quot;name&quot;</span>.equalTo(<span class="string">&quot;Mike&quot;</span>)).explain(<span class="literal">true</span>)</span><br><span class="line">== <span class="type">Parsed</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line"><span class="symbol">&#x27;Filter</span> (<span class="symbol">&#x27;name</span> = <span class="type">Mike</span>)</span><br><span class="line">+- <span class="type">LocalRelation</span> [name#<span class="number">54</span>, age#<span class="number">55</span>]</span><br><span class="line"></span><br><span class="line">== <span class="type">Analyzed</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line">name: string, age: int</span><br><span class="line"><span class="type">Filter</span> (name#<span class="number">54</span> = <span class="type">Mike</span>)</span><br><span class="line">+- <span class="type">LocalRelation</span> [name#<span class="number">54</span>, age#<span class="number">55</span>]</span><br><span class="line"></span><br><span class="line">== <span class="type">Optimized</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line"><span class="type">LocalRelation</span> [name#<span class="number">54</span>, age#<span class="number">55</span>]</span><br><span class="line"></span><br><span class="line">== <span class="type">Physical</span> <span class="type">Plan</span> ==</span><br><span class="line"><span class="type">LocalTableScan</span> [name#<span class="number">54</span>, age#<span class="number">55</span>]</span><br><span class="line"></span><br><span class="line">scala&gt; ds.filter(_.name == <span class="string">&quot;Mike&quot;</span>).explain(<span class="literal">true</span>)</span><br><span class="line">== <span class="type">Parsed</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line"><span class="symbol">&#x27;TypedFilter</span> &lt;function1&gt;, <span class="class"><span class="keyword">class</span> <span class="title">$line30</span>.<span class="title">$read$$iw$$iw$People</span>, [<span class="type">StructField</span>(name,<span class="type">StringType</span>,true), <span class="type">StructField</span>(age,<span class="type">IntegerType</span>,false)], <span class="title">unresolveddeserializer</span>(<span class="params">newInstance(class $line30.$read$$iw$$iw$<span class="type">People</span></span>))</span></span><br><span class="line"><span class="class"><span class="title">+-</span> <span class="title">LocalRelation</span> [name#54, age#55]</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">==</span> <span class="title">Analyzed</span> <span class="title">Logical</span> <span class="title">Plan</span> <span class="title">==</span></span></span><br><span class="line"><span class="class"><span class="title">name</span></span>: string, age: int</span><br><span class="line"><span class="type">TypedFilter</span> &lt;function1&gt;, <span class="class"><span class="keyword">class</span> <span class="title">$line30</span>.<span class="title">$read$$iw$$iw$People</span>, [<span class="type">StructField</span>(name,<span class="type">StringType</span>,true), <span class="type">StructField</span>(age,<span class="type">IntegerType</span>,false)], <span class="title">newInstance</span>(<span class="params">class $line30.$read$$iw$$iw$<span class="type">People</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">+-</span> <span class="title">LocalRelation</span> [name#54, age#55]</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">==</span> <span class="title">Optimized</span> <span class="title">Logical</span> <span class="title">Plan</span> <span class="title">==</span></span></span><br><span class="line"><span class="class"><span class="title">TypedFilter</span> <span class="title">&lt;function1&gt;</span>, <span class="title">class</span> <span class="title">$line30</span>.<span class="title">$read$$iw$$iw$People</span>, [<span class="type">StructField</span>(name,<span class="type">StringType</span>,true), <span class="type">StructField</span>(age,<span class="type">IntegerType</span>,false)], <span class="title">newInstance</span>(<span class="params">class $line30.$read$$iw$$iw$<span class="type">People</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">+-</span> <span class="title">LocalRelation</span> [name#54, age#55]</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">==</span> <span class="title">Physical</span> <span class="title">Plan</span> <span class="title">==</span></span></span><br><span class="line"><span class="class"><span class="title">*</span>(<span class="params">1</span>) <span class="title">Filter</span> <span class="title">&lt;function1&gt;</span>.<span class="title">apply</span></span></span><br><span class="line"><span class="class"><span class="title">+-</span> <span class="title">LocalTableScan</span> [name#54, age#55]</span></span><br></pre></td></tr></table></div></figure>


]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>doris/starrocks 碎碎念</title>
    <url>/posts/6ccdea5/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>2020 年末入职作业帮，接触到了 doris，当时还是 doris on es，后来随着业务的发展，从 doe 切到 dorisdb，再到 starrocks，期间做过了很多基础测试（包括离线，实时摄入，doris 本身的数据集成工具，apache seatunnel 的 doris connector 等），也碰到过很多慢查询问题，内部的广告用户画像平台也是基于 starrocks 进行构建的。本文主要在回忆下碰到的问题吧，对一些至今仍未解决的问题也会留下自己的猜想，希望后面有机会可以在验证吧。</p>
<a id="more"></a>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Doris</tag>
        <tag>StarRocks</tag>
      </tags>
  </entry>
  <entry>
    <title>doris实时数据摄入测试</title>
    <url>/posts/a521d11e/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>观察 flink-doris-connector 和 RoutineLoad 的数据摄入情况，包括 kafka消费速率、doris 集群情况、doris 数据导入速率、各种调参造成的性能波动、不同 doris 表数据模型以及数据处理语义对导入性能的影响。</p>
<a id="more"></a>


        <h3 id="性能测试"   >
          <a href="#性能测试" class="heading-link"><i class="fas fa-link"></i></a>性能测试</h3>
      <p>以下测试均为从 earliest 消费同一测试 topic（测试流量维持在 4w qps 左右，晚高峰在 8w qps。），逻辑是简单的 source → map → sink，即读取 kafka 数据进行简单的字段提取后就发往 doris。暂时忽略掉不同时间点 source 数据流量的差异。</p>
<p>初始 Flink 资源：</p>
<div class="table-container"><table>
<thead>
<tr>
<th>slot</th>
<th>TM</th>
<th>JM Memory</th>
<th>TM Memory</th>
</tr>
</thead>
<tbody><tr>
<td>3</td>
<td>3</td>
<td>2048</td>
<td>2048</td>
</tr>
</tbody></table></div>
<p>需要观测的指标：</p>
<ul>
<li>Kafka Topic 的写入及消费情况。</li>
<li>Flink 任务的内存、cpu 使用，checkpoint的完成情况及整体的吞吐。</li>
<li>Doris 集群 CPU ，内存，磁盘，网络 IO的情况。</li>
<li>Doris 表的数据摄入速率。</li>
<li>Doris 表后端 compaction 进程对摄入的影响。</li>
</ul>
<p>针对每一种表模型的测试步骤：</p>
<ul>
<li>初始资源运行，观察所有相关的监控。</li>
<li>单独调整 slot，翻倍，观察任务情况。</li>
<li>单独调整内存，翻倍，观察任务情况。</li>
<li>根据 2，3步任务吞吐量是否线性增长，混合调整 slot 和内存，验证想法。</li>
<li>根据数据大小以及 kafka 写入 QPS预估调整 sink.buffer.* 参数是否会有作用，如果有作用，调整之，观察任务吞吐量。</li>
<li>根据最终确定较优参数，启动测试任务运行观察 1~2 天。</li>
<li>调整数据处理语义到 extractly-once，再次启动测试任务运行观察 1~2 天。</li>
</ul>

        <h4 id="duplicate-表"   >
          <a href="#duplicate-表" class="heading-link"><i class="fas fa-link"></i></a>duplicate 表</h4>
      <p>新建 duplicate 表 imp_flow_dup_small_test，三列数据，单条数据大小为 66 bytes。。测试导入。18:06 ~ 18:40 分别以 3，6，12，20 个slot 启动任务，整个任务的吞吐分别为 18w/s，40w/s，73w/s，86w/s，基本呈线性增长。18:40 ~ 18:50 调整回 3个 slot，分别以 4096，8192 内存运行，吞吐量无明显变化，20w/s 左右。</p>
<div class="table-container"><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/blog-doris_realtime_ingest/kakfa_metrics_dup_2.png"></td>
<td><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/blog-doris_realtime_ingest/stream_load_dup_2.png"></td>
</tr>
</tbody></table></div>
<div class="table-container"><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/blog-doris_realtime_ingest/doris_cpu_dup_2.png" align='left' width="50%" height="50%"></td>
<td></td>
</tr>
</tbody></table></div>
<p>新建 duplicate 表 imp_flow_dup_big_test，五十列左右数据，单条数据 855 bytes。测试导入。21:30 ~ 22:04 分别以 3，6，12，20 个slot 启动任务，吞吐量分别为 10w/s，23w/s，43w/s，43w/s，在并发达到 12 个及以上后，可以看到处理速度陡降的现象，这是任务挂掉了，触发了 close index channel failed 和 index channel has intoleralbe failure，此时需要关注下集群表数据版本合并情况。</p>
<p>22:04 ~ 22:15 以 3个slot，内存分别为 4096，8192 启动任务，处理速度 11w/s，有 5% ~ 10% 的增加。<br>22:15 ~ 22:20 更改 flush_interval=1000ms，可以看到没啥作用，整个任务的吞吐没啥变化，另外还有个评估标准，观察导入次数和导入函数，可以看到在导入次数翻倍的情况下，吞吐量仍然没有变化，说明调整没啥作用，可以通过观察导入监控，反复修改参数测试出来单次 stream load 最大的处理量。</p>
<div class="table-container"><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/blog-doris_realtime_ingest/kafka_metrics_dup_1.png"></td>
<td><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/blog-doris_realtime_ingest/stream_load_dup_1.png"></td>
</tr>
</tbody></table></div>
<div class="table-container"><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/blog-doris_realtime_ingest/doris_cpu_dup_1.png" align='left' width="50%" height="50%"></td>
<td></td>
</tr>
</tbody></table></div>

        <h4 id="agg-表"   >
          <a href="#agg-表" class="heading-link"><i class="fas fa-link"></i></a>agg 表</h4>
      <p>新建 agg 表 imp_flow_agg_small_test，key 列 5 个，一个bitmap 列，一个 HLL 列，一个 SUM 列，单条数据 158 bytes 左右。测试导入。2021-12-05 10:39 ~ 2021-12-05 11:20。任务每秒吞吐量在 16w 左右，随 slot 线性增长。</p>
<div class="table-container"><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/blog-doris_realtime_ingest/kafka_metrics_agg_1.png"></td>
<td><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/blog-doris_realtime_ingest/stream_load_agg_1.png"></td>
</tr>
</tbody></table></div>
<div class="table-container"><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/blog-doris_realtime_ingest/doris_cpu_agg_1.png" align='left' width="50%" height="50%"></td>
<td></td>
</tr>
</tbody></table></div>
<p>新建 agg 表 imp_flow_agg_big_test，key 列在 40 个左右，混合 bitmap，HLL，sum 指标列，单条数据在 1274 bytes 左右。22:05~22:15 3 个 slot 启动导入，吞吐量在 6w/s。22:15 ~ 22:25 增加到 6 个 slot，吞吐量在 13w/s。</p>
<p>22:25 ~ 22:30 增加到 12 个slot，吞吐量在 26w/s，持续几分钟后触发 <code>close index channel failed</code>，导入频率太快，compaction 没能及时合并导致版本数过多，增量合并数据组达到 300 版本/s。随着吞吐量的增加，cpu 平均使用率线性上升。</p>
<p>22:50 ~ 23:00，调回 3个 slot，增大 TM 内存到 4096M，吞吐量无明显上升，继续增大到 8192M，吞吐量还是无明显上升，单纯增大内存不起作用。11:06 ~ 11:20，调整为 6个 slot，4096M，整个任务的吞吐量在 13w/s，和单纯调整 slot 数差不多。</p>
<p>23:30后以 4 个 slot启动整个任务，平常没有问题，到达流量高峰期后，任务吞吐量达到 8w/s 后，stream load 开始频繁报错进而导致整个任务失败，调整到 3个 slot 后，任务平稳运行，但是写入 qps 比较高，导致任务已经追不上最新 offset。按照官网 FAQ 调整 BE rpc 超时参数，重启 doris 集群后，分别 4~6 个 slot 运行，仍然会有此问题。整个 droris 集群在此期间只有网络 io 繁忙，暂时怀疑和表的分区分片及 compaction 有关系，因为只是测试，简单的设置后 hash(adpos_id) 5个分桶，导致某台节点上的网络压力比较高。</p>
<p>这里没有调节 sink-buffer 系列参数，因为意义不太大，单条数据 1274 bytes，写入 6w qps 的情况下分钟内就可以达到默认的 buffer.max.size 阈值。</p>
<div class="table-container"><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/blog-doris_realtime_ingest/kafka_metrics_agg_2.png"></td>
<td><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/blog-doris_realtime_ingest/stream_load_agg_2.png"></td>
</tr>
</tbody></table></div>
<div class="table-container"><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/blog-doris_realtime_ingest/doris_cpu_agg_2.png" align='left' width="50%" height="50%"></td>
<td></td>
</tr>
</tbody></table></div>

        <h4 id="uniq-表"   >
          <a href="#uniq-表" class="heading-link"><i class="fas fa-link"></i></a>uniq 表</h4>
      <p>新建 uniq 表 imp_flow_uniq_big_test，以某张后续链路 binlog 表为例，从 earliest 开始消费，2w/s 的吞吐。</p>

        <h4 id="exactly-once-测试"   >
          <a href="#exactly-once-测试" class="heading-link"><i class="fas fa-link"></i></a>exactly-once 测试</h4>
      <p>观察精确一次语义情况下 flink 的资源消耗，此时 doris 摄入完全依赖于每次 checkpoint 的完成。</p>
<p>不适合流量数据，有两点原因：</p>
<ul>
<li>json 数据单次最大导入数据 100M。</li>
<li>不同时间节点 QPS 的变化造成 checkpoint 存在过大的情况且累计的数据超过 100M导致发送失败。<br>理论上来说，利用 checkpoint 保证 kafka 精确处理一次 + stream load 的事务保证即可。如果需要使用 checkpoint，可以通过以下方式评估，但是只要涉及到历史数据回溯就会挂。</li>
<li>在 chekpoint interval （最小 10ms）时间内积累的数据量有多大，即 checkpoint interval * 写入 qps * 单条 json 数据大小。</li>
</ul>

        <h4 id="flink2doris-csv，json-格式导入及-RoutineLoad-对比"   >
          <a href="#flink2doris-csv，json-格式导入及-RoutineLoad-对比" class="heading-link"><i class="fas fa-link"></i></a>flink2doris csv，json 格式导入及 RoutineLoad 对比</h4>
      <p>新建 imp_flow_dup_routine_test，和 imp_flow_dup_big_test 表结构一样。以  RoutineLoad 方式导入 imp_flow_dup_routine_test 表，分别以 csv，json 格式导入 imp_flow_dup_bug_test 表。观察整个任务的吞吐。</p>
<ul>
<li>csv 与 json 相比，单条数据大小减少了 2/3，吞吐提升了 1/4。</li>
<li>RoutineLoad 的处理速度平均在 18.5w/s，默认启动 3个 task。task 个数的计算逻辑如下：<code>Min(partition num, desired_concurrent_number, alive_backend_num, Config.max_routine_load_task_concurrrent_num)</code>。</li>
</ul>
<div class="table-container"><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/blog-doris_realtime_ingest/kafka_metrics_routinload.png"></td>
<td><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/blog-doris_realtime_ingest/doris_cpu_routinload.png"></td>
</tr>
</tbody></table></div>
<ul>
<li>flink2doris csv 格式只有 10w/s 。</li>
</ul>
<div class="table-container"><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/blog-doris_realtime_ingest/kafka_metrics_csv.png" align='left' width="50%" height="50%"></td>
<td></td>
</tr>
</tbody></table></div>
<ul>
<li>flink2doris json 8w/s，从 sink 端开始反压</li>
</ul>
<div class="table-container"><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/blog-doris_realtime_ingest/flink_webui_doris_json.png"></td>
<td><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/blog-doris_realtime_ingest/flink_webui_doris_2.png"></td>
</tr>
</tbody></table></div>

        <h4 id="集群参数调整测试"   >
          <a href="#集群参数调整测试" class="heading-link"><i class="fas fa-link"></i></a>集群参数调整测试</h4>
      <p>修改导入相关的 BE，FE 参数，测试对导入速率以及集群性能的影响。这块相对来说，需要对 doris 的摄入原理有一定的认识，即 doris 的整个数据摄入过程是什么样子的。</p>
<ul>
<li>BE 参数<ul>
<li>streaming_load_rpc_max_alive_time_sec。</li>
<li>tablet_writer_rpc_timeout_sec。</li>
<li>write_buffer_size。</li>
</ul>
</li>
</ul>

        <h3 id="遇到的一些测试问题"   >
          <a href="#遇到的一些测试问题" class="heading-link"><i class="fas fa-link"></i></a>遇到的一些测试问题</h3>
      <ol>
<li><p>flink-doris-connector 的实现原理？<br>读取数据缓存在内存中，在 at-least-once 处理语义下，到达某些阈值后（通过 sink.buffer.flush.* 系列参数控制），通过 stream load 发送到 doris。在 exactly-once 语义下，数据缓存在状态中，每次进行 checkponit 时 flush 数据。</p>
</li>
<li><p>flink-doris-connector 依赖的引入？<br>有两种方式，一种是从 dorisdb 私库拉取，可以参考 <span class="exturl"><a class="exturl__link"   href="http://doc.dorisdb.com/2166258" >flink-doris-connector 官方文档</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>。另一种是从中央仓库拉取，参考 <span class="exturl"><a class="exturl__link"   href="https://github.com/StarRocks/flink-connector-starrocks" >flink-connector-starrocks</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 官方 repo，同时需要额外引入 flink-table-api 的相关依赖。建议第二种方式。</p>
</li>
<li><p>数据处理语义问题，at-least-once 和 extratly-once 分别会引入什么样的问题？</p>
</li>
</ol>
<ul>
<li>at-least-once → 可能会引起 duplicate 表的数据重复，需要明确下任务失败重启时，已经在内存中但是还没有达到阈值的数据是会丢弃还是立即发送到下游？</li>
<li>exactly-once → 需要外部系统的 two phase commit 机制。由于 DorisDB 无此机制，需要依赖 flink 的 checkpoint-interval 在每次 checkpoint 时保存已经缓存的数据组及其label，在之后的数据传入时阻塞 flush 所有 state 中已经缓存的数据，以此达到精准一次。但如果DorisDB挂掉了，会导致用户的 flink sink stream 算子长时间阻塞，并引起 flink 的监控报警或强制 kill。</li>
</ul>
<ol start="4">
<li>字段的映射问题，json stream_load 的顺序问题。有时候某些字段没解析出来？<br>connector 底层实现是微批 stream load。而 stream load 提供了 columns 参数用于处理字段映射，可以借助这个参数来实现。只要在 columns 中引入数据转换，必须依赖顺序，即 jsonpaths 参数，否则字段映射就会错误。需要在 flink 逻辑中保证顺序。</li>
</ol>

        <h3 id="测试结论"   >
          <a href="#测试结论" class="heading-link"><i class="fas fa-link"></i></a>测试结论</h3>
      <p>任务的吞吐量大小的衡量可以简单通过  <code>slot个数 * 单个 slot 的 sink 数据大小  /  sink单条数据大小</code> 来表达，单条数据越大，slot 的吞吐就越小。在单个 slot 到达瓶颈后，要想增加整个任务的吞吐，只能调整 slot 数量。但是随着 slot 数量的增加，吞吐到达某个阈值后，就会存在稳定性的问题。</p>
<ol>
<li><p>不同表数据模型对 sink 的效率影响有限。</p>
</li>
<li><p>flink2doris 性能瓶颈在 sink 端。单条数据 100 bytes 左右，flink2doris 的整体吞吐在 20w/s，单个 slot 在 6~8w/s。单条数据大小增加到 1257 bytes，flink2doris 的整体吞吐在 6w/s，单个 slot 在 2w/s。主要原因是随着数据大小的增加，sink 端很快到达阈值，单次stream load 的处理量降低，sink 端的处理速度降低反压到 source 端，从而导致整体的吞吐量降低。</p>
</li>
<li><p>整体任务的吞吐随着 slot 数的增加线性增长，kafka 测拉回来的数据大小不变的话，与 TM 内存关系不大。</p>
</li>
<li><p>随着 Doris 数据摄入吞吐量的提高，cpu 使用会明显上升。以 3 个 slot 启动三个大测试任务，cpu 平均使用在 5%。增加到 6 个 slot，cpu平均使用 15%。</p>
</li>
<li><p>随着 flink 任务吞吐量以及数据处理大小的提高，flink2doris 开始出现稳定性问题，主要表现为 stream load 失败，看 BE log 的话，怀疑是导入速度比较块，需要合并的版本比较多， fragment_mgr一直占用 tablet 锁， 进而导致 tablet writer 写 RPC 超时。以 IMP 业务数据 doris 摄入为例，吞吐超过 6w/s 后，就会频发  <code>index channel has intoleralbe failure </code> 错，进而导致 flink 任务不断失败重启，延迟越来越高。</p>
</li>
<li><p>exactly-once 不适用于处理流量数据以及需要频繁回溯的数据。</p>
</li>
<li><p>stram load 在 columns mapping  中引入表达式时，强依赖与 json 字段的顺序，否则会出现映射错误。</p>
</li>
</ol>

        <h3 id="RoutineLoad-和-flink-doris-connector-对比"   >
          <a href="#RoutineLoad-和-flink-doris-connector-对比" class="heading-link"><i class="fas fa-link"></i></a>RoutineLoad 和 flink-doris-connector 对比</h3>
      <div class="table-container"><table>
<thead>
<tr>
<th></th>
<th>RoutineLoad</th>
<th>flink-doris-connector</th>
</tr>
</thead>
<tbody><tr>
<td>稳定性</td>
<td>任务稳定运行。摄入与存储耦合。消耗 doris 集群本身资源，任务多了 cpu 使用会明显上升，可能会影响查询。</td>
<td>任务稳定运行，摄入与存储分离。（对于需要频繁更新的大宽表，吞吐量提升到一定后，摄入稳定性会有问题，此时需要调整集群参数。）</td>
</tr>
<tr>
<td>可维护性</td>
<td>基于 doris 本身的 DML 进行管理，和客户端交互。问题查找涉及 BE，FE log 和 任务本身的错误链接提示，需要对 doris 摄入原理有一定的认识。</td>
<td>基于 flink 配置化管理。问题查找可以查看 flink 日志，但是对于 stream load 摄入错误，也要结合 FE，BE log。</td>
</tr>
<tr>
<td>可伸缩性</td>
<td>基于以下逻辑控制并发，涉及任务参数以及 BE 参数，需要提前做预估，否则可能需要修改配置和启停集群。Min(partition num, desired_concurrent_number, alive_backend_num, Config.max_routine_load_task_concurrrent_num)</td>
<td>基于 flink 调整并发，只会涉及到任务启停。</td>
</tr>
<tr>
<td>数据摄入效率</td>
<td>高</td>
<td>较高（但是某些情况下会比 routinload 吞吐低 40% 左右)</td>
</tr>
<tr>
<td>数据回溯支持</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>数据格式支持</td>
<td>json/csv，对于需要的数据嵌套在不同的 json key 下无法处理。</td>
<td>所有</td>
</tr>
<tr>
<td>ETL 操作的支持</td>
<td>columns 参数暴露，但是强依赖与上游数据源中的字段顺序保证，routineload 本身无法做到。</td>
<td>columns 参数暴露，flink 层逻辑处理。</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table></div>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Doris</tag>
        <tag>StarRocks</tag>
      </tags>
  </entry>
  <entry>
    <title>druid 离线摄入任务优化</title>
    <url>/posts/b0339e9e/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>最近 Yarn 队列资源收缩后，部分流量数据的 druid 离线摄入任务执行时间过长，经常由于申请不到 container 导致并发过低或者 reduce task 频繁失败重试，耗时 6~8h 才能完成。由于本身已经是 T+1 数据，离线摄入执行过长对下游使用方造成了很不好的使用体验。本文主要记录如何定位问题及如何解决。</p>
<a id="more"></a>

        <h3 id="前提问题"   >
          <a href="#前提问题" class="heading-link"><i class="fas fa-link"></i></a>前提问题</h3>
      <p>一般碰到性能优化问题，都要对任务的执行情况有一个比较深入的了解，什么阶段发生了什么，哪个阶段耗时最长，任务的执行环境等等信息。这里首先会抛出一些问题，看看我们该如何获取我们想要的信息，包括各阶段的日志查看，任务运行的环境参数。</p>
<ol>
<li><p>MR 作业 map 阶段 spill 的次数该怎么查看？<br>主要分为三段日志，开始读文件，此处可以看到集群 mapre 的环形缓冲区大小是 892M，该map 处理的是 119 M 的文件。根据使用的 InputFormat 不同，日志会有差异，这里不一定会打印出文件路径。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/mr_spill_1.png"><br>发生 spill<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/mr_spill_2.png"><br>spill 完成，这里只溢写了一次。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/mr_spill_3.png"></p>
</li>
<li><p>MR 作业 reduce 阶段 fetcher 失败重试的日志怎么查看？<br>reduce shuffle log 中查看，fetcher#15 失败了三次，原因是连接存放目标 map 文件的机器超时。也可以查看 job counters 中的 shuffle error group。</p>
</li>
</ol>
<p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/reduce_shuffle_fetcher_error.png"></p>
<ol start="3">
<li><p>GC 日志怎么查看，该怎样定义 GC 频繁？<br>JVM 参数开启。如果发生多次老年代 GC 或者 metaspace GC，则需要注意内存是否给小了。关于 GC 日志如何查看可以参考 <span class="exturl"><a class="exturl__link"   href="https://www.cnblogs.com/klvchen/articles/11841337.html" >此处</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">-XX:+PrintGCDetails -XX:+PrintGCDateStamp</span><br></pre></td></tr></table></div></figure>
<p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/reduce_gc.png"></p>
</li>
<li><p>Hadoop 默认参数在哪里看？<br>这点是极其需要注意的，不要按照官网默认参数来调整，要按照自己集群的默认参数来调整，一开始就改错了，环形缓冲区大小 892M，官网默认大小 100 m，一开始调整成了 500M，本来是想增大结果调小了。<br>查看以往摄入任务的 MR 中的 Job configuration，搜索自己要调整的参数查看即可。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/hadoop_default_params.png"></p>
</li>
<li><p>reduce 阶段 sortmerge 日志的含义？<br>merge 拉取到的上游文件交给 reduce 逻辑去处理。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/reduce_sort_merge.png"></p>
</li>
<li><p>MR 作业 counters 的查看，对于任务分析有什么意义？<br>相关 counters 的含义参考 <span class="exturl"><a class="exturl__link"   href="https://www.cnblogs.com/codeOfLife/p/5521356.html" >此处</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>，相当于 MR 执行统计，对理解 MR 执行会有很大帮助，一般出现 shuffle error 的话，会极大影响整个作业的执行效率。</p>
</li>
<li><p>为什么被 kill 重试的 reducer 往往是最后才执行，并且一直占用着资源，处于 NEW 状态？</p>
</li>
<li><p>为什么 reduce 逻辑处理相同数据量（大小）的数据的时间差异比较大，快的 8min，慢的到 50min？<br> 主要有两点原因</p>
<ul>
<li>存在 shuffle error，网络 io 错误造成的 fetcher 超时非常容易引起 map 和 reduce 的循环重试。</li>
<li>受执行机本身影响，比如下面这几个 reduce task，当时运行的机器上内存和 cpu 消耗都非常大。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/reduce_task_detail.png"><br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/reduce_execute_machine.png"></li>
</ul>
</li>
</ol>

        <h3 id="任务执行现象"   >
          <a href="#任务执行现象" class="heading-link"><i class="fas fa-link"></i></a>任务执行现象</h3>
      <p>明确了上述前提问题，我们就需要去日志中查看分析，总结出来任务的执行现象，即任务由于哪些问题才导致的慢，最后根据总的问题来去优化，而不是想当然。<br>以其中某天数据为例，观察任务执行情况</p>

        <h4 id="处理数据量"   >
          <a href="#处理数据量" class="heading-link"><i class="fas fa-link"></i></a>处理数据量</h4>
      <p>读取 cos 上的文件，总共 4000 个文件，总计 355.5 G，平均大小 91 M，block 大小 为 16M，1 副本。使用的是单独的文件存储。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">$ hadoop fs -<span class="built_in">stat</span> <span class="string">&quot;%o %r&quot;</span> cosn://xxx</span><br><span class="line">16777216 1</span><br><span class="line">hadoop fs -ls cosn://xxx | wc -l</span><br><span class="line">4001</span><br><span class="line">$ hadoop fs -du -h cosn://xxx</span><br><span class="line">355.5 G  355.5 G  cosn://xxx</span><br></pre></td></tr></table></div></figure>


        <h4 id="MR-参数"   >
          <a href="#MR-参数" class="heading-link"><i class="fas fa-link"></i></a>MR 参数</h4>
      <figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;mapreduce.job.queuename&quot;</span>: <span class="string">&quot;xxx&quot;</span>,   <span class="comment"># 执行队列</span></span><br><span class="line"><span class="string">&quot;yarn.nodemanager.vmem-check-enabled&quot;</span>: <span class="string">&quot;false&quot;</span>, <span class="comment"># 关闭虚拟内存检查</span></span><br><span class="line"><span class="string">&quot;mapreduce.job.user.classpath.first&quot;</span>: <span class="string">&quot;true&quot;</span>,</span><br><span class="line"><span class="string">&quot;mapreduce.map.memory.mb&quot;</span>: 2048,  <span class="comment"># map 内存</span></span><br><span class="line"><span class="string">&quot;mapreduce.reduce.memory.mb&quot;</span>: 21504, <span class="comment"># reduce 内存</span></span><br><span class="line"><span class="string">&quot;mapreduce.job.classloader&quot;</span>: <span class="string">&quot;true&quot;</span>,</span><br><span class="line"><span class="string">&quot;mapreduce.job.reduce.slowstart.completedmaps&quot;</span>: 1,  <span class="comment"># 所有 map 执行完后在启动 reduce</span></span><br><span class="line"><span class="string">&quot;mapreduce.reduce.java.opts&quot;</span>: <span class="string">&quot;-server -Xms1g -Xmx25g -XX:MaxDirectMemorySize=512g&quot;</span> <span class="comment"># reduce jvm 设置，最大堆内存 25G</span></span><br></pre></td></tr></table></div></figure>


        <h4 id="任务执行情况"   >
          <a href="#任务执行情况" class="heading-link"><i class="fas fa-link"></i></a>任务执行情况</h4>
      <ul>
<li>MR job 启动了  24000个 map，960 个 rercuer。</li>
<li>map 阶段任务并发 500 左右，基本上 1min 内完成，平均耗时 30s左右，估算时间在 24000/500 *0.5=24min，最终执行了 31min。</li>
<li>reduce 阶段任务一开始并发 55~70 左右（受执行机空闲 core 数影响），分为三个阶段 copy → sortmerge → recuce，copy 阶段读取 map 为下游生成的 24000 个 map 文件，fetcher 速率受网络 io 和并发参数影响，平均拉取完成时间在 6 min 左右，平均拉取文件大小在 10000 ~ 40000 bytes 之间，即 0.04 M，并发起来后，会偶发拉取超时。查看其中一个 task，merge 结果如下，reduce 总体耗时在 16 min 左右，受拉回的上游文件大小影响。</li>
<li>2min内完成 reduce占比为 1/6，200 个，上游 map 生成的文件比较小。执行完之后，发现执行时间为 2min，10min，20min，40min, 几个小时 的执行成功的reducer 都占一定比例。在 reduce 阶段被 kill 掉的 task 有 459 个，map task 由于下游 reducer fetche 超时等原因重试 3720 个。<br>任务最重执行结果<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/mr_execute_result.png"></li>
</ul>

        <h4 id="现象总结"   >
          <a href="#现象总结" class="heading-link"><i class="fas fa-link"></i></a>现象总结</h4>
      <ol>
<li>cos 文件 blocksize 16M，启动 map 数过多，map 平均耗时 30s，进而分配资源开销占比比较大。在队列资源比较紧张的时候，并发数上不去，进而导致相同时间段内处理的数据量差异比较大。</li>
<li>reduce 阶段 task 执行完成时间差异比较大，发现 2min，10min，20min，40min, 1h 内执行成功的reducer 都占一定比例，reducer 处理的数据不均匀。</li>
<li>reduce 由于 fetcher 拉取超时和 reduce 阶段内存不足等原因失败重试，fetcher 超时会让 reduce 认为上游 map 失败，进而导致上游 map 重试，重试的 map task 优先级比较高，没有资源的情况下会抢占 reduce 资源，进而导致部分 reduce 失败。fetcher 阶段 10个线程并发拉取 35M （24000 个临时文件）数据花费了 4min 左右，fetcher 耗时比较长。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/reduce_killed_for_map.png"></li>
<li>当天存在异常任务，部分执行机磁盘打满了，导致节点不可用，进而运行在上面的 reduce task 失败。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/reduce_kill_for_container.png"></li>
<li>reduce sortMerge 后的数据部分存储在内存和磁盘中，在走 reduce 逻辑的时候会有磁盘 io。</li>
<li>由于 fetcher 超时被 kill 掉的 reducer task 一直占用着 core，但是不执行，任务状态一直为 NEW。</li>
<li>观察同期执行的 druid 合并任务，只有 140 个 map，查看 jobCounters，shuffle write 的数据量一致，都是 200G 左右，但是 map 读取的字节数差异很大，一个是 237G，一个是 35G，读取压缩文件比读取单纯的文件文件效率要高不少，可以查看 map 整体耗时。</li>
</ol>
<p><strong>有时候我们明确了问题，但是不知道内部为啥这样执行，会涉及到哪些参数，数据结构，这时候就需要去了解所使用框架的实现，不管是查看源码还是网上搜索。</strong></p>

        <h3 id="MR-job-的执行过程"   >
          <a href="#MR-job-的执行过程" class="heading-link"><i class="fas fa-link"></i></a>MR job 的执行过程</h3>
      <p>可以参考<span class="exturl"><a class="exturl__link"   href="https://imgedu.lagou.com/1537079-20181115113345165-1376342754.jpg" >此图</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>，比较详细。</p>
<p>总结一下，大致就是 split→ map → 环形缓冲区 → spill 文件 → sortMerge → reduce fetch → sortMerge → spill → reduce，其中是否发生 spill 主要受 shuffle write，sortMerge 阶段 buffer 的大小。</p>
<p>对于 druid 离线摄入任务来说，在集群 cpu，内存等资源都充足的情况下，需要关注 map 个数以及  shuffle 阶段，这里的 reduce 的逻辑可以暂时忽略（受 druid 本身影响）。</p>

        <h3 id="Druid-离线摄入任务的执行过程"   >
          <a href="#Druid-离线摄入任务的执行过程" class="heading-link"><i class="fas fa-link"></i></a>Druid 离线摄入任务的执行过程</h3>
      <p>本质上是 json2MR 的过程。根据设置的参数判断是否启动不同阶段的 MR。主要有三类 MR job。</p>
<ol>
<li><p>determine_partitions_hashed job，设置了 targetPartitionSize 后，首先会启动该 MR job，用于确认分区，计算分区的逻辑在 DetermineHashedPartitionsJob 类，逻辑可以参考 <span class="exturl"><a class="exturl__link"   href="https://blog.51cto.com/u_10120275/3530686" >https://blog.51cto.com/u_10120275/3530686</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 。就是利用 HLL 估算给定的查询粒度范围内的数据基数，在除以  targetPartitionSize 计算该时间范围内逻辑分片数。比如1小时内数据量 1000w，targetPartitionSize=200w，最终的分片数就是 5 。通过查看摄入任务的日志，可以看到每个时间段内确认的分区结果。需要注意 targetPartitionSize 并不是绝对的，一般还会由 maxPartitionSize 来限制，默认是 50%的targetPartitionSize。最终每个 segment 的行数范围是 targetPartitionSize ~ maxPartitionSize。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/druid_segment_detail.png"></p>
<p> 引入 targetPartitionSize 需要考虑几个问题。</p>
<ul>
<li>需要估算 segemt 的大小和数量，避免又引起查询性能问题。</li>
<li>启动的 reducer 个数会随着数据量浮动，有可能更少，也有可能更多。少了会导致每个 reducer 处理的数据量比较大，加大内存消耗，会拉长整个 MR 的执行时间。</li>
<li>整个摄入任务又多了一个 MR，如果均匀分区的执行效率没有太大提升，有点得不偿失。</li>
</ul>
</li>
<li><p>build-dict job，如果存在 uv 类指标，则会启动该 MR，reducer 的个数在代码中被写死，等于精确 uv 类指标的个数。</p>
</li>
<li><p>index-generator，主要是 segment 的构建逻辑，可以参考 <span class="exturl"><a class="exturl__link"   href="https://cloud.tencent.com/developer/article/1618957%E3%80%82" >https://cloud.tencent.com/developer/article/1618957。</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
</li>
</ol>
<p>除了上述 MR 过程外，摄入任务首先会注册 lifecycle，逐个执行生命周期内的所有逻辑，包括 lookup 等，这部分执行时间一般为几min。比如加载 lookup 到执行机 cache，执行 69 个 lookup 的时间在 1min 内，后续 MR 会从 cache 中读取。</p>

        <h3 id="参数调整"   >
          <a href="#参数调整" class="heading-link"><i class="fas fa-link"></i></a>参数调整</h3>
      <p>在做参数调整前，需要了解默认参数是啥样子的，这里的默认参数不是参考官方文档，而是看 MR 的 Job Configuration，我们需要了解我们的 hadoop 集群的配置。<br>结合任务执行现象和 Druid 任务执行过程。修改如下参数，目的是单个 map 控制在 5min 内，单个 reducer 控制在 15min。</p>
<ul>
<li>调整上游依赖表存储为 parquet 格式，并设置合并小文件，最终生成 500 个文件，根据流量波动，平均大小控制在 100 ~ 300M 之间。</li>
<li>调整 map 数，按照 256M 分片，500 个 map。（split 不能跨文件，所以 splieSize 在小于 256M 的情况下也会只产生一个 map，而不是两个 128M 文件合并成一个 map）。</li>
<li>调整 shuffle read 并发到 20，超时时间提高到 5 min，拉取同样的大小的文件，20 个线程拉取 500 个文件显然比拉取 24000 个文件执行效率高，并且网络 IO 开销小。</li>
<li>reduce 阶段拉取到的 map 文件合并后占 jvm最大堆内存的比例由 0 调整到 0.1，大于才会溢写到磁盘。</li>
<li>numBackgroundPersistThreads 按照官网建议设置为 1</li>
<li>调整摄入任务 targetPartitionSize=1200000，维持每个 segment 大小在官方建议的 500M~1G 之间，目的是为了 reduce 处理的数据更加均匀，避免数据不均匀引起的长尾任务。</li>
</ul>
<p>需要调整的摄入任务参数如下</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">&quot;mapreduce.input.fileinputformat.split.minsize&quot;: 128000000</span><br><span class="line">&quot;mapreduce.reduce.shuffle.parallelcopies&quot;: 20</span><br><span class="line">&quot;mapreduce.reduce.input.buffer.percent&quot;: 0.1</span><br><span class="line">&quot;mapreduce.reduce.shuffle.read.timeout&quot;: 300000</span><br><span class="line">&quot;numBackgroundPersistThreads&quot;: 1</span><br><span class="line">&quot;targetPartitionSize&quot;: 1200000</span><br></pre></td></tr></table></div></figure>


        <h3 id="测试"   >
          <a href="#测试" class="heading-link"><i class="fas fa-link"></i></a>测试</h3>
      <p>测试受集群队列资源的影响比较大，所以可以使用预估时间来评估</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">MR 耗时 &#x3D; (map 数量 &#x2F; map 并发数 * map 平均执行时间 + reducer 数量 &#x2F; reducer 并发数 * reducer 平均执行时间 + 预估等待资源及开销时间）</span><br></pre></td></tr></table></div></figure>
<p>重新测试该天数据，启动了 500 个map，730 个 reducer。在白天队列资源紧张的情况下，三个 MR 总共执行了 74min，相比之前的 420 min 提升了5倍的效率，并且由于存储改成 parquet，存储资源也降低了 5倍。</p>
<p>因为 yarn 的容量调度基于内存来管理，内存决定着能申请到的 container 的数量，即并发度。在资源有限的情况下，使用更少内存做相同的事情会更高效。通过上面的公式也能看出来，在 reducer 数量不变或者减少的情况并且平均执行时间不变的情况下，增大 reduce 的并发数会提高的 MR 的效率。</p>
<p>由于设置了 targetPartitionSize，动态分区后，reducer 数量减少了一半，一开始直接调大 reducer memory -&gt; 30G，jvm 最大堆内存设置为 30G。但是最终测试结果如下，可以看到简单的增大内存并不能加快执行效率，反而会由于并发降低拉长整个 MR 的执行时间。到底多少内存正好合适，除了不断测试，观察 jvm GC 情况也可以。</p>
<ul>
<li>30G，25G → 50min。</li>
<li>21G，20G → 45min。</li>
<li>20G，18G → 56min，存在长尾任务 30min。</li>
</ul>
<p>其实根据上述公式也能估算出来任务执行效率，白天执行，map 并发一般在 500 左右（即 yarn 子队列的最小资源保证），reducer 并发在 60 左右。</p>
<ul>
<li>修改前 24000 / 500 * 0.5 + 960 / 60 * 16 </li>
<li>修改后 500 / 500 * 0.5 + 730 / 60 * 16</li>
</ul>

        <h3 id="结论"   >
          <a href="#结论" class="heading-link"><i class="fas fa-link"></i></a>结论</h3>
      <p>从修改参数后三周内摄入任务的执行情况来看。</p>
<ul>
<li>在 6~ 9 点间执行情况差异不大，依然维持在 1.5h 左右。</li>
<li>在资源紧张+流量翻倍的情况下，摄入任务执行由原先的 6h → 2h，有2倍的性能提升，并且是在减少一半资源的情况下。</li>
</ul>
<p>具体原因主要是因为减少了 map 数量引发的效应。</p>
<ul>
<li>map 阶段时间缩短一倍。</li>
<li>shuffle read 阶段同样 4min ，修改后可以拉取到 2.4G 的数据，修改前只拉取到几十 m 的数据。</li>
<li>reducer 数量降低一倍，且内存不变，平均执行时间维持在 15min，长尾任务在 18min 左右。</li>
</ul>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Druid</tag>
      </tags>
  </entry>
  <entry>
    <title>druid 问题记录</title>
    <url>/posts/ea82a2ca/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>日常工作中采用 Druid 做流量日志分析。因为是刚接触，所以在离线/实时数据摄入过程中经常会碰到一些问题，本文主要用来记录这些问题及一些思考。</p>
<a id="more"></a>


        <h3 id="问题记录"   >
          <a href="#问题记录" class="heading-link"><i class="fas fa-link"></i></a>问题记录</h3>
      
        <h4 id="实时摄入任务-offset-获取失败"   >
          <a href="#实时摄入任务-offset-获取失败" class="heading-link"><i class="fas fa-link"></i></a>实时摄入任务 offset 获取失败</h4>
      <p>完整错误如下。实时摄入脚本在一周内发生了三次这样的错误，属于偶发错误，前两次伴随着 coordinator 节点的重启，当时查看任务日志发现请求 coordinator 接口失败，以为是任务恰好在该节点上，所以发生错误。<br>最后一次发生没有节点重启，当时查看了 <span class="exturl"><a class="exturl__link"   href="https://github.com/apache/druid/issues/7926" >issue#7926</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>，说是 0.16 之前的版本在某些竞态条件下会拿到 start 类型的 offset，而 druid 默认采用 end 类型，在做 minus 减法时，发现 <code>SeekableStreamEndSequenceNumbers</code> 和 <code>SeekableStreamEndSequenceNumbers</code> 类型不匹配，由此产生异常。</p>
<p>至于是何种条件导致拿到了  start 类型的 offset，看了下源码也没看明白，留作疑问。这里个人也有个疑问，既然默认采用了 end 类型，并且 kafka 和 kinesis 使用的方法大体一致，为什么此处不合并成一个 <code>SeekableStreamEndSequenceNumbers</code> 类呢？</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;timestamp&quot;</span>: <span class="string">&quot;2021-08-05T03:44:17.495Z&quot;</span>,</span><br><span class="line"><span class="string">&quot;exceptionClass&quot;</span>: <span class="string">&quot;org.apache.druid.java.util.common.IAE&quot;</span>,</span><br><span class="line"><span class="string">&quot;message&quot;</span>: <span class="string">&quot;org.apache.druid.java.util.common.IAE: Expected instance of org.apache.druid.indexing.seekablestream.SeekableStreamEndSequenceNumbers, got org.apache.druid.indexing.seekablestream.SeekableStreamStartSequenceNumbers\n\tat org.apache.druid.indexing.seekablestream.SeekableStreamEndSequenceNumbers.minus(SeekableStreamEndSequenceNumbers.java:159)\n\tat org.apache.druid.indexing.seekablestream.SeekableStreamDataSourceMetadata.minus(SeekableStreamDataSourceMetadata.java:95)\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.resetInternal(SeekableStreamSupervisor.java:1210)\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.getOffsetFromStorageForPartition(SeekableStreamSupervisor.java:2517)\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.generateStartingSequencesForPartitionGroup(SeekableStreamSupervisor.java:2494)\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.createNewTasks(SeekableStreamSupervisor.java:2392)\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.runInternal(SeekableStreamSupervisor.java:1068)\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor$RunNotice.handle(SeekableStreamSupervisor.java:292)\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.lambda$tryInit$3(SeekableStreamSupervisor.java:751)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n&quot;</span>,</span><br><span class="line"><span class="string">&quot;streamException&quot;</span>: <span class="keyword">false</span></span><br><span class="line"></span><br></pre></td></tr></table></div></figure>
<p>后续又看了下此问题，因为摄入任务参数中设置了 resetOffsetAutomatically 为 true，屏蔽了底层问题。修改后，报错如下。</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;timestamp&quot;</span>: <span class="string">&quot;2021-10-04T06:40:40.607Z&quot;</span>,</span><br><span class="line"><span class="string">&quot;exceptionClass&quot;</span>: <span class="string">&quot;org.apache.druid.java.util.common.ISE&quot;</span>,</span><br><span class="line"><span class="string">&quot;message&quot;</span>: <span class="string">&quot;org.apache.druid.indexing.seekablestream.common.StreamException: org.apache.druid.java.util.common.ISE: Previous sequenceNumber [95252478] is no longer available for partition [1]. You can clear the previous sequenceNumber and start reading from a valid message by using the supervisor&#x27;s reset API.\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.getOffsetFromStorageForPartition(SeekableStreamSupervisor.java:2527)\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.generateStartingSequencesForPartitionGroup(SeekableStreamSupervisor.java:2494)\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.createNewTasks(SeekableStreamSupervisor.java:2392)\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.runInternal(SeekableStreamSupervisor.java:1068)\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor$RunNotice.handle(SeekableStreamSupervisor.java:292)\n\tat org.apache.druid.indexing.seekablestream.supervisor.SeekableStreamSupervisor.lambda$tryInit$3(SeekableStreamSupervisor.java:751)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.druid.java.util.common.ISE: Previous sequenceNumber [95252478] is no longer available for partition [1]. You can clear the previous sequenceNumber and start reading from a valid message by using the supervisor&#x27;s reset API.\n\t... 11 more\n&quot;</span>,</span><br><span class="line"><span class="string">&quot;streamException&quot;</span>: <span class="keyword">true</span></span><br></pre></td></tr></table></div></figure>
<p>查看 kafka offset 信息，报错中的 offset 是三天前的，而 topic 只保留 24h 数据，同时查看 overload 日志，发现 not updating metadata 的日志，看起来是 druid_datasource 的元数据没更新导致，查看源码，发现如果是更新失败的话，还会有 <code>Not updating metadata, compare-and-swap failure</code> 的错误，但是这里没有。好嘛，一个问题变成两了，因为日志留存问题，需要等下次报错再查了。</p>
<ul>
<li>为什么今天的摄入任务会试图读取几天前的 offset ?</li>
<li>为什么设置了自动 reset，在重置的时候会一直报错？</li>
</ul>

        <h4 id="实时摄入任务直接切换-source-topic-引发的-segment-publish-failed"   >
          <a href="#实时摄入任务直接切换-source-topic-引发的-segment-publish-failed" class="heading-link"><i class="fas fa-link"></i></a>实时摄入任务直接切换 source topic 引发的 segment publish failed</h4>
      <p>完整错误如下。显示 segemt 发布失败。</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;errorMsg&quot;</span>: <span class="string">&quot;java.util.concurrent.ExecutionException: org.apache.druid.java.util.common.ISE: Failed to publish segments because of [java.lang.RuntimeException: Aborting transaction!].\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)\n\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\n\tat org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner.runInternal(SeekableStreamIndexTaskRunner.java:767)\n\tat org.apache.druid.indexing.seekablestream.SeekableStreamIndexTaskRunner.run(SeekableStreamIndexTaskRunner.java:235)\n\tat org.apache.druid.indexing.seekablestream.SeekableStreamIndexTask.run(SeekableStreamIndexTask.java:168)\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:413)\n\tat org.apache.druid.indexing.overlord.SingleTaskBackgroundRunner$SingleTaskBackgroundRunnerCallable.call(SingleTaskBackgroundRunner.java:385)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.druid.java.util.common.ISE: Failed to publish segments because of [java.lang.RuntimeException: Aborting transaction!].\n\tat org.apache.druid.segment.realtime.appenderator.BaseAppenderatorDriver.lambda$publishInBackground$8(BaseAppenderatorDriver.java:602)\n\t... 4 more\n&quot;</span></span><br></pre></td></tr></table></div></figure>
<p>还原下问题场景，实时摄入脚本一开始采用测试 topic 源，和线上旧任务双跑两天后，发现数据一致，遂修改脚本切换到线上 topic。切换过程如下</p>
<ol>
<li>暂停测试 supervisor，显示 SUSPENDED。</li>
<li>等待 supervisor 还在运行的 task 完成，手动 kill。</li>
<li>terminate 掉测试 supervisor。</li>
<li>提交修改 topic 源后的 supervisor spec。<br>因为设置了任务 task_duration 为 7200s，即实时摄入的 segement 每两个小时发布一次。在未到达发布周期前，任务运行正常。到达发布周期后，任务开始报 segement 发布失败。任务自动拉起后，失败的 segment 被移除，其元数据记录在 druid_pendingSegement 表中，在下游 BI 上的表现就是上个数据周期内数据丢失。</li>
</ol>
<p>开始查找问题原因，task log 中只有上述错误信息。segment 的发布涉及到 middlemanager 和 overload 进程。查看 overload log，搜索 datasource 名或者 topic 名称，发现如下 error。新的元数据中 topic offset 和旧的元数据中的 topic offset 不匹配。</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="code"><pre><span class="line">Not updating metadata, existing state[KafkaDataSourceMetadata&#123;SeekableStreamStartSequenceNumbers=SeekableStreamEndSequenceNumbers&#123;stream=<span class="string">&#x27;$&#123;test_topic&#125;&#x27;</span>, partitionSequenceNumberMap=&#123;<span class="number">0</span>=<span class="number">38254782</span>, <span class="number">1</span>=<span class="number">38220215</span>, <span class="number">2</span>=<span class="number">38217021</span>, <span class="number">3</span>=<span class="number">38232724</span>, <span class="number">4</span>=<span class="number">38230157</span>, <span class="number">5</span>=<span class="number">38219118</span>&#125;&#125;&#125;] in metadata store doesn<span class="string">&#x27;t match to the new start state[KafkaDataSourceMetadata&#123;SeekableStreamStartSequenceNumbers=SeekableStreamStartSequenceNumbers&#123;stream=&#x27;</span>$&#123;product_topci&#125;<span class="string">&#x27;, partitionSequenceNumberMap=&#123;1=38220215,2=38217021&#125;, exclusivePartitions=[]&#125;&#125;].</span></span><br></pre></td></tr></table></div></figure>
<p>看到这里的想法是找到对应的元数据并修正。查看元数据库，scan 了 <code>druid_segments</code>，<code>druid_tasks</code> 等几张元数据表，并没有发现存储 offset 信息的地方，怀疑是存在 zk 上。查看 <code>druid_pendingSegments</code>，发现存在未被移交的 segment 信息都在这里，查询目标 datasource，存在 124 个处于 pending 的 segment。</p>
<p>看起来是终止 supervisor 后，还存在未被发布的 segment。切换 topic 后，元数据一直更新检查失败，进而导致后续的 segment 全部发布失败，并且发布失败的 segment 被‘丢弃’掉。</p>
<p>此处参考了 <code>https://www.codeleading.com/article/33392556609/</code> 的方法，清除对应 datasource 的 pendingSegement 中的元数据后，再重新提交 supervisor spec 即可。</p>
<p>这个错误也告诉我们在终止 supervisor 前要验证剩余未发布的 segment 是否被正确处理，在做其他操作。</p>

        <h4 id="正式集群和测试集群某些配置相同引发的稳定性问题"   >
          <a href="#正式集群和测试集群某些配置相同引发的稳定性问题" class="heading-link"><i class="fas fa-link"></i></a>正式集群和测试集群某些配置相同引发的稳定性问题</h4>
      <p>正式集群和测试集群的 <code>druid.indexer.task.hadoopWorkingPath</code> 配置相同，但是两个集群的账号权限不一样，导致同一个目录下存在两个账号写入的文件，线上摄入任务在处理不同账号文件时报了权限错误。</p>
<p>这里碰到的具体问题其实是算 uv 类指标时，生成的 build-dict job 在删除过期版本字典时没有权限，导致整个 MR 失败。因为开源版本不支持精确去重，生产上用的是快手打的 patch 包，这里的详细原因开源不一定碰的到，就不细说了。</p>
<p>这件事情得到两个教训：</p>
<ul>
<li>测试 datasource 不要和正式环境的 datasource 重名。</li>
<li>在配置测试集群时要明确配置含义，尤其是存储相关的配置。</li>
</ul>

        <h4 id="druid-hadoop-index-摄入报错，-java-lang-IllegalStateException-JavaScript-is-disabled？"   >
          <a href="#druid-hadoop-index-摄入报错，-java-lang-IllegalStateException-JavaScript-is-disabled？" class="heading-link"><i class="fas fa-link"></i></a>druid hadoop_index 摄入报错， java.lang.IllegalStateException: JavaScript is disabled？</h4>
      <p>0.19.0 之前版本的 bug，在使用 js 解析并且设置 targetPartitionSize 相关分区参数时，启动了 determineHashPartitions job 会触发此错误。可以参考 <span class="exturl"><a class="exturl__link"   href="https://github.com/apache/druid/pull/9553" >PR#9953</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>。</p>
<p>生产上使用的是 0.16.0 版本，查看代码如下，DetermineHashedPartitionsJob 没有调用该方法 <code>JobHelper.injectDruidProperties(job.getConfiguration(), config.getAllowedHadoopPrefix());</code> ，导致 mapreduce.map.java.opts，mapreduce.reduce.java.opts 没有加载上 druid.javascripts 参数，最终整个 job 就会产生 JavaScript is disabled 错误。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/determinedHashPartiion_run.png"><br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/injectDruidProperties.png"></p>
<p>indexGenerator 和 buildDict job 都调用了该方法，所以不会报错。只会在产生 determineHashPartitions job 时报错。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/index_generator_run.png"></p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Druid</tag>
      </tags>
  </entry>
  <entry>
    <title>druid集群运维</title>
    <url>/posts/10507b28/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>作业帮内部营销中台（以下简称 IMP）承载着整个公司各个 app 端的流量分发，对智能化运营，精细化的营销广告投放起着至关重要的作用。对这些流量数据的链路转化进行有效的分析更是重中之重。</p>
<p>IMP 的整个链路流量数据处理采用的 flink + druid，并强依赖于 druid 提供上层的 BI，监控平台，算法接口，策略平台数据服务等线上应用。作为内部最大的 druid 集群，如何保证其能稳定的支撑海量数据的摄入与查询就成为了一个大问题。本来主要记录在对 druid 集群运维过程中碰到的一些问题以及优化。</p>
<a id="more"></a>


        <h3 id="基础"   >
          <a href="#基础" class="heading-link"><i class="fas fa-link"></i></a>基础</h3>
      
        <h4 id="配置分发"   >
          <a href="#配置分发" class="heading-link"><i class="fas fa-link"></i></a>配置分发</h4>
      
        <h4 id="故障演练"   >
          <a href="#故障演练" class="heading-link"><i class="fas fa-link"></i></a>故障演练</h4>
      
        <h3 id="监控"   >
          <a href="#监控" class="heading-link"><i class="fas fa-link"></i></a>监控</h3>
      
        <h4 id="机器监控"   >
          <a href="#机器监控" class="heading-link"><i class="fas fa-link"></i></a>机器监控</h4>
      
        <h4 id="重要服务进程监控"   >
          <a href="#重要服务进程监控" class="heading-link"><i class="fas fa-link"></i></a>重要服务进程监控</h4>
      
        <h4 id="Druid-Metrics"   >
          <a href="#Druid-Metrics" class="heading-link"><i class="fas fa-link"></i></a>Druid Metrics</h4>
      
        <h3 id="数据集成"   >
          <a href="#数据集成" class="heading-link"><i class="fas fa-link"></i></a>数据集成</h3>
      
        <h4 id="离线摄入任务优化"   >
          <a href="#离线摄入任务优化" class="heading-link"><i class="fas fa-link"></i></a>离线摄入任务优化</h4>
      
        <h4 id="实时摄入任务优化与稳定性"   >
          <a href="#实时摄入任务优化与稳定性" class="heading-link"><i class="fas fa-link"></i></a>实时摄入任务优化与稳定性</h4>
      
        <h3 id="存储"   >
          <a href="#存储" class="heading-link"><i class="fas fa-link"></i></a>存储</h3>
      
        <h4 id="segment-优化"   >
          <a href="#segment-优化" class="heading-link"><i class="fas fa-link"></i></a>segment 优化</h4>
      
        <h3 id="查询"   >
          <a href="#查询" class="heading-link"><i class="fas fa-link"></i></a>查询</h3>
      
        <h4 id="物化视图"   >
          <a href="#物化视图" class="heading-link"><i class="fas fa-link"></i></a>物化视图</h4>
      ]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Druid</tag>
      </tags>
  </entry>
  <entry>
    <title>基于 gitbook 搭建笔记站点</title>
    <url>/posts/fd7aaf9e/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>目前使用 hexo+github pages 构建博客站，但是作为笔记管理系统有两个缺点：</p>
<ol>
<li>笔记是学习一个事物的过程，记录可能比较随意。博客是学习一个事物并实践之后得到的思考。放到同一个主站点下面，即使打了 tags，给人的感觉也比较混乱。</li>
<li>hexo 笔记分层管理不太方便，需要自己新建 tab，并逐级构建章节文件夹，并且新建的 tab 对目录集成不是很好。</li>
</ol>
<p>本文主要记录 gitbook 的搭建集成，参考了 <span class="exturl"><a class="exturl__link"   href="https://blog.csdn.net/qq_40889820/article/details/110013310" >打造完美写作系统：Gitbook+Github Pages+Github Actions</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<a id="more"></a>


        <h3 id="gitbook"   >
          <a href="#gitbook" class="heading-link"><i class="fas fa-link"></i></a>gitbook</h3>
      <ol>
<li><p>安装 node，这里要安装 node 10.x，和 gitbook 版本兼容。否则会报错。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">brew search node </span><br><span class="line">brew install node@10 </span><br></pre></td></tr></table></div></figure>
</li>
<li><p>安装 gitbook，新建笔记目录，在笔记目录下执行初始化。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">npm install -g gitbook-cli</span><br><span class="line">mkdir ~/flink_learning_notes &amp;&amp; <span class="built_in">cd</span> ~/flink_learning_notes</span><br><span class="line">gitbook init</span><br></pre></td></tr></table></div></figure></li>
<li><p>配置 gitbook。第二步初始化完成后，会在对应目录下生成 <code>README.md</code> 和 <code>SUMMARY.md</code>。<code>README.md</code> 是网站首页，<code>SUMMARY.md</code> 是笔记目录，格式如下：</p>
</li>
</ol>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">* [开篇](README.md)</span><br><span class="line">* [Overview](overview&#x2F;README.md)</span><br><span class="line">    * [Stateful Stream Processing](overview&#x2F;stateful_stream_processing.md)</span><br><span class="line">    * [Timely Stream Processing](overview&#x2F;timely_stream_processing.md)</span><br><span class="line">    * [Flink Architecture](overview&#x2F;flink_architecture.md)</span><br><span class="line">* [DataStream API](datastream&#x2F;README.md)</span><br><span class="line">* [DataSet API](dataset&#x2F;README.md)</span><br><span class="line">* [Table API](table&#x2F;README.md)</span><br></pre></td></tr></table></div></figure>
<ol start="4">
<li>gitbook-summary 插件支持自动生成目录，安装完成后，执行 <code>book sum</code> 即可。<strong>注意中方式生成的目录顺序是按照字典序来的</strong>，所以可能会出现问题。</li>
</ol>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">npm install -g gitbook-summary</span><br></pre></td></tr></table></div></figure>
<ol start="5">
<li><p>安装插件。笔记目录下，新建 <code>book.json</code>，这是整个笔记站点的拓展配置文件。示例如下。需要注意的是 ignores 配置，代表的是自动生成目录插件需要忽略的文件夹。执行 <code>gitbook install</code> 即可按照配置安装插件。</p>
<figure class="highlight json"><div class="table-container"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">	<span class="attr">&quot;title&quot;</span>: <span class="string">&quot;Summary&quot;</span>,</span><br><span class="line">	<span class="attr">&quot;plugins&quot;</span> : [</span><br><span class="line">		<span class="string">&quot;expandable-chapters&quot;</span>, </span><br><span class="line">		<span class="string">&quot;github-buttons&quot;</span>,</span><br><span class="line">		<span class="string">&quot;editlink&quot;</span>,</span><br><span class="line">		<span class="string">&quot;copy-code-button&quot;</span>,</span><br><span class="line">		<span class="string">&quot;page-footer-ex&quot;</span>,</span><br><span class="line">		<span class="string">&quot;anchor-navigation-ex&quot;</span>,</span><br><span class="line">		<span class="string">&quot;expandable-chapters-small&quot;</span>,</span><br><span class="line">		<span class="string">&quot;prism&quot;</span>, </span><br><span class="line">		<span class="string">&quot;-highlight&quot;</span>,</span><br><span class="line">		<span class="string">&quot;lunr&quot;</span>, </span><br><span class="line">		<span class="string">&quot;-search&quot;</span>, </span><br><span class="line">		<span class="string">&quot;search-pro&quot;</span>,</span><br><span class="line">		<span class="string">&quot;splitter&quot;</span></span><br><span class="line">	],</span><br><span class="line">	<span class="attr">&quot;pluginsConfig&quot;</span>: &#123;</span><br><span class="line">		<span class="attr">&quot;editlink&quot;</span>: &#123;</span><br><span class="line">			<span class="attr">&quot;base&quot;</span>: <span class="string">&quot;https://github.com/Flyraty/flink_learning_notes/tree/gitbook&quot;</span>,</span><br><span class="line">			<span class="attr">&quot;label&quot;</span>: <span class="string">&quot;Edit This Page&quot;</span></span><br><span class="line">		&#125;,	</span><br><span class="line">		<span class="attr">&quot;github-buttons&quot;</span>: &#123;</span><br><span class="line">			<span class="attr">&quot;buttons&quot;</span>: [&#123;</span><br><span class="line">				<span class="attr">&quot;user&quot;</span>: <span class="string">&quot;Flyraty&quot;</span>,</span><br><span class="line">				<span class="attr">&quot;repo&quot;</span>: <span class="string">&quot;flink_learning_notes&quot;</span>,</span><br><span class="line">				<span class="attr">&quot;type&quot;</span>: <span class="string">&quot;star&quot;</span>,</span><br><span class="line">				<span class="attr">&quot;size&quot;</span>: <span class="string">&quot;small&quot;</span></span><br><span class="line">			&#125;]</span><br><span class="line">		&#125;,	</span><br><span class="line">		<span class="attr">&quot;page-footer-ex&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;copyright&quot;</span>: <span class="string">&quot;By [Flyraty](https://github.com/Flyraty)，使用[知识共享 署名-相同方式共享 4.0协议](https://creativecommons.org/licenses/by-sa/4.0/)发布&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;markdown&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">            <span class="attr">&quot;update_label&quot;</span>: <span class="string">&quot;&lt;i&gt;updated&lt;/i&gt;&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;update_format&quot;</span>: <span class="string">&quot;YYYY-MM-DD HH:mm:ss&quot;</span></span><br><span class="line">		&#125;,	</span><br><span class="line">		<span class="attr">&quot;prism&quot;</span>: &#123;</span><br><span class="line">			<span class="attr">&quot;css&quot;</span>: [<span class="string">&quot;prismjs/themes/prism-solarizedlight.css&quot;</span>],</span><br><span class="line">			<span class="attr">&quot;lang&quot;</span>: &#123;<span class="attr">&quot;flow&quot;</span>: <span class="string">&quot;typescript&quot;</span>&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">	&#125;,</span><br><span class="line">	<span class="attr">&quot;ignores&quot;</span> : [<span class="string">&quot;_book&quot;</span>, <span class="string">&quot;node_modules&quot;</span>]</span><br><span class="line">&#125;	</span><br></pre></td></tr></table></div></figure>
</li>
<li><p><code>gitbook serve</code> 本地启动服务，和 hexo 一样会生成一个 <code>localhost:4000</code> 的静态站点。生成的 <code>_book</code> 文件夹即为站点的静态资源。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/gitbook_local.png"><br><code>gitbook build</code> 只会生成站点资源文件，但是不会部署，类似于 <code>hexo -g</code>。</p>
</li>
<li><p>安装问题，gitbook 版本只支持 note 10.x，使用最新的 node 安装就会报以下错误，重新安装 node 即可。</p>
</li>
</ol>
<figure class="highlight js"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="built_in">TypeError</span>: cb.apply is not a <span class="function"><span class="keyword">function</span> <span class="title">at</span> /<span class="title">usr</span>/<span class="title">local</span>/<span class="title">lib</span>/<span class="title">node_modules</span>/<span class="title">gitbook</span>-<span class="title">cli</span>/<span class="title">node_modules</span>/<span class="title">npm</span>/<span class="title">node_modules</span>/<span class="title">graceful</span>-<span class="title">fs</span>/<span class="title">polyfills</span>.<span class="title">js</span>:287:18</span></span><br></pre></td></tr></table></div></figure>


        <h3 id="集成-github-pages"   >
          <a href="#集成-github-pages" class="heading-link"><i class="fas fa-link"></i></a>集成 github pages</h3>
      <p>github pages 分主站点和子站点的概念，每个 github 用户有一个主站点和若干个子站点。主站点就是命名为 xx.github.io 的仓库，开启 github pages 服务后，浏览器输入 <code>xx.github.io</code> 即可访问。子站点为同一用户下开通 github pages 服务的其他仓库，比如存在另外一个仓库 <code>flink_learning_notes</code>，浏览器输入 <code>xx.github.io\flink_learning_notes</code> 即可访问。</p>
<p>站点正常访问的前提是仓库根目录存在 _index.html 文件。执行 <code>gitbook serve</code> 生成的就是这个文件及其对应的静态资源，所以 gitbook 和<br>github pages 集成原理和 hexo 是一样的。</p>
<ol>
<li>本地编辑文件，生成资源文件。</li>
<li>github 新建仓库</li>
<li>本地目录关联远程仓库，push 到 github 仓库。</li>
<li>开启 github pages 服务。<br>而这些步骤又可以集成 github actions，走 CI。</li>
</ol>

        <h3 id="集成-github-actions"   >
          <a href="#集成-github-actions" class="heading-link"><i class="fas fa-link"></i></a>集成 github actions</h3>
      <p>其实就是用 CI 把上面的 gitbook 安装部署步骤走一遍。</p>
<ol>
<li>安装 node 和 npm。</li>
<li>安装 gitbook 和 gitbook-summary。</li>
<li><code>book sum</code> 生成目录文件，<code>github build</code> 生成站点资源文件</li>
<li><code>cd _book &amp;&amp; git push</code>。<br>CI 配置文件如下，这里和 hexo 一样，采用了双分支，站点 <code>_book</code> 部署到 main 分支，而 markdown 源文件在 gitbook 分支。不明白的可以参考下 <a href="https://timemachine.icu/posts/1eb3f811/">github actions 实现 hexo 自动化部署</a>。<br>需要额外注意的是 $，就是在个人 repo 设置中生成一个 access token 供 CI 权限访问。</li>
</ol>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"> </span><br><span class="line"><span class="attr">name:</span> <span class="string">CI</span></span><br><span class="line"><span class="attr">on:</span>                                 </span><br><span class="line">  <span class="attr">push:</span></span><br><span class="line">    <span class="attr">branches:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">gitbook</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="attr">main-to-gh-pages:</span></span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span></span><br><span class="line">        </span><br><span class="line">  <span class="attr">steps:</span>                          </span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Checkout</span> <span class="string">gitbook</span></span><br><span class="line">      <span class="attr">uses:</span> <span class="string">actions/checkout@v2</span></span><br><span class="line">      <span class="attr">with:</span></span><br><span class="line">        <span class="attr">ref:</span> <span class="string">gitbook</span></span><br><span class="line"></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Install</span> <span class="string">nodejs</span></span><br><span class="line">      <span class="attr">uses:</span> <span class="string">actions/setup-node@v1</span></span><br><span class="line"></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Configue</span> <span class="string">gitbook</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line">        <span class="string">npm</span> <span class="string">install</span> <span class="string">-g</span> <span class="string">gitbook-cli</span>          </span><br><span class="line">        <span class="string">gitbook</span> <span class="string">install</span></span><br><span class="line">        <span class="string">npm</span> <span class="string">install</span> <span class="string">-g</span> <span class="string">gitbook-summary</span></span><br><span class="line">                </span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Generate</span> <span class="string">_book</span> <span class="string">folder</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line">        <span class="string">gitbook</span> <span class="string">build</span></span><br><span class="line">        <span class="string">cp</span> <span class="string">SUMMARY.md</span> <span class="string">_book</span></span><br><span class="line">                </span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">push</span> <span class="string">_book</span> <span class="string">to</span> <span class="string">branch</span> <span class="string">main</span></span><br><span class="line">      <span class="attr">env:</span></span><br><span class="line">        <span class="attr">TOKEN:</span> <span class="string">$&#123;&#123;</span> <span class="string">secrets.TOKEN</span> <span class="string">&#125;&#125;</span></span><br><span class="line">        <span class="attr">REF:</span> <span class="string">github.com/$&#123;&#123;github.repository&#125;&#125;</span></span><br><span class="line">        <span class="attr">MYEMAIL:</span> <span class="number">1397554745</span><span class="string">@qq.com</span>                  </span><br><span class="line">        <span class="attr">MYNAME:</span> <span class="string">$&#123;&#123;github.repository_owner&#125;&#125;</span>          </span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line">        <span class="string">cd</span> <span class="string">_book</span></span><br><span class="line">        <span class="string">git</span> <span class="string">config</span> <span class="string">--global</span> <span class="string">user.email</span> <span class="string">&quot;$&#123;MYEMAIL&#125;&quot;</span></span><br><span class="line">        <span class="string">git</span> <span class="string">config</span> <span class="string">--global</span> <span class="string">user.name</span> <span class="string">&quot;$&#123;MYNAME&#125;&quot;</span></span><br><span class="line">        <span class="string">git</span> <span class="string">init</span></span><br><span class="line">        <span class="string">git</span> <span class="string">remote</span> <span class="string">add</span> <span class="string">origin</span> <span class="string">https://$&#123;REF&#125;</span></span><br><span class="line">        <span class="string">git</span> <span class="string">add</span> <span class="string">.</span> </span><br><span class="line">        <span class="string">git</span> <span class="string">commit</span> <span class="string">-m</span> <span class="string">&quot;Updated By Github Actions With Build $<span class="template-variable">&#123;&#123;github.run_number&#125;&#125;</span> of $<span class="template-variable">&#123;&#123;github.workflow&#125;&#125;</span> For Github Pages&quot;</span></span><br><span class="line">        <span class="string">git</span> <span class="string">branch</span> <span class="string">-M</span> <span class="string">gitbook</span></span><br><span class="line">        <span class="string">git</span> <span class="string">push</span> <span class="string">--force</span> <span class="string">--quiet</span> <span class="string">&quot;https://$&#123;TOKEN&#125;@$&#123;REF&#125;&quot;</span> <span class="string">gitbook:main</span></span><br></pre></td></tr></table></div></figure>



        <h3 id="与-github-pages-主站点集成"   >
          <a href="#与-github-pages-主站点集成" class="heading-link"><i class="fas fa-link"></i></a>与 github pages 主站点集成</h3>
      <p>主站点是博客站，新建 tab 页，配置外链，直接跳转到笔记站点即可。修改 hexo 主题配置文件，添加 notes 页面配置。示例如下</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: &#x2F; || fas fa-home</span><br><span class="line">  archives: &#x2F;archives&#x2F; || fas fa-folder-open</span><br><span class="line">  categories: &#x2F;categories&#x2F; || fas fa-layer-group</span><br><span class="line">  tags: &#x2F;tags&#x2F; || fas fa-tags</span><br><span class="line">  about: &#x2F;about&#x2F; || fas fa-user</span><br><span class="line">  book: &#x2F;book&#x2F; || fas fa-book</span><br><span class="line">  gallery: &#x2F;gallery&#x2F; || fas fa-image</span><br><span class="line">  notes: https:&#x2F;&#x2F;timemachine.icu&#x2F;flink_learning_notes || fas fa-sticky-note</span><br></pre></td></tr></table></div></figure>






]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>gitbook</tag>
      </tags>
  </entry>
  <entry>
    <title>git常用命令</title>
    <url>/posts/423abe9e/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>这里记录一些常见的 git 操作。</p>
<a id="more"></a>


        <h3 id="撤销上一次的-commit"   >
          <a href="#撤销上一次的-commit" class="heading-link"><i class="fas fa-link"></i></a>撤销上一次的 commit</h3>
      <figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">git reset --soft HEAD^</span><br></pre></td></tr></table></div></figure>

        <h3 id="撤销上一次的-add"   >
          <a href="#撤销上一次的-add" class="heading-link"><i class="fas fa-link"></i></a>撤销上一次的 add</h3>
      <figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">git reset --mixed</span><br></pre></td></tr></table></div></figure>

        <h3 id="关联远程仓库"   >
          <a href="#关联远程仓库" class="heading-link"><i class="fas fa-link"></i></a>关联远程仓库</h3>
      <figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">git init</span><br><span class="line">git remote add <span class="variable">$&#123;http url | ssh url&#125;</span> <span class="comment"># 建议使用 ssh</span></span><br></pre></td></tr></table></div></figure>

        <h3 id="删除关联的远程仓库"   >
          <a href="#删除关联的远程仓库" class="heading-link"><i class="fas fa-link"></i></a>删除关联的远程仓库</h3>
      <figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">git remote remove origin</span><br></pre></td></tr></table></div></figure>

        <h3 id="开启二次验证后，push-提示密码不对"   >
          <a href="#开启二次验证后，push-提示密码不对" class="heading-link"><i class="fas fa-link"></i></a>开启二次验证后，push 提示密码不对</h3>
      <ul>
<li>ssh<br><a href="">配置SSH</a><br>将生成的 id_pub 文件中的内容复制到 github 上的 SSH 配置里</li>
<li>personal token<br>使用 personal token 代替密码，比较麻烦，建议上种</li>
</ul>

        <h3 id="暂存工作区进度"   >
          <a href="#暂存工作区进度" class="heading-link"><i class="fas fa-link"></i></a>暂存工作区进度</h3>
      <figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">git stash</span><br><span class="line">git stash pop 恢复工作区，可能会产生冲突</span><br></pre></td></tr></table></div></figure>

        <h3 id="获取远程仓库地址"   >
          <a href="#获取远程仓库地址" class="heading-link"><i class="fas fa-link"></i></a>获取远程仓库地址</h3>
      <figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">git remote get-url origin</span><br></pre></td></tr></table></div></figure>


        <h3 id="rebase-合并提交"   >
          <a href="#rebase-合并提交" class="heading-link"><i class="fas fa-link"></i></a>rebase 合并提交</h3>
      <p>rebase 可以合并多次提交，对重复功能的提交是有好处的，可以维持简洁的 commit log。不过，合并提交意味着远端丢失更改信息，在生产中是好是坏孰未可知。</p>
<ul>
<li>指定合并提交的位置<br>即 start，end。需要注意的是从你想要合并的提交的前一个位置开始计数，像下面这个命令是合并最新提交前的 9 个提交 <figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">git rebase -i HEAD~9 </span><br></pre></td></tr></table></div></figure></li>
<li>squash 合并<br>执行上步完成后就会有相应的提示，将想要合并的 commit 前面改成 squash，下面的注释都有解释每个操作是干啥的。<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">pick bce5037 chore: xxxx</span><br><span class="line">  </span><br><span class="line"><span class="comment"># Rebase c4796b6..bce5037 onto c4796b6 (1 command)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Commands:</span></span><br><span class="line"><span class="comment"># p, pick &lt;commit&gt; = use commit</span></span><br><span class="line"><span class="comment"># r, reword &lt;commit&gt; = use commit, but edit the commit message</span></span><br><span class="line"><span class="comment"># e, edit &lt;commit&gt; = use commit, but stop for amending</span></span><br><span class="line"><span class="comment"># s, squash &lt;commit&gt; = use commit, but meld into previous commit</span></span><br><span class="line"><span class="comment"># f, fixup &lt;commit&gt; = like &quot;squash&quot;, but discard this commit&#x27;s log message</span></span><br><span class="line"><span class="comment"># x, exec &lt;command&gt; = run command (the rest of the line) using shell</span></span><br><span class="line"><span class="comment"># b, break = stop here (continue rebase later with &#x27;git rebase --continue&#x27;)</span></span><br><span class="line"><span class="comment"># d, drop &lt;commit&gt; = remove commit</span></span><br><span class="line"><span class="comment"># l, label &lt;label&gt; = label current HEAD with a name</span></span><br><span class="line"><span class="comment"># t, reset &lt;label&gt; = reset HEAD to a label</span></span><br><span class="line"><span class="comment"># m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]</span></span><br><span class="line"><span class="comment"># .       create a merge commit using the original merge commit&#x27;s</span></span><br></pre></td></tr></table></div></figure></li>
<li>冲突解决<br>如果产生冲突，就解决冲突，继续执行 rebase<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">git rebase --<span class="built_in">continue</span></span><br></pre></td></tr></table></div></figure></li>
<li>停止 rebase<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">git rebase --abort</span><br></pre></td></tr></table></div></figure>
</li>
</ul>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo之SEO优化</title>
    <url>/posts/cfd1b897/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>为你的站点生成站点地图并提交百度和谷歌收录</p>
<a id="more"></a>


        <h3 id="文章永久链接"   >
          <a href="#文章永久链接" class="heading-link"><i class="fas fa-link"></i></a>文章永久链接</h3>
      <p>这样你修改文章名称或者日期后，文章链接不会发生变化<br>安装插件</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">npm install hexo-abbrlink --save</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>
<p>修改站点配置文件</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">permalink: posts&#x2F;:abbrlink&#x2F;</span><br><span class="line">abbrlink:</span><br><span class="line">    alg: crc32   #算法： crc16(default) and crc32</span><br><span class="line">    rep: hex</span><br></pre></td></tr></table></div></figure>


        <h3 id="生成站点地图"   >
          <a href="#生成站点地图" class="heading-link"><i class="fas fa-link"></i></a>生成站点地图</h3>
      <p>安装插件</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">npm install hexo-generator-baidu-sitemap --save</span><br><span class="line">npm install hexo-generator-sitemap --save</span><br></pre></td></tr></table></div></figure>
<p>修改站点配置文件</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">url: https:&#x2F;&#x2F;flyraty.github.io</span><br><span class="line"># sitemap</span><br><span class="line">Plugins:</span><br><span class="line">  - hexo-generator-baidu-sitemap</span><br><span class="line">  - hexo-generator-sitemap</span><br><span class="line">baidusitemap:</span><br><span class="line">  path: baidusitemap.xml</span><br><span class="line">sitemap:</span><br><span class="line">  path: sitemap.xml</span><br></pre></td></tr></table></div></figure>
<p><code>hexo g</code> ，<code>hexo s</code> 后，可以访问 <code>localhost:4000/sitemap.xml</code> 查看站点地图。</p>

        <h3 id="提交到-google-search-console"   >
          <a href="#提交到-google-search-console" class="heading-link"><i class="fas fa-link"></i></a>提交到 google search console</h3>
      <p>查看站点是否被收录。</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">site:https:&#x2F;&#x2F;flyraty.github.io</span><br></pre></td></tr></table></div></figure>
<p>登录 google search console ，验证自己对网站的所有权，选择适合自己的方式，建议选择验证码方式，直接修改主题配置文件</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">google_site_verification: zkSnlx4XqngA-8SYFGRahJ85Xh3odO9uB6ILJk6UZHM</span><br></pre></td></tr></table></div></figure>
<p>重新部署后，点击验证。提交 sitemap。</p>

        <h3 id="提交百度收录"   >
          <a href="#提交百度收录" class="heading-link"><i class="fas fa-link"></i></a>提交百度收录</h3>
      <p>登录百度搜索资源平台，添加站点信息，验证自己对网站的所有权，建议选择验证码方式，直接修改主题配置文件</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">baidu_site_verification: code-XBmDG5fVMm</span><br></pre></td></tr></table></div></figure>
<p>提交 sitemap</p>

        <h3 id="新文章自动提交百度收录"   >
          <a href="#新文章自动提交百度收录" class="heading-link"><i class="fas fa-link"></i></a>新文章自动提交百度收录</h3>
      <p>安装插件</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">npm install hexo-baidu-url-submit --save</span><br></pre></td></tr></table></div></figure>
<p>修改站点配置文件</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">baidu_url_submit:</span><br><span class="line">  count: 3 ## 比如3，代表提交最新的三个链接</span><br><span class="line">  host: https:&#x2F;&#x2F;flyraty.github.io ## 在百度站长平台中注册的域名</span><br><span class="line">  token: xxxxx ## 请注意这是您的秘钥， 请不要发布在公众仓库里!</span><br><span class="line">  path: baidu_urls.txt ## 文本文档的地址， 新链接会保存在此文本文档里</span><br></pre></td></tr></table></div></figure>
<p>以后每次 hexo d -g 的时候都会主动推送百度</p>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo + github pages 打造你自己的博客</title>
    <url>/posts/c5404504/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>一周前手贱，<code>spark.write.mode(&quot;overwrite&quot;).save(&quot;xxx&quot;)</code> 覆盖了家目录，导致大部分代码和博客源文件被删除，mac 的个性化配置基本上也都没了，欲哭无泪，生活还要继续。一切从头再来吧，本文记录重新搭建博客的过程，踩坑甚多（主要是 next 7.x 相比以前的版本更改了很多配置项）。本文主要参考了 <span class="exturl"><a class="exturl__link"   href="https://tding.top/" >小丁的博客</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>。</p>
<a id="more"></a>
<p>tips: 强烈建议大家一定要备份，备份，备份！</p>

        <h3 id="hexo"   >
          <a href="#hexo" class="heading-link"><i class="fas fa-link"></i></a>hexo</h3>
      
        <h4 id="hexo-安装"   >
          <a href="#hexo-安装" class="heading-link"><i class="fas fa-link"></i></a>hexo 安装</h4>
      <p>创建 github pages 和创建普通的 github 仓库没太大区别（记住仓库名称是 username.github.io 就行了），这里就不在赘述了。以下的安装基于 mac，其实都差不多，现在的操作系统都提供了良好的包管理工具用于简化软件的安装。</p>
<ul>
<li>安装 node 和 npm<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">brew install node</span><br></pre></td></tr></table></div></figure>
如果你想安装其他版本的 node，可以使用 <code>brew search node</code> ，找到对应的版本包然后安装。</li>
<li>安装 hexo <figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></div></figure></li>
<li>hexo init<br>hexo 初始化并自动创建博客目录，根据提示，初始化完成后，进入到博客目录，执行 <code>npm install</code><figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">hexo init Flyraty.github.io</span><br></pre></td></tr></table></div></figure></li>
<li>常用命令<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">hexo d -g 生成并部署</span><br><span class="line">hexo s 本地部署</span><br><span class="line">hexo g 生成站点</span><br><span class="line">hexo clean</span><br><span class="line">hexo new 新建文章</span><br></pre></td></tr></table></div></figure>

</li>
</ul>

        <h4 id="基础配置"   >
          <a href="#基础配置" class="heading-link"><i class="fas fa-link"></i></a>基础配置</h4>
      <p>一下是一些基础配置，主题选择，页面布局等，我们只需要记住两个配置文件就行了。</p>
<ul>
<li>站点配置文件 <code>_config.yml</code></li>
<li>主题配置文件 <code>themes/主题名/_config.yml</code>，我这里就是 <code>themes/next/_config.yml</code></li>
</ul>

        <h5 id="next-安装"   >
          <a href="#next-安装" class="heading-link"><i class="fas fa-link"></i></a>next 安装</h5>
      <p>下载解压放入 themes 目录或者 git clone。</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;iissnan&#x2F;hexo-theme-next themes&#x2F;next</span><br></pre></td></tr></table></div></figure>
<p>推荐 git 的方式，可以持续跟进更新 next。也可以用 git submoudle 维护。</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">git stash</span><br><span class="line">git pull</span><br><span class="line">git stash pop</span><br></pre></td></tr></table></div></figure>

        <h5 id="启用-next"   >
          <a href="#启用-next" class="heading-link"><i class="fas fa-link"></i></a>启用 next</h5>
      <p>编辑站点配置文件，修改 theme 选项。</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="attr">theme:</span> <span class="string">next</span></span><br></pre></td></tr></table></div></figure>

        <h5 id="更换-Schema"   >
          <a href="#更换-Schema" class="heading-link"><i class="fas fa-link"></i></a>更换 Schema</h5>
      <p>next 主题同样有好几种模板，这里选用的是 Gemini，因为文章间的界限比较清晰。</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Schemes</span></span><br><span class="line"><span class="comment">#scheme: Muse</span></span><br><span class="line"><span class="comment">#scheme: Mist</span></span><br><span class="line"><span class="comment">#scheme: Pisces</span></span><br><span class="line"><span class="attr">scheme:</span> <span class="string">Gemini</span></span><br></pre></td></tr></table></div></figure>


        <h5 id="新建标签，分类，关于页面"   >
          <a href="#新建标签，分类，关于页面" class="heading-link"><i class="fas fa-link"></i></a>新建标签，分类，关于页面</h5>
      <p>会在 source 目录下生成对应的文件夹。这几个页面也是 markdown 文件，你可以自由编辑，比如关于页面。</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">hexo new page tag</span><br><span class="line">hexo new page categories</span><br><span class="line">hexo new page about</span><br></pre></td></tr></table></div></figure>

        <h5 id="设置站点语言"   >
          <a href="#设置站点语言" class="heading-link"><i class="fas fa-link"></i></a>设置站点语言</h5>
      <p>修改站点配置文件，如果你发现配置不管用的话，可以查看下 <code>themes/next/languages</code> 目录，看下是否存在 zh-Hans.yml 或者 zh-CN.yml。如果只存在 zh-CN.yml，重命名成 zh-Hans.yml 即可。</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="attr">language:</span> <span class="string">zh-Hans</span></span><br></pre></td></tr></table></div></figure>

        <h5 id="设置侧边栏菜单"   >
          <a href="#设置侧边栏菜单" class="heading-link"><i class="fas fa-link"></i></a>设置侧边栏菜单</h5>
      <p>修改主题配置文件，想显示哪个菜单，把对应的注释去掉就行。</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="attr">menu:</span></span><br><span class="line">  <span class="attr">home:</span> <span class="string">/</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-home</span></span><br><span class="line">  <span class="attr">about:</span> <span class="string">/about/</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-user</span></span><br><span class="line">  <span class="attr">tags:</span> <span class="string">/tags</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-tags</span></span><br><span class="line">  <span class="attr">categories:</span> <span class="string">/categories</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-th</span></span><br><span class="line">  <span class="attr">archives:</span> <span class="string">/archives</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-archive</span></span><br><span class="line">  <span class="comment">#schedule: /schedule/ || fa fa-calendar</span></span><br><span class="line">  <span class="comment">#sitemap: /sitemap.xml || fa fa-sitemap</span></span><br><span class="line">  <span class="comment">#commonweal: /404/ || fa fa-heartbeat</span></span><br></pre></td></tr></table></div></figure>

        <h5 id="侧栏位置"   >
          <a href="#侧栏位置" class="heading-link"><i class="fas fa-link"></i></a>侧栏位置</h5>
      <p>修改主题配置文件</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="attr">sidebar:</span></span><br><span class="line">  <span class="comment"># Sidebar Position.</span></span><br><span class="line">  <span class="attr">position:</span> <span class="string">left</span></span><br><span class="line">  <span class="comment">#position: right</span></span><br></pre></td></tr></table></div></figure>

        <h5 id="头像"   >
          <a href="#头像" class="heading-link"><i class="fas fa-link"></i></a>头像</h5>
      <p>修改主题配置文件，可以选择是否圆框，是否鼠标点击头像旋转。</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line"># Sidebar Avatar</span><br><span class="line">avatar:</span><br><span class="line">  # Replace the default image and set the url here.</span><br><span class="line">  url: https:&#x2F;&#x2F;avatars2.githubusercontent.com&#x2F;u&#x2F;24888835?s&#x3D;400&amp;u&#x3D;20f46b828b9ee5d5a93dfce95ec7c01d07cff6cf&amp;v&#x3D;4</span><br><span class="line">  # If true, the avatar will be dispalyed in circle.</span><br><span class="line">  rounded: true</span><br><span class="line">  # If true, the avatar will be rotated with the cursor.</span><br><span class="line">  rotated: true</span><br></pre></td></tr></table></div></figure>

        <h5 id="站点描述"   >
          <a href="#站点描述" class="heading-link"><i class="fas fa-link"></i></a>站点描述</h5>
      <p>修改站点配置文件，主要是站点名称，描述，关键字，作者这些</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Site</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">TimeMachine</span> <span class="string">Notes</span></span><br><span class="line"><span class="attr">subtitle:</span> <span class="string">&#x27;愿一直遇到有趣的人和事&#x27;</span></span><br><span class="line"><span class="attr">descriptin:</span> <span class="string">&#x27;于离别之朝束起约定之花&#x27;</span></span><br><span class="line"><span class="attr">keywords:</span> <span class="string">大数据,bigdata,Mac,Linux,生活,life</span></span><br><span class="line"><span class="attr">author:</span> <span class="string">时光机</span></span><br><span class="line"><span class="attr">language:</span> <span class="string">zh-Hans</span></span><br><span class="line"><span class="attr">timezone:</span> <span class="string">Asia/shanghai</span></span><br><span class="line"></span><br></pre></td></tr></table></div></figure>

        <h5 id="开启阅读数，字数统计"   >
          <a href="#开启阅读数，字数统计" class="heading-link"><i class="fas fa-link"></i></a>开启阅读数，字数统计</h5>
      <p>修改主题配置文件</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line"># Post wordcount display settings</span><br><span class="line"># Dependencies: https:&#x2F;&#x2F;github.com&#x2F;theme-next&#x2F;hexo-symbols-count-time</span><br><span class="line">symbols_count_time:</span><br><span class="line">  separated_meta: true</span><br><span class="line">  item_text_post: true</span><br><span class="line">  item_text_total: false</span><br><span class="line">  awl: 4</span><br><span class="line">  wpm: 275</span><br></pre></td></tr></table></div></figure>

        <h5 id="不蒜子统计"   >
          <a href="#不蒜子统计" class="heading-link"><i class="fas fa-link"></i></a>不蒜子统计</h5>
      <p>添加站点统计，比如 uv，pv 这些，可以选择百度统计，谷歌统计，不蒜字等。可以参考<span class="exturl"><a class="exturl__link"   href="https://theme-next.iissnan.com/third-party-services.html" >官方文档</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>。这里选用了不蒜子<br>修改主题配置文件</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="attr">busuanzi_count:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">total_visitors:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">total_visitors_icon:</span> <span class="string">fa</span> <span class="string">fa-user</span></span><br><span class="line">  <span class="attr">total_views:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">total_views_icon:</span> <span class="string">fa</span> <span class="string">fa-eye</span></span><br><span class="line">  <span class="attr">post_views:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">post_views_icon:</span> <span class="string">fa</span> <span class="string">fa-eye</span></span><br></pre></td></tr></table></div></figure>

        <h5 id="添加搜索栏"   >
          <a href="#添加搜索栏" class="heading-link"><i class="fas fa-link"></i></a>添加搜索栏</h5>
      <p>安装搜索插件</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">npm install hexo-generator-searchdb --save</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>
<p>修改站点配置文件，添加 search 配置</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="comment"># search</span></span><br><span class="line"><span class="attr">search:</span></span><br><span class="line">  <span class="attr">path:</span> <span class="string">search.xml</span></span><br><span class="line">  <span class="attr">field:</span> <span class="string">post</span></span><br><span class="line">  <span class="attr">format:</span> <span class="string">html</span></span><br><span class="line">  <span class="attr">limit:</span> <span class="number">10000</span></span><br></pre></td></tr></table></div></figure>

        <h5 id="RSS-订阅"   >
          <a href="#RSS-订阅" class="heading-link"><i class="fas fa-link"></i></a>RSS 订阅</h5>
      <p>安装插件</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">npm install hexo-generator-feed --save</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>
<p>修改站点配置文件，添加以下内容</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line"># RSS</span><br><span class="line"># feed</span><br><span class="line"># Dependencies: https:&#x2F;&#x2F;github.com&#x2F;hexojs&#x2F;hexo-generator-feed</span><br><span class="line">feed:</span><br><span class="line">  type: atom</span><br><span class="line">  path: atom.xml</span><br><span class="line">  limit: 20</span><br><span class="line">  hub:</span><br><span class="line">  content:</span><br></pre></td></tr></table></div></figure>
<p>修改主题配置文件</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">follow_me:</span><br><span class="line">  #Twitter: https:&#x2F;&#x2F;twitter.com&#x2F;username || fab fa-twitter</span><br><span class="line">  #Telegram: https:&#x2F;&#x2F;t.me&#x2F;channel_name || fab fa-telegram</span><br><span class="line">  #WeChat: &#x2F;images&#x2F;wechat_channel.jpg || fab fa-weixin</span><br><span class="line">  RSS: &#x2F;atom.xml || fa fa-rss</span><br></pre></td></tr></table></div></figure>

        <h5 id="相关文章"   >
          <a href="#相关文章" class="heading-link"><i class="fas fa-link"></i></a>相关文章</h5>
      <p>安装插件</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">npm install hexo-related-popular-posts --save</span><br></pre></td></tr></table></div></figure>
<p>修改主题配置文件</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="attr">related_posts:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">title:</span> <span class="string">相关文章推荐#</span> <span class="string">Custom</span> <span class="string">header,</span> <span class="string">leave</span> <span class="string">empty</span> <span class="string">to</span> <span class="string">use</span> <span class="string">the</span> <span class="string">default</span> <span class="string">one</span></span><br><span class="line">  <span class="attr">display_in_home:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">params:</span></span><br><span class="line">    <span class="attr">maxCount:</span> <span class="number">5</span></span><br><span class="line">    <span class="comment">#PPMixingRate: 0.0</span></span><br><span class="line">    <span class="comment">#isDate: false</span></span><br><span class="line">    <span class="comment">#isImage: false</span></span><br><span class="line">    <span class="attr">isExcerpt:</span> <span class="string">fals</span></span><br></pre></td></tr></table></div></figure>

        <h5 id="社交地址"   >
          <a href="#社交地址" class="heading-link"><i class="fas fa-link"></i></a>社交地址</h5>
      <p>修改主题配置文件，用自带的图标其实就可以了</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="attr">social:</span></span><br><span class="line">  <span class="attr">GitHub:</span> <span class="string">https://github.com/Flyraty</span> <span class="string">||</span> <span class="string">fab</span> <span class="string">fa-github</span></span><br><span class="line">  <span class="attr">Zhihu:</span> <span class="string">https://www.zhihu.com/people/zhang-hai-liang-83-28</span> <span class="string">||</span> <span class="string">fab</span> <span class="string">fa-zhihu</span></span><br><span class="line">  <span class="attr">E-Mail:</span> <span class="string">139hailiangabc@gmail.com</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-envelope</span></span><br><span class="line">  <span class="attr">Weibo:</span> <span class="string">https://weibo.com/yourname</span> <span class="string">||</span> <span class="string">fab</span> <span class="string">fa-weibo</span></span><br><span class="line">  <span class="comment">#Google: https://plus.google.com/yourname || fab fa-google</span></span><br><span class="line">  <span class="comment">#Twitter: https://twitter.com/yourname || fab fa-twitter</span></span><br><span class="line">  <span class="comment">#FB Page: https://www.facebook.com/yourname || fab fa-facebook</span></span><br><span class="line">  <span class="comment">#StackOverflow: https://stackoverflow.com/yourname || fab fa-stack-overflow</span></span><br><span class="line">  <span class="comment">#YouTube: https://youtube.com/yourname || fab fa-youtube</span></span><br><span class="line">  <span class="comment">#Instagram: https://instagram.com/yourname || fab fa-instagram</span></span><br><span class="line">  <span class="comment">#Skype: skype:yourname?call|chat || fab fa-skype</span></span><br><span class="line"></span><br><span class="line"><span class="attr">social_icons:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">icons_only:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">transition:</span> <span class="literal">false</span></span><br></pre></td></tr></table></div></figure>

        <h5 id="自动生成摘要"   >
          <a href="#自动生成摘要" class="heading-link"><i class="fas fa-link"></i></a>自动生成摘要</h5>
      <p>自动生成摘要在 next7 中被去除掉了。不想显示全文的话，有两种方式控制，建议第一种方式</p>
<ul>
<li><code>&lt;!--more--&gt;</code>，会显示之前的内容，之后的内容不会显示</li>
<li>文章 meta 中添加 description字段<br>不要在设置啥 auto_encrypt 了。。。</li>
</ul>

        <h5 id="添加-Disqus-评论系统"   >
          <a href="#添加-Disqus-评论系统" class="heading-link"><i class="fas fa-link"></i></a>添加 Disqus 评论系统</h5>
      <p>访问 <code>disqus.com</code>，选择 <code>i want to install disqus on my site</code>。然后跟着提示一步步走，只需要记住自己的 shortname 即可。<br>修改主题配置文件</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"> <span class="string">Disqus</span></span><br><span class="line"><span class="attr">disqus:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">shortname:</span> <span class="string">flyraty</span></span><br><span class="line">  <span class="attr">count:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment">#post_meta_order: 0</span></span><br></pre></td></tr></table></div></figure>


        <h5 id="部署到-git"   >
          <a href="#部署到-git" class="heading-link"><i class="fas fa-link"></i></a>部署到 git</h5>
      <p>安装插件</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></div></figure>
<p>修改站点配置文件，部署站点，如果你以前没有使用过 git 的话，最好配置下 SSH</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  - type: git</span><br><span class="line">    repo: git@github.com:Flyraty&#x2F;Flyraty.github.io.git</span><br><span class="line">    branch: master</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>
<p>直接 <code>hexo d -g</code> 并不会把你的博客源文件，比如文章 markdown push 到仓库，只是生成 public 下的静态 html push 到了站点进行渲染。此处建议新开一分支 hexo ，push 站点源文件到 hexo 分之上用于备份，可以把 hexo 分支设置为默认分支。</p>

        <h4 id="美化"   >
          <a href="#美化" class="heading-link"><i class="fas fa-link"></i></a>美化</h4>
      
        <h5 id="代码框风格，并添加复制按钮"   >
          <a href="#代码框风格，并添加复制按钮" class="heading-link"><i class="fas fa-link"></i></a>代码框风格，并添加复制按钮</h5>
      <p>修改主题配置文件，设置 theme 和 style</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="attr">codeblock:</span></span><br><span class="line">  <span class="comment"># Code Highlight theme</span></span><br><span class="line">  <span class="comment"># Available values: normal | night | night eighties | night blue | night bright | solarized | solarized dark | galactic</span></span><br><span class="line">  <span class="comment"># See: https://github.com/chriskempson/tomorrow-theme</span></span><br><span class="line">  <span class="attr">highlight_theme:</span> <span class="string">night</span></span><br><span class="line">  <span class="comment"># Add copy button on codeblock</span></span><br><span class="line">  <span class="attr">copy_button:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="comment"># Show text copy result.</span></span><br><span class="line">    <span class="attr">show_result:</span> <span class="literal">true</span></span><br><span class="line">    <span class="comment"># Available values: default | flat | mac</span></span><br><span class="line">    <span class="attr">style:</span> <span class="string">mac</span></span><br></pre></td></tr></table></div></figure>

        <h5 id="动态背景图片"   >
          <a href="#动态背景图片" class="heading-link"><i class="fas fa-link"></i></a>动态背景图片</h5>
      <p>新建 source/_date/styles.styl，添加如下代码，如果想要设置动态背景效果，只需要将 <code>background:url</code>替换成一个动态背景接口就行了。<br>设置了背景图片，可能会导致文字看不到，所以需要设置透明度。<br>tips: 推荐图片网址 <code>https://wallhaven.cc/</code></p>
<details>
    <summary>styles.styl</summary>>
    <figure class="highlight css"><div class="table-container"><table><tr><td class="code"><pre><span class="line">// 设置背景图片</span><br><span class="line"><span class="selector-tag">body</span> &#123;</span><br><span class="line">    <span class="attribute">background</span>:<span class="built_in">url</span>(/images/girl.jpg);</span><br><span class="line">    <span class="attribute">background-repeat</span>: no-repeat;</span><br><span class="line">    background-attachment:fixed; //不重复</span><br><span class="line">    background-size: cover;      //填充</span><br><span class="line">    <span class="selector-tag">background-position</span><span class="selector-pseudo">:50</span>% 50%;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//博客内容透明化</span><br><span class="line">//文章内容的透明度设置</span><br><span class="line"><span class="selector-class">.content-wrap</span> &#123;</span><br><span class="line">  <span class="attribute">opacity</span>: <span class="number">0.9</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//侧边框的透明度设置</span><br><span class="line"><span class="selector-class">.sidebar</span> &#123;</span><br><span class="line">  <span class="attribute">opacity</span>: <span class="number">0.9</span>;</span><br><span class="line">&#125;</span><br><span class="line">//菜单栏的透明度设置</span><br><span class="line"><span class="selector-class">.header-inner</span> &#123;</span><br><span class="line">  <span class="attribute">background</span>: <span class="built_in">rgba</span>(<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>,<span class="number">0.9</span>);</span><br><span class="line">&#125;</span><br><span class="line">//搜索框（local-search）的透明度设置</span><br><span class="line"><span class="selector-class">.popup</span> &#123;</span><br><span class="line">  <span class="attribute">opacity</span>: <span class="number">0.9</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

</details>

<p>修改主题配置文件，打开自定义 styles.styl 设置</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="attr">custom_file_path:</span></span><br><span class="line">	<span class="attr">style:</span> <span class="string">source/_data/styles.sty</span></span><br></pre></td></tr></table></div></figure>


        <h5 id="页面动画效果"   >
          <a href="#页面动画效果" class="heading-link"><i class="fas fa-link"></i></a>页面动画效果</h5>
      <p>hexo 内置了一些页面动态效果。如果想打开的话，只需要在主题配置文件里搜索打开即可。</p>
<ul>
<li>canvas_nest</li>
<li>canvas_ribbon</li>
<li>three_waves </li>
</ul>
<p>比如 canvas_nest</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Canvas-nest</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/theme-next/theme-next-canvas-nest</span></span><br><span class="line"><span class="attr">canvas_nest:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">onmobile:</span> <span class="literal">true</span> <span class="comment"># display on mobile or not</span></span><br><span class="line">  <span class="attr">color:</span> <span class="string">&quot;0,0,255&quot;</span> <span class="comment"># RGB values, use &#x27;,&#x27; to separate</span></span><br><span class="line">  <span class="attr">opacity:</span> <span class="number">0.5</span> <span class="comment"># the opacity of line: 0~1</span></span><br><span class="line">  <span class="attr">zIndex:</span> <span class="number">-1</span> <span class="comment"># z-index property of the background</span></span><br><span class="line">  <span class="attr">count:</span> <span class="number">99</span> <span class="comment"># the number of lines</span></span><br></pre></td></tr></table></div></figure>


        <h5 id="页面顶部加载阅读进度条"   >
          <a href="#页面顶部加载阅读进度条" class="heading-link"><i class="fas fa-link"></i></a>页面顶部加载阅读进度条</h5>
      <p>修改主题配置文件</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Reading progress bar</span></span><br><span class="line"><span class="attr">reading_progress:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># Available values: top | bottom</span></span><br><span class="line">  <span class="attr">position:</span> <span class="string">top</span></span><br><span class="line">  <span class="attr">color:</span> <span class="string">&quot;#37c6c0&quot;</span></span><br><span class="line">  <span class="attr">height:</span> <span class="string">3px</span></span><br></pre></td></tr></table></div></figure>

        <h5 id="文章阅读进度条"   >
          <a href="#文章阅读进度条" class="heading-link"><i class="fas fa-link"></i></a>文章阅读进度条</h5>
      <p>安装插件</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">npm install hexo-cake-moon-menu --save</span><br></pre></td></tr></table></div></figure>
<p>修改主题配置文件，添加如下内容</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="attr">moon_menu:</span></span><br><span class="line">  <span class="attr">back2top:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">icon:</span> <span class="string">fa</span> <span class="string">fa-chevron-up</span></span><br><span class="line">    <span class="attr">func:</span> <span class="string">back2top</span></span><br><span class="line">    <span class="attr">order:</span> <span class="number">-1</span></span><br><span class="line">  <span class="attr">back2bottom:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">icon:</span> <span class="string">fa</span> <span class="string">fa-chevron-down</span></span><br><span class="line">    <span class="attr">func:</span> <span class="string">back2bottom</span></span><br><span class="line">    <span class="attr">order:</span> <span class="number">-2</span></span><br></pre></td></tr></table></div></figure>
<p>next  自带了文章阅读进度条（pace 配置），但是不如这个插件好看。</p>

        <h5 id="鼠标点击烟花效果"   >
          <a href="#鼠标点击烟花效果" class="heading-link"><i class="fas fa-link"></i></a>鼠标点击烟花效果</h5>
      <p>参考 <span class="exturl"><a class="exturl__link"   href="https://tding.top/archives/58cff12b.html" >小丁的博客</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h5 id="修改页面布局为圆角"   >
          <a href="#修改页面布局为圆角" class="heading-link"><i class="fas fa-link"></i></a>修改页面布局为圆角</h5>
      <p>新建 source/_data/variables.styl</p>
<figure class="highlight"><div class="table-container"><table><tr><td class="code"><pre><span class="line">// 圆角设置</span><br><span class="line">$border-radius-inner     = 20px 20px 20px 20px;</span><br><span class="line">$border-radius           = 20px;</span><br></pre></td></tr></table></div></figure>
<p>修改主题配置文件，打开自定义 variables.styl 的设置</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="attr">custom_file_path:</span></span><br><span class="line">	<span class="attr">variable:</span> <span class="string">source/_data/variables.sty</span></span><br></pre></td></tr></table></div></figure>


        <h5 id="添加粒子时钟"   >
          <a href="#添加粒子时钟" class="heading-link"><i class="fas fa-link"></i></a>添加粒子时钟</h5>
      <p>参考 <span class="exturl"><a class="exturl__link"   href="https://tding.top/archives/dd68b70.html" >小丁的博客</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h5 id="去掉底部·强力驱动·"   >
          <a href="#去掉底部·强力驱动·" class="heading-link"><i class="fas fa-link"></i></a>去掉底部·强力驱动·</h5>
      <p>修改主题配置文件</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">footer:</span><br><span class="line">	power: false</span><br></pre></td></tr></table></div></figure>

        <h5 id="关于页面显示-github-commit-chart"   >
          <a href="#关于页面显示-github-commit-chart" class="heading-link"><i class="fas fa-link"></i></a>关于页面显示 github commit chart</h5>
      <p>参考 <span class="exturl"><a class="exturl__link"   href="https://tding.top/" >小丁的博客</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h4 id="优化加速"   >
          <a href="#优化加速" class="heading-link"><i class="fas fa-link"></i></a>优化加速</h4>
      
        <h5 id="启用-FastClick"   >
          <a href="#启用-FastClick" class="heading-link"><i class="fas fa-link"></i></a>启用 FastClick</h5>
      <p>修改主题配置文件</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">fastclick: true</span><br></pre></td></tr></table></div></figure>

        <h5 id="启用-QuickLink"   >
          <a href="#启用-QuickLink" class="heading-link"><i class="fas fa-link"></i></a>启用 QuickLink</h5>
      <p>修改主题配置文件，quickclick 用于资源文件的预加载</p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="attr">quicklink:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Home page and archive page can be controlled through home and archive options below.</span></span><br><span class="line">  <span class="comment"># This configuration item is independent of `enable`.</span></span><br><span class="line">  <span class="attr">home:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">archive:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Default (true) will initialize quicklink after the load event fires.</span></span><br><span class="line">  <span class="attr">delay:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># Custom a time in milliseconds by which the browser must execute prefetching.</span></span><br><span class="line">  <span class="attr">timeout:</span> <span class="number">3000</span></span><br><span class="line">  <span class="comment"># Default (true) will enable fetch() or falls back to XHR.</span></span><br><span class="line">  <span class="attr">priority:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># For more flexibility you can add some patterns (RegExp, Function, or Array) to ignores.</span></span><br><span class="line">  <span class="comment"># See: https://github.com/GoogleChromeLabs/quicklink#custom-ignore-patterns</span></span><br><span class="line">  <span class="attr">ignores:</span></span><br><span class="line"></span><br></pre></td></tr></table></div></figure>

        <h5 id="SEO"   >
          <a href="#SEO" class="heading-link"><i class="fas fa-link"></i></a>SEO</h5>
      <p>主要就是生成站点地图并提交百度和谷歌收录，生成永久链接，参考 <span class="exturl"><a class="exturl__link"   href="https://flyraty.github.io/posts/cfd1b897/" >https://flyraty.github.io/posts/cfd1b897/</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h3 id="参考"   >
          <a href="#参考" class="heading-link"><i class="fas fa-link"></i></a>参考</h3>
      <p><span class="exturl"><a class="exturl__link"   href="https://tding.top/" >小丁的博客</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo绑定域名和更换网站图标</title>
    <url>/posts/ea2b28e2/</url>
    <content><![CDATA[
        <h3 id="绑定域名"   >
          <a href="#绑定域名" class="heading-link"><i class="fas fa-link"></i></a>绑定域名</h3>
      <ul>
<li>注册域名，我选的是阿里云，注册了 .icu 域名。需要经过实名认证后才能使用，然后域名管理中更改下主体信息。基本上跟着步骤走就行。</li>
<li>域名解析，在域名管理中新建 CNAME 域名解析，其中记录值就是你要将注册域名解析到哪里，比如我这里就是 flyraty.github.io。<a id="more"></a>
<img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1gisghn2l7rj31ue04qwfe.jpg"></li>
<li>修改 github pages 配置，去往你的博客仓库，点击 settings 找到如下内容，修改 custom domain<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1gisglk8oymj31h60pugqt.jpg"></li>
</ul>

        <h3 id="更换网站图标"   >
          <a href="#更换网站图标" class="heading-link"><i class="fas fa-link"></i></a>更换网站图标</h3>
      <ul>
<li>去图标库下载自己喜欢的图标，推荐<span class="exturl"><a class="exturl__link"   href="https://www.iconfont.cn/" >阿里矢量图标库</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>。下载图标的 32<em>32，16</em>16 尺寸。</li>
<li>复制两张图标文件到 <code>themes/next/source/images</code></li>
<li>修改主题配置文件，更改 favicon 的 small 和 medium 选项<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">favicon:</span><br><span class="line">  small: &#x2F;images&#x2F;jiqiren-16x16.svg</span><br><span class="line">  medium: &#x2F;images&#x2F;jiqiren-32x32.svg</span><br></pre></td></tr></table></div></figure></li>
<li><code>hexo d -g</code> 部署你的站点</li>
</ul>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>github actions 实现 hexo 自动化部署</title>
    <url>/posts/1eb3f811/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>使用 github pages 托管个人博客网站，使用双分支来保存博客源文件，使用 git submodule 来管理更新主题文件，使用 github actions 来做持续集成。</p>
<a id="more"></a>


        <h3 id="hexo-持续集成"   >
          <a href="#hexo-持续集成" class="heading-link"><i class="fas fa-link"></i></a>hexo 持续集成</h3>
      
        <h4 id="生成公钥私钥"   >
          <a href="#生成公钥私钥" class="heading-link"><i class="fas fa-link"></i></a>生成公钥私钥</h4>
      <p>这一步主要是为了 CI 中提交代码，生成了两个文件，公钥文件 github-deploy-key.pub，私钥文件 github-deploy-key。需要注意，如果你是在博客目录执行的命令，需要在 .gitignore 中加入这两个文件，避免上传到仓库中。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa  -C <span class="string">&quot;<span class="subst">$(git config user.name)</span>&quot;</span> -f github-deploy-key</span><br></pre></td></tr></table></div></figure>


        <h4 id="添加仓库环境变量"   >
          <a href="#添加仓库环境变量" class="heading-link"><i class="fas fa-link"></i></a>添加仓库环境变量</h4>
      <p>设置 HEXO_DEPLOY_PUB，value 是上步生成的 github-deploy-key.pub 文件内容。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/0081Kckwgy1gk0pc9xn3hj31yq0s6aex.jpg"><br>设置 HEXO_DEPLOY_PRI<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/0081Kckwgy1gk0pe8e498j321e0t8djp.jpg"></p>

        <h4 id="添加-workflow"   >
          <a href="#添加-workflow" class="heading-link"><i class="fas fa-link"></i></a>添加 workflow</h4>
      <p>编写 workflow，新建的时候会有对应的注释提示你该如何写。<strong>需要注意的是 submodule 不会自动下载，需要添加 check submodules 这一步。</strong></p>
<figure class="highlight yml"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">CI</span></span><br><span class="line"><span class="attr">on:</span></span><br><span class="line">  <span class="attr">push:</span></span><br><span class="line">    <span class="attr">branches:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">hexo</span></span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="attr">build:</span></span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Checkout</span> <span class="string">source</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">actions/checkout@v1</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="attr">ref:</span> <span class="string">hexo</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Configration</span> <span class="string">hexo</span> <span class="string">repo</span></span><br><span class="line">        <span class="attr">env:</span></span><br><span class="line">          <span class="attr">ACTION_DEPLOY_KEY:</span> <span class="string">$&#123;&#123;</span> <span class="string">secrets.HEXO_DEPLOY_PRI</span> <span class="string">&#125;&#125;</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line">          <span class="string">mkdir</span> <span class="string">-p</span> <span class="string">~/.ssh/</span></span><br><span class="line">          <span class="string">echo</span> <span class="string">&quot;$ACTION_DEPLOY_KEY&quot;</span> <span class="string">&gt;</span> <span class="string">~/.ssh/id_rsa</span></span><br><span class="line">          <span class="string">chmod</span> <span class="number">600</span> <span class="string">~/.ssh/id_rsa</span></span><br><span class="line">          <span class="string">ssh-keyscan</span> <span class="string">github.com</span> <span class="string">&gt;&gt;</span> <span class="string">~/.ssh/known_hosts</span></span><br><span class="line">          <span class="string">git</span> <span class="string">config</span> <span class="string">--global</span> <span class="string">user.email</span> <span class="string">&quot;1397554745@qq.com&quot;</span></span><br><span class="line">          <span class="string">git</span> <span class="string">config</span> <span class="string">--global</span> <span class="string">user.name</span> <span class="string">&quot;Flyraty&quot;</span> </span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Checkout</span> <span class="string">submodules</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line">          <span class="string">git</span> <span class="string">submodule</span> <span class="string">init</span></span><br><span class="line">          <span class="string">git</span> <span class="string">submodule</span> <span class="string">update</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Use</span> <span class="string">Node.js</span> <span class="string">$&#123;&#123;</span> <span class="string">matrix.node_version</span> <span class="string">&#125;&#125;</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">actions/setup-node@v1</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="attr">version:</span> <span class="string">$&#123;&#123;</span> <span class="string">matrix.node_version</span> <span class="string">&#125;&#125;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Setup</span> <span class="string">Hexo</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line">         <span class="string">npm</span> <span class="string">install</span> <span class="string">hexo-cli</span> <span class="string">-g</span></span><br><span class="line">         <span class="string">npm</span> <span class="string">install</span> </span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Hexo</span> <span class="string">deploy</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line">          <span class="string">hexo</span> <span class="string">clean</span></span><br><span class="line">          <span class="string">hexo</span> <span class="string">d</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></div></figure>


        <h4 id="测试持续集成"   >
          <a href="#测试持续集成" class="heading-link"><i class="fas fa-link"></i></a>测试持续集成</h4>
      <p>本地 hexo 分支提交代码即可，部署站点会由 github actions 自动完成。可以去仓库 actions 设置中查看执行完成的 flow。如果有错，点开查看错误的 step 修改即可。 </p>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>python命令行解析模块ArgPrase</title>
    <url>/posts/29312bb7/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>最近工作中需要封装一些 API 做成命令行工具，用到了 ArgParse，看似挺简单，其实坑也蛮多，主要是子命令父命令参数冲突的问题。</p>
<a id="more"></a>


        <h3 id="ArgParse"   >
          <a href="#ArgParse" class="heading-link"><i class="fas fa-link"></i></a>ArgParse</h3>
      <p>通过 parents 可以共用一些公共参数，比如下面的 trigger_parser 就可以使用 parent_parser 的参数<br>子命令中设置了 <code>add_help=False</code>，主要是为了解决子命令和父命令的 -h 参数冲突<br>公共参数设置了 <code>required=False</code>，主要是为了解决子命令一定需要父命令的参数</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="code"><pre><span class="line">parent_parser = argparse.ArgumentParser()</span><br><span class="line"><span class="comment"># 公共参数</span></span><br><span class="line">parent_parser.add_argument(<span class="string">&quot;-u&quot;</span>, <span class="string">&quot;--url&quot;</span>, dest=<span class="string">&quot;url&quot;</span>, help=<span class="string">&quot;sa url&quot;</span>)</span><br><span class="line">parent_parser.add_argument(<span class="string">&quot;-p&quot;</span>, <span class="string">&quot;--project&quot;</span>, dest=<span class="string">&quot;project&quot;</span>, help=<span class="string">&quot; project name&quot;</span>)</span><br><span class="line">parent_parser.add_argument(<span class="string">&quot;-t&quot;</span>, <span class="string">&quot;--token&quot;</span>, dest=<span class="string">&quot;token&quot;</span>, help=<span class="string">&quot;token，API secret&quot;</span>)</span><br><span class="line"><span class="comment"># 子命令</span></span><br><span class="line">sub_parser = parent_parser.add_subparsers(dest=<span class="string">&quot;subparsers_name&quot;</span>)</span><br><span class="line"><span class="comment"># trigger</span></span><br><span class="line">trigger_parser = sub_parser.add_parser(<span class="string">&quot;trigger&quot;</span>, help=<span class="string">&quot;trigger tag calculate&quot;</span>, parents=[parent_parser],</span><br><span class="line">                                       add_help=<span class="literal">False</span>)</span><br><span class="line">trigger_parser.add_argument(<span class="string">&quot;-d&quot;</span>, <span class="string">&quot;--etl_date&quot;</span>, dest=<span class="string">&quot;etl_date&quot;</span>, help=<span class="string">&quot;source data etl_date&quot;</span>, required=<span class="literal">True</span>)</span><br><span class="line">trigger_parser.add_argument(<span class="string">&quot;-e&quot;</span>, <span class="string">&quot;--event&quot;</span>, dest=<span class="string">&quot;event&quot;</span>, help=<span class="string">&quot;the tag that relation events&quot;</span>, required=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># delete</span></span><br><span class="line">delete_parser = sub_parser.add_parser(<span class="string">&quot;delete&quot;</span>, help=<span class="string">&quot;delete tag&quot;</span>, parents=[parent_parser], add_help=<span class="literal">False</span>)</span><br><span class="line">delete_parser.add_argument(<span class="string">&quot;-fd&quot;</span>, <span class="string">&quot;--from_date&quot;</span>, dest=<span class="string">&quot;from_date&quot;</span>, help=<span class="string">&quot;delete start date&quot;</span>, required=<span class="literal">True</span>)</span><br><span class="line">delete_parser.add_argument(<span class="string">&quot;-td&quot;</span>, <span class="string">&quot;--to_date&quot;</span>, dest=<span class="string">&quot;to_date&quot;</span>, help=<span class="string">&quot;delete end date&quot;</span>, required=<span class="literal">True</span>)</span><br><span class="line">delete_parser.add_argument(<span class="string">&quot;-i&quot;</span>, <span class="string">&quot;--tag_id&quot;</span>, dest=<span class="string">&quot;tag_id&quot;</span>, help=<span class="string">&quot;delete tag id&quot;</span>, required=<span class="literal">False</span>)</span><br><span class="line">delete_parser.add_argument(<span class="string">&quot;-f&quot;</span>, <span class="string">&quot;--tag_config&quot;</span>, dest=<span class="string">&quot;tag_config&quot;</span>, help=<span class="string">&quot;delete tag id file&quot;</span>, required=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">args = parent_parser.parse_args()</span><br></pre></td></tr></table></div></figure>

<p>关于使用，可以直接执行 <code>python3 xx.py -h</code>，<code>python3 xx.py trigger -h</code>查看。<br>如果觉得每次使用 python3 太麻烦，也可以重命名命令，就像下面这样，以后只需要 <code>etladmin -h </code>就好了</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="built_in">alias</span> etladmin=<span class="string">&quot;python3 xx.py&quot;</span></span><br></pre></td></tr></table></div></figure>
<p>最好的办法是使用 pyinstaller 打包成 unix可执行程序。在 dist 目录下可以找到打包的文件</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">pip3 install pyinstaller</span><br><span class="line">pyinstaller --version</span><br><span class="line">pyinstaller -w -F -p . xx.py <span class="comment"># -F 指定生成可执行文件，-w 去除黑框 -p . 指定程序的入口搜索路径，否则只能在生成目录下执行程序。</span></span><br></pre></td></tr></table></div></figure>

]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>shell多进程与管道</title>
    <url>/posts/25dc2834/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>并发编程很常见，因为在绝大多数场景下，业务上对任务的完成时间都是有要求的，一般情况并发度越高，任务的完成速度就越快。那么如何在 shell 里面实现部分代码块的并发执行呢？如何控制并发数量呢？</p>
<a id="more"></a>


        <h3 id="管道"   >
          <a href="#管道" class="heading-link"><i class="fas fa-link"></i></a>管道</h3>
      
        <h4 id="匿名管道"   >
          <a href="#匿名管道" class="heading-link"><i class="fas fa-link"></i></a>匿名管道</h4>
      <p>匿名管道就是我们常见的管道符 <code>|</code>，匿名管道的两端是两个文件描述符，一个只读端，一个只写端，这样其他的进程就无法连接到此匿名管道。<br>比如 <code>impala-shell -q sql | python xx.py</code>，shell 会创建两个进程来执行 imapla-shell 和 python 程序，impala-shell 进程的标准输出（文件描述符1）会作为 python 脚本的标准输入（文件描述符0）。两个进程之间并不知道管道的存在，只是从文件描述符中读取或者写入数据，内部实现被包在了 shell 里面。</p>

        <h4 id="命名管道"   >
          <a href="#命名管道" class="heading-link"><i class="fas fa-link"></i></a>命名管道</h4>
      <p>命名管道即 FIFO，具有和匿名管道一样的性质，但是需要注意以下几点</p>
<ul>
<li>命名管道存在于文件系统中，作为特殊文件存在</li>
<li>命名管道是双向字节流，读写缺少其一，便会堵塞</li>
<li>匿名管道是 shell 自动创建的，命名管道是由程序创建的（mkinfo），存在于文件系统中</li>
</ul>

        <h4 id="amp-实现多进程，管道控制并发数量"   >
          <a href="#amp-实现多进程，管道控制并发数量" class="heading-link"><i class="fas fa-link"></i></a>&amp; 实现多进程，管道控制并发数量</h4>
      <p>&amp; 可以把程序放到后台执行<br>命名管道用于控制并发数量</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 接收 SIGINT 信号（ctrl+C），关闭文件描述符读写并退出程序</span></span><br><span class="line"><span class="built_in">trap</span> <span class="string">&quot;exec 10&gt;&amp;-;exec 10&lt;&amp;-;exit 0&quot;</span> 2 </span><br><span class="line"></span><br><span class="line">thread_num=10</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建管道文件</span></span><br><span class="line">tempfifo=<span class="string">&quot;my.fifo&quot;</span></span><br><span class="line">mkfifo <span class="variable">$&#123;tempfifo&#125;</span></span><br><span class="line"><span class="comment"># 将文件描述符与管道文件绑定</span></span><br><span class="line"><span class="built_in">exec</span> 10&lt;&gt;<span class="variable">$&#123;tempfifo&#125;</span></span><br><span class="line"><span class="comment"># 为啥创建了又删除不太清楚</span></span><br><span class="line">rm -f <span class="variable">$&#123;tempfifo&#125;</span></span><br><span class="line"><span class="comment"># 向管道文件中写入空行，控制并发数</span></span><br><span class="line"><span class="keyword">for</span> ((i=1;i&lt;=<span class="variable">$&#123;thread_num&#125;</span>;i++)); <span class="keyword">do</span></span><br><span class="line">	    <span class="built_in">echo</span> &gt;&amp;10</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> `seq 1 100`</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">	<span class="comment"># 读取管道内容，并放到后台执行</span></span><br><span class="line">	<span class="built_in">read</span> -u10 </span><br><span class="line">	&#123; </span><br><span class="line">		<span class="built_in">echo</span> <span class="string">&quot;sleep 10 &quot;</span></span><br><span class="line">		sleep 10</span><br><span class="line">		<span class="comment"># 上面任务执行完后，在写入一行，避免阻塞任务</span></span><br><span class="line">		<span class="built_in">echo</span> &gt;&amp;10</span><br><span class="line">	&#125; &amp;</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="comment"># 等待上述子进程执行完，并关闭文件描述符读写</span></span><br><span class="line"><span class="built_in">wait</span></span><br><span class="line"><span class="built_in">exec</span> 10&gt;&amp;-</span><br><span class="line"><span class="built_in">exec</span> 10&lt;&amp;-</span><br></pre></td></tr></table></div></figure>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>shell终端间传递文件</title>
    <url>/posts/bd8af4de/</url>
    <content><![CDATA[
        <h3 id="背景"   >
          <a href="#背景" class="heading-link"><i class="fas fa-link"></i></a>背景</h3>
      <p>碰到有些系统的环境权限认证太复杂，往往套了好几层。现在有本地主机 A，远程主机 B。现在 A 要向 B 上传文件。</p>
<ol>
<li>主机 A 不能直接访问主机 B</li>
<li>只有主机 B 能访问远程服务器</li>
<li>主机 A 上有工具可以和主机 B 通信<a id="more"></a>

</li>
</ol>

        <h3 id="直接通过shell传递文件"   >
          <a href="#直接通过shell传递文件" class="heading-link"><i class="fas fa-link"></i></a>直接通过shell传递文件</h3>
      <p>主机 A 终端上执行，生成文件的 base64</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">tar -c file | gzip -9 | base64 &gt;&gt; file.txt</span><br></pre></td></tr></table></div></figure>
<p>主机 B 终端上执行，在 heredoc 中输入文件的 base64</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;<span class="string">&#x27;EOF&#x27;</span> | base64 --decode | tar -zx</span><br></pre></td></tr></table></div></figure>]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>transform - transformations chain</title>
    <url>/posts/3e030130/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>transform 是一个灵活的转换算子，接收一个自定义的函数作为参数来处理计算逻辑。它最大的功能是链接多个自定义的转换算子，简化代码，将相似的计算统一起来。本文会通过两个例子来介绍 transform 的功能。</p>
<a id="more"></a>


        <h3 id="transfrom"   >
          <a href="#transfrom" class="heading-link"><i class="fas fa-link"></i></a>transfrom</h3>
      
        <h4 id="均值计算填充数组中的空值"   >
          <a href="#均值计算填充数组中的空值" class="heading-link"><i class="fas fa-link"></i></a>均值计算填充数组中的空值</h4>
      <p>根据数组每个值前四项的均值来填充 -1 ，不足四项，则按照前面的长度来计算均值。在 Build-In Functions 里面抛出过这个问题。当然我们可以写 UDF 来处理数组。这里用 transform 的实现是这样的。我们只需要考虑均值计算的逻辑即可。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">transform</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">      .config(<span class="string">&quot;spark.ui.port&quot;</span>, <span class="string">&quot;14040&quot;</span>).getOrCreate()</span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> colNames = <span class="type">Seq</span>(<span class="string">&quot;vendor&quot;</span>, <span class="string">&quot;20190101&quot;</span>, <span class="string">&quot;20190102&quot;</span>, <span class="string">&quot;20190103&quot;</span>, <span class="string">&quot;20190104&quot;</span>,</span><br><span class="line">      <span class="string">&quot;20190105&quot;</span>, <span class="string">&quot;20190106&quot;</span>, <span class="string">&quot;20190107&quot;</span>, <span class="string">&quot;20190108&quot;</span>, <span class="string">&quot;20190109&quot;</span>)</span><br><span class="line">    <span class="keyword">var</span> ds = <span class="type">Seq</span>(</span><br><span class="line">      (<span class="string">&quot;20015545&quot;</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>),</span><br><span class="line">      (<span class="string">&quot;20015546&quot;</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">-1</span>),</span><br><span class="line">      (<span class="string">&quot;20015547&quot;</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">-1</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">-1</span>))</span><br><span class="line">      .toDF(colNames: _*)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> valColNames = colNames.drop(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">averageFunc</span></span>(colNames: <span class="type">Seq</span>[<span class="type">String</span>]) = &#123;</span><br><span class="line">      <span class="keyword">val</span> markCols = colNames.map(col(_))</span><br><span class="line">      markCols.foldLeft(lit(<span class="number">0</span>)) &#123; (x, y) =&gt; x + y &#125; / markCols.length</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">replaceCol</span></span>(colIdx: <span class="type">Int</span>, colNames: <span class="type">Seq</span>[<span class="type">String</span>])(df: <span class="type">DataFrame</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> colI = colNames(colIdx)</span><br><span class="line">      <span class="keyword">val</span> start = <span class="keyword">if</span> (colIdx &gt;= <span class="number">4</span>) colIdx - <span class="number">4</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">      <span class="keyword">val</span> cols = colNames.slice(start, colIdx)</span><br><span class="line">      println(cols)</span><br><span class="line">      <span class="keyword">val</span> checkVal = udf((v: <span class="type">Int</span>) =&gt; v != <span class="number">-1</span>)</span><br><span class="line">      <span class="keyword">if</span> (cols.length == <span class="number">0</span>) df <span class="keyword">else</span> df.withColumn(colI, when(checkVal(col(colI)), col(colI)).otherwise(averageFunc(cols)))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ds.show()</span><br><span class="line">    valColNames.indices.foreach(idx =&gt; &#123;</span><br><span class="line">      ds = ds.transform(replaceCol(idx, valColNames))</span><br><span class="line">      ds.show()</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>



        <h4 id="一列变多列"   >
          <a href="#一列变多列" class="heading-link"><i class="fas fa-link"></i></a>一列变多列</h4>
      <p>根据 Array[Array[String]] 生成多列。Array[String] 长度为二，以第一个值为列名，第二个值为列值。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cusExplodeArray</span></span>(columns: <span class="type">Seq</span>[<span class="type">String</span>])(df: <span class="type">DataFrame</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">     <span class="keyword">var</span> dfi = df</span><br><span class="line">     <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until columns.size) &#123;</span><br><span class="line">       <span class="keyword">if</span> (i == <span class="number">0</span>) &#123;</span><br><span class="line">         dfi = dfi.withColumn(columns(i), col(<span class="string">&quot;fill_revenue_list&quot;</span>)(i)(<span class="number">0</span>))</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">         dfi = dfi.withColumn(columns(i), col(<span class="string">&quot;fill_revenue_list&quot;</span>)(i))</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     dfi</span><br><span class="line">   &#125;</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>


        <h4 id="去除过多的-withColumn"   >
          <a href="#去除过多的-withColumn" class="heading-link"><i class="fas fa-link"></i></a>去除过多的 withColumn</h4>
      <p>withColumn 用来生成新列或者对现有列做一些改变。假设我们有一个数据集有上百个字段，其中很多字段要求 String -&gt; Int。我们肯定是不能写上百个 withColumn 的。这时候就可以通过 transform 来统一处理类似的计算处理逻辑。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transformInt</span></span>(columns: <span class="type">Seq</span>[<span class="type">String</span>])(df: <span class="type">DataFrame</span>) = &#123;</span><br><span class="line">      <span class="keyword">var</span> dfi = df</span><br><span class="line">      <span class="keyword">for</span> (column &lt;- columns) &#123;</span><br><span class="line">        dfi = dfi.withColumn(column, col(<span class="string">s&quot;<span class="subst">$column</span>&quot;</span>).cast(<span class="string">&quot;int&quot;</span>))</span><br><span class="line">      &#125;</span><br><span class="line">      dfi</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>vim常用的命令</title>
    <url>/posts/15118c2d/</url>
    <content><![CDATA[
        <h4 id="背景"   >
          <a href="#背景" class="heading-link"><i class="fas fa-link"></i></a>背景</h4>
      <p>脑子老是记不住东西，每次都反复查，遂分类记录下来</p>
<a id="more"></a>


        <h4 id="vim-替换"   >
          <a href="#vim-替换" class="heading-link"><i class="fas fa-link"></i></a>vim 替换</h4>
      <ul>
<li>全局替换<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">:%s/foo/bar/g</span><br></pre></td></tr></table></div></figure></li>
<li>当前行替换<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">:s/foo/bar/g</span><br></pre></td></tr></table></div></figure></li>
<li>指定行替换，比如下面是替换一行到四行<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">:1,4s/foo/bar/g</span><br></pre></td></tr></table></div></figure>

</li>
</ul>

        <h4 id="vim-查看文件编码"   >
          <a href="#vim-查看文件编码" class="heading-link"><i class="fas fa-link"></i></a>vim 查看文件编码</h4>
      <figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">: <span class="built_in">set</span> fileencoding</span><br></pre></td></tr></table></div></figure>
<p>如果你想改变当前文件的编码的话，可以直接设置 fileencoding 的属性，不过直接更改可能会造成乱码。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">: <span class="built_in">set</span> fileencoding=utf-8</span><br></pre></td></tr></table></div></figure>

        <h4 id="显示行号"   >
          <a href="#显示行号" class="heading-link"><i class="fas fa-link"></i></a>显示行号</h4>
      <figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">:<span class="built_in">set</span> number</span><br></pre></td></tr></table></div></figure>

        <h4 id="撤销更改"   >
          <a href="#撤销更改" class="heading-link"><i class="fas fa-link"></i></a>撤销更改</h4>
      <figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">u 或者 ctrl+R</span><br></pre></td></tr></table></div></figure>


        <h4 id="快速跳行"   >
          <a href="#快速跳行" class="heading-link"><i class="fas fa-link"></i></a>快速跳行</h4>
      <p>命令行模式下输入行号即可</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">: 17</span><br></pre></td></tr></table></div></figure>


        <h4 id="复制粘贴到系统剪切板"   >
          <a href="#复制粘贴到系统剪切板" class="heading-link"><i class="fas fa-link"></i></a>复制粘贴到系统剪切板</h4>
      <p>参考<span class="exturl"><a class="exturl__link"   href="https://www.zhihu.com/question/19863631/answer/89354508" >如何将 Vim 剪贴板里面的东西粘贴到 Vim 之外的地方？</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">+Y 复制当前行</span><br><span class="line">+nY 复制当前行往下 n 行</span><br></pre></td></tr></table></div></figure>

]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>wakatime记录你的工作</title>
    <url>/posts/a92e8bd0/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>偶然发现了别人的 github profile 多了一个 📊 Weekly development breakdown，用于展示各种语言工具的使用时长。感觉很有意思，遂研究了一下。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1gjk925gh1gj30pd02iq36.jpg"></p>
<a id="more"></a>


        <h3 id="wakatime"   >
          <a href="#wakatime" class="heading-link"><i class="fas fa-link"></i></a>wakatime</h3>
      <p>具体实现是有人已经写好的 waka-box plugin，github actions 定时更新 gist。具体的步骤可以见 <span class="exturl"><a class="exturl__link"   href="https://github.com/matchai/waka-box" >waka-box</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>，这里不再赘述，只说几个需要注意的地方。</p>
<ul>
<li>要想收集到 wakatime 的统计信息，需要先安装 wakatime 的 plugin，像常见的 Pycharm，IDEA，Sublime Text，Iterm，Chrome 都支持，具体的 plugin 安装见 <span class="exturl"><a class="exturl__link"   href="https://wakatime.com/help/editors" >wakatime install plugin</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 。</li>
<li>github actions 需要先手动触发一次 workflow，后续的才能正常执行。修改你 fork 的 waka-box 仓库 .github/workflows 目录下的 schedual.yml 文件，修改以下内容设置手动触发，提交更改后，可以在 actions 界面看到该 flow 的 Run WorkFlow 按钮，点击运行。<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">on:</span><br><span class="line">  workflow_dispatch:</span><br></pre></td></tr></table></div></figure></li>
<li>你可能会发现 actions 运行完之后，你的 gist 提示 cann`t find any file。这个不用担心，检查下第一步的 plugin 是否安装，静静等待凌晨更新（UTC 凌晨实际上是北京时间早上8点）。</li>
</ul>
<p>先上下我第一天的统计图<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1gjk9i8qthwj31bi0laadd.jpg"></p>
<p>都搞定之后，可以将统计信息放到你自己 pinned 中</p>

        <h3 id="同名仓库"   >
          <a href="#同名仓库" class="heading-link"><i class="fas fa-link"></i></a>同名仓库</h3>
      <p>github 新建一个以你的账户名称命名的仓库会触发彩蛋，意思是该项目的 readme 会显示在 profile 中。其实就是自定义你的 github profile。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1gjk9mmheenj31em0d4wh3.jpg"></p>
<p>这里参考了 <span class="exturl"><a class="exturl__link"   href="https://github.com/liuyib" >liuyib</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 的仓库，通过 github 的 api 展示 github 的统计信息。就像下面这样</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">![flyraty<span class="string">&#x27;s github stats](https://github-readme-stats.vercel.app/api?username=flyraty&amp;show_icons=true)</span></span><br></pre></td></tr></table></div></figure>

<p><img   src="https://github-readme-stats.vercel.app/api?username=flyraty&show_icons=true" style=""  alt="flyraty&#39;s github stats"></p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>wakatime</tag>
      </tags>
  </entry>
  <entry>
    <title>zero_copy</title>
    <url>/posts/4f67b333/</url>
    <content><![CDATA[
        <h3 id="基本概念"   >
          <a href="#基本概念" class="heading-link"><i class="fas fa-link"></i></a>基本概念</h3>
      
        <h4 id="用户态与内核态"   >
          <a href="#用户态与内核态" class="heading-link"><i class="fas fa-link"></i></a>用户态与内核态</h4>
      <p>机器的资源是固定的，应用程序进程却有很多，如果无节制的使用资源会导致系统崩溃。所以必须要对进程使用何种资源进行限制，由此权限的不同可以分为用户态和内核态。处于内核态中的程序可以说为所欲为。内核态像外管理硬件资源，像内管理操作系统中的进程，内存等资源。用户态和内核态的划分可以表示为</p>
<a id="more"></a>
<p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1gejzon86ylj30wm0fu74w.jpg"></p>

        <h4 id="系统调用"   >
          <a href="#系统调用" class="heading-link"><i class="fas fa-link"></i></a>系统调用</h4>
      <p>上面也提到了一个系统的资源是有限的，但是系统中却存在很多很多的进程，进程不可能无限制自由使用这些资源，所有会由操作系统来统一管理。应用程序进程要想使用资源，就必须通过操作系统提供的入口，这个入口及 System Call 系统调用。举个例子，copy 文件就可能会用到 sys_read 系统调用。系统调用是运行于内核态的。</p>

        <h4 id="库函数"   >
          <a href="#库函数" class="heading-link"><i class="fas fa-link"></i></a>库函数</h4>
      <p>系统调用本身过于底层，所以封装了一些逻辑作为上层的库函数调用</p>

        <h4 id="Shell"   >
          <a href="#Shell" class="heading-link"><i class="fas fa-link"></i></a>Shell</h4>
      <p>外壳，什么的外壳，内核的外壳，一种比较特殊的应用程序，通过 shell 我们可以直接和操作系统进行交互。</p>

        <h4 id="用户态到内核态是如何切换的"   >
          <a href="#用户态到内核态是如何切换的" class="heading-link"><i class="fas fa-link"></i></a>用户态到内核态是如何切换的</h4>
      <p>上面提到了进程请求资源需要系统调用，而系统调用运行在内核态，所以可以说系统调用使得我们由用户态切换到内核态。异常和外设中断也会使得我们切换到内核态。系统调用是一种软件中断。内核态完成后，相应进程会在切换回用户态。简单来说用户态切换到内核态的方式有3种</p>
<ul>
<li>System Call</li>
<li>Exception</li>
<li>Interrupt</li>
</ul>

        <h4 id="DMA"   >
          <a href="#DMA" class="heading-link"><i class="fas fa-link"></i></a>DMA</h4>
      <p>Direct Memory Access ，直接内存访问，一种常见的内存访问方式，允许部分硬件子系统直接读写系统内存，而不需要 CPU 的介入</p>

        <h4 id="用户进程缓冲区"   >
          <a href="#用户进程缓冲区" class="heading-link"><i class="fas fa-link"></i></a>用户进程缓冲区</h4>
      <p>用户进程要想访问系统资源，就要系统调用，切换到内核态，调用返回后还要切换回用户态，上下文切换耗费了大量的 CPU 资源，因此可以分配一个用户进程 buffer，就比如读取文件时，可以分配一个 buffer，每次 read 系统调用读到的数据都会放到 buffer 里面，供后面的程序使用，不用每次涉及到的 read 都调用。由此可见用户进程缓冲区的作用就是减少系统调用的次数，从而减少 CPU 消耗</p>

        <h4 id="内核缓冲区"   >
          <a href="#内核缓冲区" class="heading-link"><i class="fas fa-link"></i></a>内核缓冲区</h4>
      <p>当用户进程读取文件时，一般不会直接读取文件，而是先问一下内核缓冲区，我有的东西你有木有，如果有的话，直接复制，如果没有的话就把进程挂起，去处理其他的事情。等到数据准备完毕，在通知相应的进程</p>

        <h4 id="套接字缓冲区"   >
          <a href="#套接字缓冲区" class="heading-link"><i class="fas fa-link"></i></a>套接字缓冲区</h4>
      <p>数据在 write/send 的时候，不会立即发送数据，而是先放到套接字缓冲区里面。关闭套接字会丢失套接字输入缓冲区中的内容，输出缓冲区中的内容会继续发送<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1gejznqb24ej31830u0tdu.jpg"></p>

        <h3 id="一般的-远程文件-copy（从磁盘读取文件的时候是以DMA方式）"   >
          <a href="#一般的-远程文件-copy（从磁盘读取文件的时候是以DMA方式）" class="heading-link"><i class="fas fa-link"></i></a>一般的 远程文件 copy（从磁盘读取文件的时候是以DMA方式）</h3>
      <p>上图最左侧，共经历了4次copy，4次上下文切换，数据的每次 copy 都涉及到了 CPU，4次 CPU 拷贝</p>
<ul>
<li>用户从磁盘读取文件内容，触发 sys_read 调用，由用户态切换为内核态，经历了一次上下文切换。文件被copy 到内核缓冲区</li>
<li>read 调用返回 ，数据从内核缓冲区 copy 到 用户缓冲区，又一次上下文切换（内核态切换为用户态）</li>
<li>触发 sys_send 系统调用，数据从用户缓冲区 copy 到内核套接字缓冲区，用户态切换为内核态，一次上下文切换</li>
<li>send 调用返回，触发一次上下文切换，数据被copy 目标缓冲区</li>
</ul>
<p>中间是一种 “zero copy”，共经历了3次copy，2次上下文切换，1次 CPU 拷贝</p>
<ul>
<li>用户从磁盘读取文件内容，文件被 copy 到读取缓冲区，同时触发 sys_read ，sys_send 系统调用，2次上下文切换，数据直接被 copy 到 相应的套接字缓冲区</li>
<li>数据被copy 目标缓冲区</li>
</ul>

        <h3 id="zero-copy"   >
          <a href="#zero-copy" class="heading-link"><i class="fas fa-link"></i></a>zero copy</h3>
      <p>上图最右侧，数据只经历了1次copy，0次上下文切换也就是0次 CPU 拷贝</p>
<ul>
<li>用户读取文件，放到内核缓冲区，文件描述符（记录数据的位置信息等元数据）被发往套接字缓冲区，直接从本机内存读取数据发送到远程机器</li>
<li>零拷贝中的零是指涉及到的 CPU 拷贝，即用户态切换到内核态的上下文切换</li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>之前的一些学习目标和计划</title>
    <url>/posts/291450/</url>
    <content><![CDATA[
        <h4 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h4>
      <p>好久没有更新过博客了，去年接触了很多大数据技术相关的东西，也做过一些基准测试。但是很多东西依然了解的相对浅显，只停留在使用层面，想下笔却又不知道从何写起。2021 年结束的时候曾列过一些计划，也算是学习过程中的一份指南吧。</p>
<a id="more"></a>


        <h4 id="计划"   >
          <a href="#计划" class="heading-link"><i class="fas fa-link"></i></a>计划</h4>
      <ul>
<li><p>Hadoop</p>
<ul>
<li>yarn 官方文档，要搞明白 yarn ui 上的参数，比如容量调度这些<ul>
<li>yarn 架构</li>
<li>yarn 的容量调度策略，常见参数，作业帮采用的就是容量调度策略 <span class="exturl"><a class="exturl__link"   href="https://cloud.tencent.com/developer/article/1195056" >https://cloud.tencent.com/developer/article/1195056</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li>yarn 的公平调度策略 </li>
<li>RM 的重启和 HA</li>
<li>NM </li>
<li>分层次来看的话，就是 yarn 的整体架构（例如整个 job 的提交流程和通信） -&gt; 各个组件内部的子组件（例如 RM 中的 schedual 和 AM） -&gt; 子组件常见的一些策略（比如 failrecover, HA，调度策略，状态管理，资源模型）。这其中涉及到一些常见的配置，对于集群运维和平常的信息集群查看比较有用。另外可以拓展的来看，其和 flink 和 spark 等框架提交任务的过程都是异曲同工之妙。</li>
</ul>
</li>
<li>MR 官方文档，主要是执行计划</li>
<li>HDFS 存储</li>
</ul>
</li>
<li><p>数仓</p>
<ul>
<li>业界提供的只是一个思想，大部分需要结合内部业务场景做灵活调整，并且要结合技术选型，确定常用维度，schema 的设计很重要。大体上分为如下步骤<ul>
<li>数仓规划，数据域和业务过程的划分，数仓建设的基础。</li>
<li>数据标准，词根表，字段命名，字段业务含义和口径，强依赖于产品提供。</li>
<li>数据指标，指标含义及口径，强依赖于产品提供。</li>
<li>模型设计，需要着重考虑易用性，拓展性，规范性，定义分层数据流向标准，并且要考虑从表模型的设计及存储索引的简历来优化 SQL 的开发。</li>
<li>模型评审，上面每一步都录入建模平台以保证业务一致性及可查性，避免歧义。</li>
<li>资源预估，ETL 资源的合理预估，不限于存储，计算队列。</li>
<li>离线拓展到实时，避免重复消费原始 topic，通过分发服务分发计算好的指标供各个分析主题链路消费</li>
</ul>
</li>
</ul>
</li>
<li><p>Hive</p>
<ul>
<li>hive 参数优化、</li>
<li>主要是扫描小文件，join 的倾斜，怎么定位出问题的 sql 逻辑。</li>
<li>hive on spark &amp; hive on tez &amp; hive on mr</li>
<li>小文件主要存在三个方面的问题，存储压力（无法充分利用压缩），查询（扫描小文件，尤其是某些情况下的串行扫描，耗时非常长），元数据管理（增加元数据压力）</li>
</ul>
</li>
<li><p>Flink</p>
<ul>
<li><p>Flink 官方入门+进阶教程</p>
<ul>
<li>Flink overview</li>
<li>Flink 的一些基础概念，比如有界无界流，状态，window + watermark</li>
<li>Flink 部署运行，常见命令</li>
<li>Flink DataStream API</li>
<li>Flink 客户端操作，比如实验性的 sql client </li>
<li>Flink Table API，在作业帮主要用的是 Table API，并且开发平台屏蔽掉了大部分的底层逻辑</li>
<li>Flink SQL</li>
<li>Flink Runtime 运行机制，比如提交流程，streamGraph -&gt; jobGraph -&gt; ExecutionGraph 的转换，语法树的解析，DAG 的生成，任务的重启策略</li>
<li>Flink Time 的概念和机制，以及要解决的问题</li>
<li>以分层的角度来看 Flink 任务的提交过程，目前有个大致了解，但是说不上来，需要总结下</li>
<li>Flink 执行作业解析</li>
<li>Flink 网络流控和反压</li>
<li>Flink metrics</li>
<li>Flink Connector</li>
<li>Flink State</li>
<li>Flink Table SQL </li>
</ul>
</li>
<li><p>Flink 的优化</p>
<ul>
<li>重点在于前期良好的资源评估，主要结合业务高峰期 qps</li>
<li>因为本身资源足够，碰到的纯性能问题不多，优化点主要是分区不均匀的 topic，空跑的 slot 造成的资源利用率过低。以及释放掉高峰期数据处理空闲的内存。</li>
</ul>
</li>
<li><p>Flink 官方文档阅读笔记</p>
</li>
</ul>
</li>
<li><p>Kafka</p>
<ul>
<li>Kafka 视频教程</li>
</ul>
</li>
<li><p>Spark</p>
<ul>
<li>spark catalyst 优化器源码</li>
</ul>
</li>
<li><p>Doris</p>
<ul>
<li>Dorisdb 执行计划查看，DAG 查看</li>
<li>使用 dorisdb 过程总遇到的问题，大部分性能问题是通过重建 schema 来解决的，让存储更均匀，并且尽量命中前缀索引。<ul>
<li>dorisdb 对 value 列的索引支持不太好</li>
</ul>
</li>
<li>doris 的实时摄入性能测试及优化</li>
<li>doris 的离线摄入性能测试及优化</li>
</ul>
</li>
<li><p>Druid</p>
<ul>
<li>官方文档</li>
<li>常见优化<ul>
<li>摄入任务优化，hadoop based &amp;&amp; kafka based，任务性能和最终生成的 segment 大小，分片的的折衷。</li>
<li>segment 存储优化</li>
</ul>
</li>
<li>Druid 和 Doris 的不同地方，哪些场景下用 druid 比较好<ul>
<li>druid 全列索引。dorisdb 只有 key 列上的前缀索引，bitmap 索引，大数据量下，低基维的检索可能不如 druid 好。</li>
<li>druid 无法存储明细数据，但是实施摄入相对简单，本身集成支持足够。doris 可以存储明细数据，但是实时摄入的话需要自己开发代码，受限于基建服务的问题，整个链路比较长。</li>
</ul>
</li>
<li>Druid 离线摄入过程及优化</li>
<li>Druid 实时摄入过程及优化</li>
<li>Druid 集群运维，metrics 收集</li>
<li>Druid 物化视图</li>
</ul>
</li>
</ul>
<ul>
<li><p>ElstaicSearch</p>
<ul>
<li>官方文档书籍 <code>ElstaicSearch 权威指南</code></li>
<li>基本原理</li>
<li>深入的不多，只知道设置字段类型的时候需要考虑是否用到倒排索引，尤其是数字类型的低基维</li>
</ul>
</li>
<li><p>常见数据结构</p>
<ul>
<li>bitmap</li>
<li>倒排索引</li>
<li>HLL</li>
<li>前缀索引</li>
<li>LSM</li>
<li>稀疏索引</li>
</ul>
</li>
<li><p>metrics 监控</p>
<ul>
<li>常见监控指标</li>
<li>kafka metrics </li>
<li>spark metrics</li>
<li>flink metrics</li>
<li>druid metrics</li>
<li>对于部分计算系统，如何集成预期的自定义指标，比如处理错误量等</li>
<li>grafana + prometheus 简单配置</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title>关于数开的一些问题</title>
    <url>/posts/d6dc8cdf/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>emnn，先把问题罗列一下，周末梳理梳理。借用了很多文章，侵删</p>
<a id="more"></a>


        <h4 id="问题"   >
          <a href="#问题" class="heading-link"><i class="fas fa-link"></i></a>问题</h4>
      <ul>
<li><p>Spark sortmergeshuffle 和 hashshuffle 的实现和区别，bypass 机制是什么？可以看下这篇文章 <span class="exturl"><a class="exturl__link"   href="https://www.jianshu.com/p/6f55b8412f03" >Spark–Spark Shuffle发展史及调优方法</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
</li>
<li><p>Scala 的高阶函数 map reduce fold，这些是怎么实现的呢？<br>像 fold 函数，一般都是赋一个初始值，然后交给操作函数，操作函数的值作为下一次的初始值。这其实也是一种思想，告诉编译器做什么，而不是告诉编译器怎么做。</p>
</li>
<li><p>java 多线程 <span class="exturl"><a class="exturl__link"   href="https://www.liaoxuefeng.com/wiki/1252599548343744/1255943750561472" >廖雪峰Java教程</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
</li>
<li><p>生产者-消费者模型，互斥锁和信号量<br><span class="exturl"><a class="exturl__link"   href="https://github.com/CyC2018/CS-Notes/blob/master/notes/%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%20-%20%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86.md" >生产者-消费者模型</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
</li>
<li><p>JVM 模块 和 内存模型<br><img src="https://pic4.zhimg.com/80/v2-d345bf5412825d79cc342a0f0e274677_1440w.jpg"></p>
</li>
<li><p>Flink watermark trigger，trigger 是怎么实现的？迟到数据是怎么处理的？watermark 到底是做什么的<br>watermark 更像是为了控制状态无限制的增长，会告诉框架什么时间结果不会在变化。关于流处理的一些概念，曾经读过 DataFlowModal（ps：忘光了。。）。这里推荐一篇文章 <span class="exturl"><a class="exturl__link"   href="https://zhuanlan.zhihu.com/p/61355244" >由Dataflow模型聊Flink和Spark</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
</li>
<li><p>Spark StructStreaming<br>推荐多度几遍官方文档，Spark 3.x 的重点好像在 ML，contionus processing 发展的比较慢。</p>
</li>
<li><p>该如何理解 Flink<br><span class="exturl"><a class="exturl__link"   href="https://zhuanlan.zhihu.com/p/61355244" >由Dataflow模型聊Flink和Spark</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
</li>
<li><p>Flink 架构中重要的模块<br>官方文档<span class="exturl"><a class="exturl__link"   href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/concepts/runtime.html" >Distributed Runtime Environment</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
</li>
<li><p>列裁剪，谓词下推，常量累加<br><span class="exturl"><a class="exturl__link"   href="https://www.iteblog.com/archives/2562.html" >一条 SQL 在 Apache Spark 之旅（中）</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
</li>
<li><p>漏斗怎么实现<br>join 或者 UDAF</p>
</li>
<li><p>RDD 的特性，五个属性是什么<br>只读，并行，分布式，粗粒度，可以直接查看源码，<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd46hfy7xgj313w0nugsi.jpg"></p>
</li>
</ul>
<ul>
<li><p>RDD 之间的宽窄依赖是什么？有什么区别<br>窄依赖，即子RDD依赖于父RDD中固定的Partition。NarrowDependency 分为 OneToOneDependency 和 RangeDependency两种。<br>宽依赖，shuffle 依赖，即子 RDD 对父 RDD 中的所有 Partition 都有依赖。</p>
</li>
<li><p>unresolverelation plan 和 logiacl plan 的区别，unresolverelation plan 是由谁解析的，怎么解析的？<br><span class="exturl"><a class="exturl__link"   href="https://www.iteblog.com/archives/2562.html" >一条 SQL 在 Apache Spark 之旅（中）</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>，这几篇文章详细讲解了 DSL 或者 SQL 在 Spark 中的历程，不得不感叹框架为我们做了太多太多的事情，当然明白执行过程，也方便我们查看 DAG，读懂各种执行计划。</p>
</li>
<li><p>spark catalyst 语法分析优化，spark catayast 对语法分析树是通过什么规则来优化的<br><span class="exturl"><a class="exturl__link"   href="https://www.cnblogs.com/shishanyuan/p/8455786.html" >深入研究Spark SQL的Catalyst优化器（原创翻译）</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
</li>
<li><p>CBO RBO 的具体实现，有哪种规则<br><span class="exturl"><a class="exturl__link"   href="http://www.jasongj.com/spark/rbo/" >Spark SQL / Catalyst 内部原理 与 RBO</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://blog.csdn.net/Habren/article/details/82847908" >https://blog.csdn.net/Habren/article/details/82847908</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
</li>
<li><p>Spark 作业的提交过程，Spark SQL 在 spark 内部是怎么执行的？<br><span class="exturl"><a class="exturl__link"   href="https://www.iteblog.com/archives/2562.html" >一条 SQL 在 Apache Spark 之旅（中）</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
</li>
<li><p>WholeStageCodeGen 是用来做什么的<br>生成类似手写效果代码，为了提高运行效率。</p>
</li>
<li><p>spark on hive<br><span class="exturl"><a class="exturl__link"   href="https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html" >Hive Tables</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
</li>
<li><p>Spark 统一内存模型</p>
<ul>
<li>spark.memory<ul>
<li>executor memory</li>
<li>storage memory</li>
</ul>
</li>
<li>user.mermory</li>
<li>reverse memory<br>spark.memory 与 user.memory 默认比例为 0.6 0.4。executor memory与storage memory 属于动态分配。但是观察web UI 发现频繁 GC 的话，还是要调高 executor memory的占用比例。reverse memory 300M。</li>
</ul>
</li>
<li><p>虚拟内存与磁盘缓存<br><span class="exturl"><a class="exturl__link"   href="https://flyraty.github.io/2020/05/07/%E7%A3%81%E7%9B%98%E4%B8%8E%E5%86%85%E5%AD%98/#more" >磁盘与内存</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
</li>
<li><p>对数据仓库的理解<br>数据仓库与传统数仓到底有啥区别？数据中台的概念。</p>
</li>
<li><p>用户态内核态，什么情况会造成用户态和内核态的切换，有哪些异常<br><span class="exturl"><a class="exturl__link"   href="https://flyraty.github.io/2020/05/07/zero-copy/" >zero_copy</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
</li>
<li><p>sql join 中 on 的作用是什么，不加 on 和加 on 有什么区别<br>指定关联条件</p>
</li>
<li><p>Scala 的不可变性与函数组合子 ，不可变性的优势<br>emnn，这个原先看过，趁着最近在读 Scala 实用指南，在熟悉一遍</p>
</li>
</ul>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>mac os 升级到 big sur的坑</title>
    <url>/posts/9894dc13/</url>
    <content><![CDATA[
        <h4 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h4>
      <p>升级到 big sur 后，一些系统命令及软件包找不到了（比如 git，python3..）。</p>
<a id="more"></a>


        <h4 id="解决方式"   >
          <a href="#解决方式" class="heading-link"><i class="fas fa-link"></i></a>解决方式</h4>
      <p>升级完成之后，使用 git 报以下错，还有其他一些软件包也是报的类似的错。</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">sh: line 1: 97132 Abort trap: 6 &#x2F;Applications&#x2F;Xcode.app&#x2F;Contents&#x2F;Developer&#x2F;usr&#x2F;bin&#x2F;xcodebuild -sdk &#x2F; -find git 2&gt; &#x2F;dev&#x2F;null</span><br><span class="line">git: error: unable to find utility &quot;git&quot;, not a developer tool or in PATH</span><br></pre></td></tr></table></div></figure>
<p>找不到 git ？？？而且报的是 xcode 的错，难道是升级把 xcode comandline tools 搞没了。但是看了下 brew，svn 啥的都没啥问题。于是重装 git</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">brew uninstall git</span><br><span class="line">brew search git</span><br><span class="line">brew install git</span><br></pre></td></tr></table></div></figure>
<p>emmm，重装之后也不管用，看报错是 git 没有在环境变量里，这东西不是 xcode omandline tools 自带的吗? 于是试着加了下环境变量</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">PATH&#x3D;&#x2F;Applications&#x2F;Xcode.app&#x2F;Contents&#x2F;Developer&#x2F;usr&#x2F;bin:$PATH</span><br></pre></td></tr></table></div></figure>
<p>加上之后，git 倒是好了。接下来修 python3。也用的重装大法，最后提示 xcode 版本过低。于是又重装 xcode 。</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">xcode-select --install</span><br></pre></td></tr></table></div></figure>
<p>开启 python3，可以正常没问题，本以为解决好了，由于重装了 python，很多三方库都没了，开始拿 pip3 重装，结果一堆库安装失败，还是上面的报错，只不过 git 变成了什么 gcc，clang。严重怀疑是什么地方环境变量指错了。看下 xcode commandline tools 的指向。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">xcode-select --<span class="built_in">print</span>-path </span><br></pre></td></tr></table></div></figure>
<p>指向的是 Developer 文件夹，其实应该指向 CommandLineTools。遂修改</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">sudo xcode-select --switch /Library/Developer/CommandLineTool</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>
<p>改完之后，重启了 itrem2 啥的，再各种安装就没有问题了。</p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title>回溯之矩阵中是否存在某个字符串</title>
    <url>/posts/5ae016f7/</url>
    <content><![CDATA[
        <h3 id="题目"   >
          <a href="#题目" class="heading-link"><i class="fas fa-link"></i></a>题目</h3>
      <p>存在类似以下的矩阵，判断矩阵中是否存在某个字符串，比如存在 adeh，不存在 adhk</p>
<a id="more"></a>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;</span><br><span class="line"> &#39;d&#39;, &#39;e&#39;, &#39;f&#39;</span><br><span class="line"> &#39;g&#39;, &#39;h&#39;, &#39;i&#39; </span><br><span class="line"> &#39;j&#39;, &#39;k&#39;, &#39;l&#39;</span><br><span class="line">]</span><br></pre></td></tr></table></div></figure>

        <h3 id="题解"   >
          <a href="#题解" class="heading-link"><i class="fas fa-link"></i></a>题解</h3>
      <p>穷举所有路径，找到就返回真，否则为假。矩阵中每个点都假设有上下左右四个选择，从起点开始遍历路径，遇到正确的就接着往下走，不正确的就回退到上一步。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">solution</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">val</span> notions = <span class="type">Seq</span>(<span class="type">Seq</span>(<span class="number">0</span>, <span class="number">1</span>), <span class="type">Seq</span>(<span class="number">0</span>, <span class="number">-1</span>), <span class="type">Seq</span>(<span class="number">1</span>, <span class="number">0</span>), <span class="type">Seq</span>(<span class="number">-1</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">adjust</span></span>(array:<span class="type">Array</span>[<span class="type">Array</span>[<span class="type">String</span>]], str:<span class="type">String</span>) = &#123;</span><br><span class="line">		<span class="keyword">val</span> row = array.length</span><br><span class="line">		<span class="keyword">val</span> col = array(<span class="number">0</span>).length</span><br><span class="line">		<span class="keyword">val</span> marked = <span class="type">Array</span>.ofDim[<span class="type">Boolean</span>](row, col)</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (row == <span class="number">0</span> || col == <span class="number">0</span> )&#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="function"><span class="keyword">def</span> <span class="title">backtracking</span></span>(marked:<span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Boolean</span>]], strLen:<span class="type">Int</span>, r:<span class="type">Int</span>, c:<span class="type">Int</span>) :<span class="type">Boolean</span>= &#123;</span><br><span class="line">				<span class="keyword">val</span> str_ = str(strLen)</span><br><span class="line">				<span class="keyword">if</span> (r&lt;<span class="number">0</span> || c&lt;<span class="number">0</span> || c&gt;col || r&gt;row || marked(r)(c)==<span class="literal">false</span> || array(r)(c) != str_)&#123;</span><br><span class="line">					<span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				marked(r)(c) = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">				<span class="keyword">for</span>(n &lt;- notions) &#123;</span><br><span class="line">					<span class="keyword">if</span> (backTracking(marked, sLen+<span class="number">1</span>, r+n.head, c+n(<span class="number">1</span>))) &#123;</span><br><span class="line">            			<span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">          			&#125;</span><br><span class="line">				&#125;</span><br><span class="line">				marked(r)(c) = <span class="literal">false</span></span><br><span class="line">				<span class="literal">false</span></span><br><span class="line"></span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">for</span> (i &lt;- <span class="number">0</span> until row) &#123;</span><br><span class="line">		        <span class="keyword">for</span> (j &lt;- <span class="number">0</span> until col)&#123;</span><br><span class="line">		          <span class="keyword">if</span> (backTracking(marked, <span class="number">0</span>, i, j))&#123;</span><br><span class="line">		            <span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">          			&#125;</span><br><span class="line">        		&#125;</span><br><span class="line"></span><br><span class="line">      		&#125;</span><br><span class="line">      		<span class="literal">false</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>多个 git 账户配置 SSH</title>
    <url>/posts/9e62987f/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>个人代码维护在 github，而目前大多数公司代码维护在私有 gitlab。这是两套不同的账户体系，并且一般私有 gitlab 的 commit email 不能更改，git 全局的用户名和邮箱只能有一个。这就导致了如下问题 → 不管配置了几个 SSH pub key，SSH 认证最终走的都是 global 的用户名的认证（比如你全局的用户是 github 的，那么你提交 gitlab 就会报 Permission Denied）。本文主要用来解决此问题。<br>其实多个 SSH 配置的话都是这样搞的，配个路由就好了。</p>
<a id="more"></a>


        <h3 id="配置-SSH"   >
          <a href="#配置-SSH" class="heading-link"><i class="fas fa-link"></i></a>配置 SSH</h3>
      
        <h4 id="github"   >
          <a href="#github" class="heading-link"><i class="fas fa-link"></i></a>github</h4>
      <p>生成 ssh pub key，并将其添加到 github settings 的 SSH 配置中。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">&#x27;$&#123;your github emial&#125;&#x27;</span> -f github</span><br></pre></td></tr></table></div></figure>


        <h4 id="gitlab"   >
          <a href="#gitlab" class="heading-link"><i class="fas fa-link"></i></a>gitlab</h4>
      <p>生成 ssh pub key，并将其添加到 gitlab settings 的 SSH 配置中。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">&#x27;$&#123;your gitlab emial&#125;&#x27;</span> -f gitlab</span><br></pre></td></tr></table></div></figure>


        <h4 id="SSH-config"   >
          <a href="#SSH-config" class="heading-link"><i class="fas fa-link"></i></a>SSH config</h4>
      <p>修改 ssh 配置文件，用来配置不同 host 下用的 SSH 验证文件是哪个。可以理解为路由，发的请求都会查阅此文件来确定验证的 key 是哪个。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">Host gitlab</span><br><span class="line">  HostName git.zuoyebang.cc</span><br><span class="line">  User zhanghailiang01</span><br><span class="line">  IdentityFile ~/.ssh/gitlab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Host github.com</span><br><span class="line">  HostName github.com</span><br><span class="line">  User xxx</span><br><span class="line">  IdentityFile ~/.ssh/github</span><br></pre></td></tr></table></div></figure>


        <h4 id="验证"   >
          <a href="#验证" class="heading-link"><i class="fas fa-link"></i></a>验证</h4>
      <p>验证配置好的 SSH 是否可用</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">$ ssh -T git@gitlab</span><br><span class="line">Welcome to GitLab, @xxx!</span><br><span class="line"></span><br><span class="line">$ ssh -T git@github.com</span><br><span class="line">Hi xxx! You<span class="string">&#x27;ve successfully authenticated, but GitHub does not provide shell access.</span></span><br></pre></td></tr></table></div></figure>

        <h4 id="使用"   >
          <a href="#使用" class="heading-link"><i class="fas fa-link"></i></a>使用</h4>
      <p>经过上步验证通过后，免密提交 gitlab 还有一定的问题。我们还需要修改对应 gitlab 项目的用户名和邮箱，因为我们提交代码的账户认证是这个，而不是 git 全局的用户。修改完成之后，可以查看对应项目下的 .git/config 文件是否修改成功。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">git config --<span class="built_in">local</span> user.name <span class="string">&#x27;xxx&#x27;</span> &amp;&amp; git config --<span class="built_in">local</span> user.email <span class="string">&#x27;xxx@xxx.com&#x27;</span></span><br></pre></td></tr></table></div></figure>
<p>对于以后新增的 gitlab 项目，都要执行此步。</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>实时消费 MySQL Binlog</title>
    <url>/posts/d5c98b55/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>最近工作中用到的，以前没有搞过 binlog，遂在本地完整的跑遍 demo 看看。整体数据流如下，Canal 接收 MySQL Binlog 到 Kafka。Spark Streaming 消费数据写到 ES。</p>
<a id="more"></a>

        <h3 id="实时消费binlog"   >
          <a href="#实时消费binlog" class="heading-link"><i class="fas fa-link"></i></a>实时消费binlog</h3>
      
        <h4 id="环境准备"   >
          <a href="#环境准备" class="heading-link"><i class="fas fa-link"></i></a>环境准备</h4>
      <p>在 Mac 上安装开源软件包都比较方便，brew 可安一切，用好 search，info，tap，insatall 几个命令就行了。安装 MySQL，Kibana，ElasticSearch。<br>Kafka 安装请参考 <a href="https://timemachine.icu/posts/5f8acd6f/">Apache Kafka实战-搭建zookeeper与kafka集群</a>。</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">brew install mysql@5.7</span><br><span class="line">brew install kafka</span><br><span class="line">brew install elasticsearch</span><br><span class="line">brew install kibana</span><br></pre></td></tr></table></div></figure>
<p>用 brew 安装还有一个好处，就是可以依赖 brew 来管理启停服务。</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">brew services start mysql</span><br><span class="line">brew services start elasticsearch</span><br><span class="line">brew services start kibana</span><br></pre></td></tr></table></div></figure>
<p>现在本地访问 <code>localhost:5601 和 localhost:9200</code> 就可以看到 Kibana 和 ES 的返回了。 </p>

        <h4 id="开启-MySQL-Binlog"   >
          <a href="#开启-MySQL-Binlog" class="heading-link"><i class="fas fa-link"></i></a>开启 MySQL Binlog</h4>
      <p>Binlog 是 MySQL sever 层维护的一种二进制日志，主要用来记录 MySQL 产生更新时的行为（即产生变化的 SQL 语句）到磁盘。主要用来数据恢复，主从数据同步，数据备份。<br>开启 MySQL Binlog 只需要几步</p>
<ul>
<li>修改 /etc/my.cnf<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="built_in">log</span>-bin=mysql-bin <span class="comment">#开启binlog</span></span><br><span class="line">binlog-format=ROW <span class="comment">#选择row模式</span></span><br></pre></td></tr></table></div></figure></li>
<li>重启 MySQL <figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">brew service restart mysql</span><br></pre></td></tr></table></div></figure></li>
<li>查看是否开启 Binlog。ON 代表开启，此时也可以看到 Binlog 在磁盘上的位置。<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">mysql&gt; show variables like <span class="string">&#x27;%log_bin%&#x27;</span>; </span><br><span class="line">+---------------------------------+---------------------------------------+</span><br><span class="line">| Variable_name                   | Value                                 |</span><br><span class="line">+---------------------------------+---------------------------------------+</span><br><span class="line">| log_bin                         | ON                                    |</span><br><span class="line">| log_bin_basename                | /usr/<span class="built_in">local</span>/mysql/data/mysql-bin       |</span><br><span class="line">| log_bin_index                   | /usr/<span class="built_in">local</span>/mysql/data/mysql-bin.index |</span><br><span class="line">| log_bin_trust_function_creators | OFF                                   |</span><br><span class="line">| log_bin_use_v1_row_events       | OFF                                   |</span><br><span class="line">| sql_log_bin                     | ON                                    |</span><br><span class="line">+---------------------------------+---------------------------------------+</span><br><span class="line">6 rows <span class="keyword">in</span> <span class="built_in">set</span> (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; show binary logs;</span><br><span class="line">+------------------+-----------+</span><br><span class="line">| Log_name         | File_size |</span><br><span class="line">+------------------+-----------+</span><br><span class="line">| mysql-bin.000001 |      4070 |</span><br><span class="line">| mysql-bin.000002 |       154 |</span><br><span class="line">+------------------+-----------+</span><br><span class="line">2 rows <span class="keyword">in</span> <span class="built_in">set</span> (0.01 sec)</span><br></pre></td></tr></table></div></figure>

</li>
</ul>

        <h4 id="Canal-配置"   >
          <a href="#Canal-配置" class="heading-link"><i class="fas fa-link"></i></a>Canal 配置</h4>
      <p>由于 Binlog 是二进制文件，不能直接查看，需要使用 MySQL 自带的查看工具 <code>/bin/mysql/binlog</code>。不过这样查看仍然不是很方便，对我们使用订阅 Binlog帮助不大。这个时候就需要 Canal 了。Canal 是阿里开源的基于数据库 Binlog 的增量消费/订阅组件，其原理是伪装成 MySQL Slaver，这样 Master 就会通过某些协议将 binlog 推送给我 Canal。Canal 做了一些解析工作，将 Binlog 转换为 JSON 格式便于后续处理。</p>
<ul>
<li><p>下载 Canal</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">wget https://github.com/alibaba/canal/releases/download/canal-1.1.3/canal.deployer-1.1.3.tar.gz</span><br><span class="line">tar -zxvf canal.deployer-1.1.3.tar.gz</span><br></pre></td></tr></table></div></figure></li>
<li><p>配置 CANAL_HOME</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> CANAL_HOME=/Users/zaoshu/canal</span><br><span class="line">soure ~/.bash_profile</span><br></pre></td></tr></table></div></figure></li>
<li><p>MySQL 中创建 Canal 的用户</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">mysql&gt;  CREATE USER canal IDENTIFIED BY <span class="string">&#x27;canal&#x27;</span>;  </span><br><span class="line">mysql&gt;  GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO <span class="string">&#x27;canal&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line">mysql&gt;  GRANT ALL PRIVILEGES ON *.* TO <span class="string">&#x27;canal&#x27;</span>@<span class="string">&#x27;%&#x27;</span> ;</span><br><span class="line">mysql&gt;  FLUSH PRIVILEGES;</span><br></pre></td></tr></table></div></figure>
</li>
<li><p>修改 $CANAL_HOME/conf/example/instance.properties</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mysql slave id</span></span><br><span class="line">canal.instance.mysql.slaveId=3</span><br><span class="line"><span class="comment"># mysql 地址</span></span><br><span class="line">canal.instance.master.address=127.0.0.1:3306</span><br><span class="line"><span class="comment"># 上步配置的 mysql canal 用户</span></span><br><span class="line">canal.instance.dbUsername=canal</span><br><span class="line">canal.instance.dbPassword=canal</span><br><span class="line"><span class="comment"># 监控所有数据库中的所有表</span></span><br><span class="line">canal.instance.filter.regex=.*\\..*</span><br><span class="line"><span class="comment"># binlog 对接的 kafka topic</span></span><br><span class="line">canal.mq.topic=ms_binlog</span><br></pre></td></tr></table></div></figure></li>
<li><p>修改 canal.properties</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Kafka broker 信息</span></span><br><span class="line">canal.mq.servers = localhost:9092,localhost:9093,localhost:9094</span><br><span class="line"><span class="comment"># json 格式</span></span><br><span class="line">canal.mq.flatMessage = <span class="literal">true</span></span><br></pre></td></tr></table></div></figure></li>
<li><p>启动 Canal</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="variable">$CANAL_HOME</span>/bin/startup.sh</span><br></pre></td></tr></table></div></figure></li>
<li><p>Kafka consumer 消费数据验证 Canal 是否启用成功</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">kafka-console-consumer --bootstrap-server localhost:9092 --topic ms_binlog --from-beginning</span><br><span class="line"></span><br><span class="line"><span class="comment"># Binlog 示例</span></span><br><span class="line">&#123;<span class="string">&quot;data&quot;</span>:[&#123;<span class="string">&quot;rec_id&quot;</span>:<span class="string">&quot;4&quot;</span>,<span class="string">&quot;url_name&quot;</span>:<span class="string">&quot;nanshanweila&quot;</span>,<span class="string">&quot;time&quot;</span>:<span class="string">&quot;2017-12-31&quot;</span>,<span class="string">&quot;jiaofang_info1&quot;</span>:<span class="string">&quot;交房时间：2018年年底交付洋房3#、11#等&quot;</span>,<span class="string">&quot;jiaofang_info2&quot;</span>:<span class="string">&quot;交房楼栋：10#，11#，12#，15#，16#，2#，21#，22#，23#，27#，28#，3#，4#，5#&quot;</span>,<span class="string">&quot;jiaofang_info3&quot;</span>:<span class="string">&quot;交房详情：2017年年底交付洋房3#、11#、12#、15#、16#、4#、5#等。&quot;</span>,<span class="string">&quot;kaipan_info1&quot;</span>:null,<span class="string">&quot;kaipan_info2&quot;</span>:null,<span class="string">&quot;kaipan_info3&quot;</span>:null,<span class="string">&quot;created_time&quot;</span>:<span class="string">&quot;2019-05-27 12:50:50&quot;</span>,<span class="string">&quot;created_time_ts&quot;</span>:<span class="string">&quot;1558918250187&quot;</span>&#125;],<span class="string">&quot;database&quot;</span>:<span class="string">&quot;fangtianxia&quot;</span>,<span class="string">&quot;es&quot;</span>:1610378360000,<span class="string">&quot;id&quot;</span>:3,<span class="string">&quot;isDdl&quot;</span>:<span class="literal">false</span>,<span class="string">&quot;mysqlType&quot;</span>:&#123;<span class="string">&quot;rec_id&quot;</span>:<span class="string">&quot;int(11)&quot;</span>,<span class="string">&quot;url_name&quot;</span>:<span class="string">&quot;varchar(50)&quot;</span>,<span class="string">&quot;time&quot;</span>:<span class="string">&quot;varchar(50)&quot;</span>,<span class="string">&quot;jiaofang_info1&quot;</span>:<span class="string">&quot;text&quot;</span>,<span class="string">&quot;jiaofang_info2&quot;</span>:<span class="string">&quot;text&quot;</span>,<span class="string">&quot;jiaofang_info3&quot;</span>:<span class="string">&quot;text&quot;</span>,<span class="string">&quot;kaipan_info1&quot;</span>:<span class="string">&quot;text&quot;</span>,<span class="string">&quot;kaipan_info2&quot;</span>:<span class="string">&quot;text&quot;</span>,<span class="string">&quot;kaipan_info3&quot;</span>:<span class="string">&quot;text&quot;</span>,<span class="string">&quot;created_time&quot;</span>:<span class="string">&quot;datetime&quot;</span>,<span class="string">&quot;created_time_ts&quot;</span>:<span class="string">&quot;bigint(20)&quot;</span>&#125;,<span class="string">&quot;old&quot;</span>:[&#123;<span class="string">&quot;jiaofang_info1&quot;</span>:<span class="string">&quot;交房时间：2017年年底交付洋房3#、11#等&quot;</span>&#125;],<span class="string">&quot;pkNames&quot;</span>:[<span class="string">&quot;rec_id&quot;</span>],<span class="string">&quot;sql&quot;</span>:<span class="string">&quot;&quot;</span>,<span class="string">&quot;sqlType&quot;</span>:&#123;<span class="string">&quot;rec_id&quot;</span>:4,<span class="string">&quot;url_name&quot;</span>:12,<span class="string">&quot;time&quot;</span>:12,<span class="string">&quot;jiaofang_info1&quot;</span>:2005,<span class="string">&quot;jiaofang_info2&quot;</span>:2005,<span class="string">&quot;jiaofang_info3&quot;</span>:2005,<span class="string">&quot;kaipan_info1&quot;</span>:-4,<span class="string">&quot;kaipan_info2&quot;</span>:-4,<span class="string">&quot;kaipan_info3&quot;</span>:-4,<span class="string">&quot;created_time&quot;</span>:93,<span class="string">&quot;created_time_ts&quot;</span>:-5&#125;,<span class="string">&quot;table&quot;</span>:<span class="string">&quot;newfangwork&quot;</span>,<span class="string">&quot;ts&quot;</span>:1610378360683,<span class="string">&quot;type&quot;</span>:<span class="string">&quot;UPDATE&quot;</span>&#125;</span><br></pre></td></tr></table></div></figure>

</li>
</ul>

        <h4 id="Spark-Streaming-消费-Kafka"   >
          <a href="#Spark-Streaming-消费-Kafka" class="heading-link"><i class="fas fa-link"></i></a>Spark Streaming 消费 Kafka</h4>
      <p>关于消费 Kakfa 可以直接参考官方文档上的代码 <span class="exturl"><a class="exturl__link"   href="https://spark.apache.org/docs/2.2.0/streaming-kafka-0-10-integration.html" >Spark Streaming + Kafka Integration Guide </a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerRecord</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">InputDStream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Milliseconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.<span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.<span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Kafka2ESDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">&quot;Kafka2ESDemo&quot;</span>).master(<span class="string">&quot;local[*]&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> checkpointDir = <span class="string">&quot;./checkpoint&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Milliseconds</span>(<span class="number">100000</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="string">&quot;localhost:9092&quot;</span>,</span><br><span class="line">      <span class="string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">&quot;group.id&quot;</span> -&gt; <span class="string">&quot;test&quot;</span>,</span><br><span class="line">      <span class="string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="string">&quot;earliest&quot;</span>,</span><br><span class="line">      <span class="string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Array</span>(<span class="string">&quot;ms_binlog&quot;</span>, <span class="string">&quot;test_topic&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> resultDStream = stream.map(x =&gt; x.value())</span><br><span class="line">    resultDStream.print()</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>































]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>工作中实践过的数据流架构</title>
    <url>/posts/6796f734/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>在作业帮工作过程中，接触了 leads 线索数据、端外信息流广告，端内营销广告、用户画像平台等各种数据的处理。每种数据结合其业务查询场景又有不同的架构选型。本文主要用来记录这几份业务数据的数据流架构，并讨论为什么这样实现，有哪些优缺点。</p>
<a id="more"></a>
]]></content>
      <tags>
        <tag>数据架构</tag>
      </tags>
  </entry>
  <entry>
    <title>常用linux命令积累</title>
    <url>/posts/a1fe0a10/</url>
    <content><![CDATA[
        <h4 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h4>
      <p>工作中经常碰到的一些linxu命令组合</p>
<a id="more"></a>


        <h4 id="文件添加第一列"   >
          <a href="#文件添加第一列" class="heading-link"><i class="fas fa-link"></i></a>文件添加第一列</h4>
      <figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">awk <span class="string">&#x27;&#123;$1=&quot;order_data|&quot;$1; print&#125;&#x27;</span>  xx.txt  &gt;&gt; xxx.txt</span><br></pre></td></tr></table></div></figure>

        <h4 id="文件添加第一行"   >
          <a href="#文件添加第一行" class="heading-link"><i class="fas fa-link"></i></a>文件添加第一行</h4>
      <p>1i 代指第一行</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">sed -i <span class="string">&#x27;1ievent_name|apid|pegtime|product_name|empno&#x27;</span> xxx.txt</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>


        <h5 id="对接受同一个参数不同值的shell多次调用"   >
          <a href="#对接受同一个参数不同值的shell多次调用" class="heading-link"><i class="fas fa-link"></i></a>对接受同一个参数不同值的shell多次调用</h5>
      <p>懒得在写 for 循环处理了</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">cat events.ini | while read line; do sh simple_delete.sh $line ; done</span><br></pre></td></tr></table></div></figure>]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>常见的Linux数据处理命令</title>
    <url>/posts/265c42ae/</url>
    <content><![CDATA[
        <h3 id="背景"   >
          <a href="#背景" class="heading-link"><i class="fas fa-link"></i></a>背景</h3>
      <p>读到了一篇文章讲的如何用 linux 命令来实现一些常见的数据处理操作，如排序，去重，聚合等，感觉非常不错。正好最近工作也用到了这些，遂翻译过来，顺便实践一下。原文地址 <span class="exturl"><a class="exturl__link"   href="https://blog.robertelder.org/data-science-linux-command-line/" >An Introduction To Data Science On The Linux Command Line</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<a id="more"></a>


        <h3 id="The-‘-’-Symbol"   >
          <a href="#The-‘-’-Symbol" class="heading-link"><i class="fas fa-link"></i></a>The ‘|’ Symbol</h3>
      <p>许多读者可能已经非常熟悉 | 符号，但是有的或许并不明白其中的含义。在这里需要提前说明一下：在下面几个章节中许多命令的输入和输出都是通过 | 管道来连接的，这意味着我们可以链接很多有用的命令来组合成有用的程序，而这些程序可以直接运行在命令行上。</p>

        <h3 id="grep"   >
          <a href="#grep" class="heading-link"><i class="fas fa-link"></i></a>grep</h3>
      <p><strong>what’s grep？</strong>grep 是一个用于提取文件中指定匹配文本的工具。我们可以通过很多不同的标志或者选项来选择从文件或者流中匹配指定的文本。grep 通常是一个面向行的工具，这意味着当它匹配到指定文本，包含匹配文本的整行数据就会被输出出来。当然你可以使用 -o 选项只输出匹配到的文本。</p>
<p><strong>why is grep useful？</strong>grep 非常有用是因为这是一个比较快的从大量文本总找出匹配文本的方法。一些比较常见的例子有：过滤 web 日志文件查看指定网页的访问量；在你的代码包中搜索指定的关键词（grep 往往比编辑器搜索的更快更可靠）；在 Unix 的管道中间过滤前一个命令的输出。</p>
<p><strong>How does grep relate to data science？</strong> grep 对特定的数据科学任务非常有用，因为它可以很快的从数据集中匹配得到你想要的信息。很有可能你的数据集中存在很多无关的信息，你想要的数据分布在文本的各个行中。如果你能想到一个完美的规则来过滤它们，那么 grep 非常有用。比如，你现在有一个记录销售信息的 csv 文件</p>
<details>
    <summary>sales.csv</summary>
    tem, modelnumber, price, tax <br>
    Sneakers, MN009, 49.99, 1.11<br>
    Sneakers, MTG09, 139.99, 4.11<br>
    Shirt, MN089, 8.99, 1.44<br>
    Pants, N09, 39.99, 1.11<br>
    Sneakers, KN09, 49.99, 1.11<br>
    Shoes, BN009, 449.22, 4.31<br>
    Sneakers, dN099, 9.99, 1.22<br>
    Bananas, GG009, 4.99, 1.11<br>
</details>
你可以使用这样的一个命令
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">grep Sneakers sales.csv</span><br></pre></td></tr></table></div></figure>
这会过滤出包含 Sneakers 文本的销售记录，下面是执行这条命令的结果
<details>
    <summary>results.csv</summary>
    Sneakers, MN009, 49.99, 1.11<br>
    Sneakers, MTG09, 139.99, 4.11<br>
    Sneakers, KN09, 49.99, 1.11<br>
    Sneakers, dN099, 9.99, 1.22<br>
</details>

<p>你也可以通过正则表达式来寻找符合特定模式的文本，比如下面的这条命令可以用来过滤以 BN 或者 MN 开头且跟着三个数字的 modelnumber</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">grep -o <span class="string">&quot;\(BN\|MN\)\([0-9]\)\&#123;3\&#125;&quot;</span> sales.csv</span><br></pre></td></tr></table></div></figure>

        <h3 id="sed"   >
          <a href="#sed" class="heading-link"><i class="fas fa-link"></i></a>sed</h3>
      <p><strong>what’s sed？</strong>sed 用于搜索和替换文本。比如，你可以使用下面的命令将该目录下所有文件中的 dog 替换成 cat</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">sed -i <span class="string">&#x27;s/dog/cat/g&#x27;</span> *</span><br></pre></td></tr></table></div></figure>
<p><strong>why is sed useful？</strong> sed 可以使用正则来表达复杂的匹配场景。sed 的正则替换同样支持反向引用，这使得我们可以匹配任意的模式并且只改变匹配文本中的一部分内容。比如下面的命令会从给定的所有文本中搜索到双引号字符文本行并且交换位置，不需要改动匹配行中的其他文本，同时还将 “ 替换成 ()。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;The &quot;quick brown&quot; fox jumped over the &quot;lazy red&quot; dog.&#x27;</span> | sed -E <span class="string">&#x27;s/&quot;([^&quot;]+)&quot;([^&quot;]+)&quot;([^&quot;]+)&quot;/(\3)\2(\1)/&#x27;</span></span><br></pre></td></tr></table></div></figure>
<p>下面是执行的结果<br><code>The (lazy red) fox jumped over the (quck brown) dog.</code></p>
<p><strong>How does sed relate to data science？</strong>sed  可以用来转换数据格式，比如你需要计算销售额，但是很不幸，原数据的数值型数据加上了 ”，这时候你就可以使用 sed 去掉 “ 做最简单的格式转换</p>
<details>
    <summary>sales.csv</summary>
    age,value<br>
    "33","5943"<br>
    "32","543"<br>
    "34","93"<br>
    "39","5943"<br>
    "36","9943"<br>
    "38","8943"<br>
</details>

<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">cat sales.csv | sed -e <span class="string">&#x27;s/&quot;//g&#x27;</span></span><br></pre></td></tr></table></div></figure>
<p>如果你经常遇到以下场合，你需要将某个数据集传给你的程序去处理，但是文件中的某些字符无法被程序处理，这时候你就可以使用 sed 去格式化你的数据</p>

        <h3 id="awk"   >
          <a href="#awk" class="heading-link"><i class="fas fa-link"></i></a>awk</h3>
      <p><strong>What is awk?</strong>  awk 可以用作通用计算，有一些更高级的搜索和替换用法。</p>
<p><strong>Why is awk Useful?</strong> awk 可以轻松的处理格式化的文本行，就像一些高级编程语言一样。当然像搜索替换我们用 sed 也可以实现，但是 awk 表现的更漂亮。awk 还可以处理行与行之间的操作，比如第一列加和。</p>
<p><strong>How does awk relate to data science？</strong>现在我们有一个包含温度信息的文件，里面有华氏度和摄氏度，现在我们需要统一成摄氏度</p>
<details>
    <summary>temps.csv</summary>
    temp,unit<br>
    26.1,C<br>
    78.1,F<br>
    23.1,C<br>
    25.7,C<br>
    76.3,F<br>
    77.3,F<br>
    24.2,C<br>
    79.3,F<br>
    27.9,C<br>
    75.1,F<br>
    25.9,C<br>
    79.0,F<br>
</details>
你可以用简单的 awk 实现

<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">cat temps.txt | awk -F<span class="string">&#x27;,&#x27;</span> <span class="string">&#x27;&#123;if($2==&quot;F&quot;)print (($1-32)*5/9)&quot;,C&quot;;else print $1&quot;,&quot;$2&#125;&#x27;</span></span><br></pre></td></tr></table></div></figure>

        <h3 id="sort"   >
          <a href="#sort" class="heading-link"><i class="fas fa-link"></i></a>sort</h3>
      <p><strong>What is sort?</strong> 正如其名，sort 是用来排序的</p>
<p><strong>Why is sort Useful?</strong> sort本身可能并没有意义，其常用作一些数据的预处理：你想找到销售额最多的销售人员？sort it ! 然后输出第一行。你想得到 top N，sort it ! 然后输出前 n 行。你想根据数值或者字典排序，这些 sort 都帮你实现了。让我们去感受下不同方式的 sort</p>
<details>
    <summary>nums.txt</summary>
    0<br>
    1<br>
    1234<br>
    11<br>
    ZZZZ<br>
    1010<br>
    0123<br>
    hello world<br>
    abc123<br>
    Hello World<br>
    9<br>
    zzzz
</details>    
默认 sort 的结果，由下面可以看到，排序是按照字典序来的，所以下面的数值排序可能不符合你的期望。

<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">cat foo.txt | sort</span><br></pre></td></tr></table></div></figure>

<details>
    <summary>results</summary>
    0<br>
    0123<br>
    1<br>
    1010<br>
    11<br>
    1234<br>
    9<br>
    abc123<br>
    Hello World<br>
    hello world<br>
    ZZZZ<br>
    zzzz<br>
</details>
如果你想使用数值排序，加上 -n 选项就好了

<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">cat foo.txt | sort -n</span><br></pre></td></tr></table></div></figure>

<details>
    <summary>results</summary>
    0<br>
    abc123<br>
    Hello World<br>
    hello world<br>
    ZZZZ<br>
    zzzz<br>
    1<br>
    9<br>
    11<br>
    0123<br>
    1010<br>
    1234<br>
</details>
倒序排序，可以使用 -r 

<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">cat foo.txt | sort -r</span><br></pre></td></tr></table></div></figure>
<details>
    <summary>results</summary>
    zzzz<br>
    ZZZZ<br>
    hello world<br>
    Hello World<br>
    abc123<br>
    9<br>
    1234<br>
    11<br>
    1010<br>
    1<br>
    0123<br>
    0<br>
</details>

<p><strong>How does sort relate to data science？</strong>像下面提到的 comm，uniq 就需要接收 sort 排序后的输入。sort -R 还允许我们随机排列输入，这对于生成测试用例测试程序非常有帮助。</p>

        <h3 id="comm"   >
          <a href="#comm" class="heading-link"><i class="fas fa-link"></i></a>comm</h3>
      <p><strong>What is comm?</strong> comm 可以用来计算文件间的差集，并集，补集。</p>
<p>**Why is comm Useful? **comm 可以使我们很容易了解到 2 个文件的不同与相同之处。</p>
<p><strong>How does comm relate to data science？</strong>一个很好的例子就是你现在有 2 个邮箱列表，一个记录了登录的邮箱，signup.txt。一个记录了产生购买行为的邮箱列表，purchase.txt。</p>
<details>
    <summary>signup.txt</summary>
    8_so_late@hotmail.com<br>
    fred@example.com<br>
    info@info.info<br>
    something@somewhere.com<br>
    ted@example.net<br>
</details>

<details>
    <summary>purchase.txt</summary>
    example@gmail.com<br>
    fred@example.com<br>
    mark@facebook.com<br>
    something@somewhere.com<br>
</details>

<p>我们现在有3个问题，1）登录并购买的用户有哪些？2）登录没有购买的用户有哪些？3）没有登录就购买的用户有哪些？comm 可以非常容易的解决这些问题，比如下面的命令就可以看到登录并购买的用户</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">comm -12 signups.txt purchases.txt</span><br></pre></td></tr></table></div></figure>
<p>需要注意的是 comm 处理的文件必须是被排序过的</p>

        <h3 id="uniq"   >
          <a href="#uniq" class="heading-link"><i class="fas fa-link"></i></a>uniq</h3>
      <p>**What is uniq?**uniq 会帮你解决唯一性的问题。</p>
<p><strong>Why is uniq Useful?</strong> 如果你想删除重复数据，uniq 会帮助你。如果你想知道每条数据的重复次数，uniq 会告诉你。uniq 也可以帮你输出重复项，甚至完整性唯一性的检查。</p>
<p><strong>How does uniq relate to data science？</strong>举个例子，现在你有一个销售数据文件 sales.csv </p>
<details>
    <summary>sales.csv</summary>
    Shoes,19.00<br>
    Shoes,28.00<br>
    Pants,77.00<br>
    Socks,12.00<br>
    Shirt,22.00<br>
    Socks,12.00<br>
    Socks,12.00<br>
    Boots,82.00<br>
</details>

<p>现在你想知道我们到底销售了哪几样商品，你可以通过 awk 提取商品名称然后通过管道传递给 uniq </p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">cat sales.csv | awk -F<span class="string">&#x27;,&#x27;</span> <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | sort | uniq</span><br></pre></td></tr></table></div></figure>
<p>我们也可以得到每件商品的销售量</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">cat sales.csv | awk -F<span class="string">&#x27;,&#x27;</span> <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | sort | uniq -c</span><br></pre></td></tr></table></div></figure>
<p>下面是执行的结果</p>
<details>
    <summary>results.csv</summary>
    1 Boots<br>
    1 Pants<br>
    1 Shirt<br>
    2 Shoes<br>
    3 Socks<br>
</details>    


        <h3 id="tr"   >
          <a href="#tr" class="heading-link"><i class="fas fa-link"></i></a>tr</h3>
      <p><strong>What is tr?</strong> tr 可以用来删除替换一些文件中的字符，如 \n</p>
<p>**Why is  Ustreful? ** tr 比较有用的一个地方就是替换掉 windows 上产生的回车符等一些乱七八糟的符号。也可以做一些大小写转换。比如下面的例子将 test.txt 中的 a-z 字符集替换成 大写</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">cat test.txt | tr a-z A-Z</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">**How does tr relate to data science？**tr 命令并不像上面的命令与数据处理的联系很深，但是对于特殊情况的数据修复和处理还是非常有帮助的。</span><br><span class="line"></span><br><span class="line"><span class="comment">### cat</span></span><br><span class="line">**What is cat?** cat 可以读取一个或者多个文件，并将内容输出到标准输出</span><br><span class="line"></span><br><span class="line">**Why is cat Useful?** cat 对于合并多个文件非常有用，或者你想把文件输出</span><br><span class="line"></span><br><span class="line">**How does cat relate to data science？**cat 在日常的数据处理中非常常见，最常见的例子是你想聚合几个格式相同的文件然后做数据处理。比如你想对目录下的 csv 文件进行聚合去重</span><br><span class="line">```sh</span><br><span class="line">cat *.csv | awk -F<span class="string">&#x27; &#x27;</span> <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | sort | uniq</span><br></pre></td></tr></table></div></figure>
<p>你可能经常看到用 cat 读取文件然后交给其他程序去处理</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">cat test.txt | somecommand</span><br></pre></td></tr></table></div></figure>
<p>也许有人说 cat 并没有用，这是因为我们可以使用输入重定向</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">somecommand &lt; file.txt</span><br></pre></td></tr></table></div></figure>

        <h3 id="head"   >
          <a href="#head" class="heading-link"><i class="fas fa-link"></i></a>head</h3>
      <p><strong>What is head?</strong> head 允许你只输出一个文件的前几行数据</p>
<p><strong>Why is head Useful?</strong> 你只想检查一个大文件的一部分内容，你只想计算前几行数据</p>
<p><strong>How does head relate to data science？</strong>比如你有以下的商品销售文件 sales.csv</p>
<details>
    <summary>sales.csv</summary>
    Shoes,19.00<br>
    Shoes,19.00<br>
    Pants,77.00<br>
    Pants,77.00<br>
    Shoes,19.00<br>
    Shoes,28.00<br>
    Pants,77.00<br>
    Boots,22.00<br>
    Socks,12.00<br>
    Socks,12.00<br>
    Socks,12.00<br>
    Shirt,22.00<br>
    Socks,12.00<br>
    Boots,82.00<br>
    Boots,82.00<br>
</details>

<p>你想计算 top 3 最受欢迎的商品，可以使用如下命令</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">cat sales.csv | awk -F <span class="string">&#x27;,&#x27;</span> <span class="string">&#x27;print $1&#x27;</span> | sort | uniq -c | sort -n -r | head -n 3</span><br></pre></td></tr></table></div></figure>

        <h3 id="tail"   >
          <a href="#tail" class="heading-link"><i class="fas fa-link"></i></a>tail</h3>
      <p><strong>What is tail?</strong> tail 和head 是相反的，tail 输出文件末尾的内容</p>
<p>**Why is tail Useful? ** tail 和 head 的功能差不多</p>
<p><strong>How does tail relate to data science？</strong> 你可以使用 tail 去计算上面 head的例子</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">cat sales.csv | awk -F<span class="string">&#x27;,&#x27;</span> <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | sort | uniq -c | sort -n | tail -n 3</span><br></pre></td></tr></table></div></figure>
<p>tail 还可以用来指定起始行，最常见的是 csv 的表头，如果我们只关注数据而不需要表头的话，那么可以使用 tail</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">cat sales.csv | tail -n +2 </span><br></pre></td></tr></table></div></figure>

        <h3 id="wc"   >
          <a href="#wc" class="heading-link"><i class="fas fa-link"></i></a>wc</h3>
      <p><strong>What is wc?</strong> wc 用于单词或者行计数</p>
<p><strong>Why is wc Useful?</strong>  wc 总是能很快解答你的文件有多少行，每行有多少个字符这样的问题。</p>
<p><strong>How does wc relate to data science？</strong>比如你想快速知道你的邮件列表文件中有多少个邮件，wc 就可以做到</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">wc -l email.csv</span><br></pre></td></tr></table></div></figure>
<p>wc 也可以用来对多个文件进行计数，这时候就可以和下面提到的 find 命令组合。下面这条命令会统计 data 目录下所有 json  文件的行数</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">wc -l `find /data -name *.json`</span><br></pre></td></tr></table></div></figure>
<p>wc 用作字符计数，这里的 -n 是避免换行，换行会导致字符数 + 1</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> -n <span class="string">&quot;Here is some text that you&#x27;ll get a character count for&quot;</span> | wc -c</span><br></pre></td></tr></table></div></figure>

        <h3 id="find"   >
          <a href="#find" class="heading-link"><i class="fas fa-link"></i></a>find</h3>
      <p><strong>What is find?</strong>  find 用于查找文件，并且可以对查找的文件进行一些处理</p>
<p><strong>Why is find Useful?</strong> find 可以通过指定不同的选项（文件类型，文件大小，文件权限等）来查找文件，最有用的是 -exec 选项，它使得我们每找到一个文件，就可以立即对该文件进行操作。</p>
<p><strong>How does find relate to data science？</strong>让我们用 find 来查找并替换文件中的内容，find 和 sed 的结合。</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">find . -<span class="built_in">type</span> f -<span class="built_in">exec</span> sed -i <span class="string">&#x27;s/dog/cat/g&#x27;</span> &#123;&#125; \;</span><br></pre></td></tr></table></div></figure>
<p>find 后面 . 的含义的是当前目录，find 会从当前目录递归查找所有符合条件的文件并做处理，我们也可以通过 find 来删除一些程序生成的临时文件，比如 spark 的 *crc 文件</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">find . -name <span class="string">&#x27;*.crc&#x27;</span> -<span class="built_in">type</span> f -<span class="built_in">exec</span> rm -rf &#123;&#125; \;</span><br></pre></td></tr></table></div></figure>

        <h3 id="tsort"   >
          <a href="#tsort" class="heading-link"><i class="fas fa-link"></i></a>tsort</h3>
      <p><strong>What is tsort?</strong> tsort 用来拓补排序，有点类似计算数据之间的血缘关系</p>
<p><strong>Why is tsort Useful?</strong>  这里用一个简单的例子来描述 tsort，既然是计算血缘关系，那么也就是下一个任务的进行依赖于前一个任务的完成。现在我们就有一个这样的文件 ‘task_dependencies.txt’</p>
<details>
    <summary>task_dependencies.txt</summary>
    wall_framing foundation<br>
    foundation excavation<br>
    excavation construction_permits<br>
    dry_wall electrical<br>
    electrical wall_framing<br>
    wall_painting crack_filling<br>
    crack_filling dry_wall<br>
</details>

<p>上述文件描述了乱序的事物依赖关系，让我们用 tsort 来梳理一下</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">cat task_dependencies.txt | tsort</span><br></pre></td></tr></table></div></figure>
<p>tsort 会遍历每一行内容，并假设前面的一个事物是后面的依赖，由此计算排序整体的依赖关系，下面是执行的结果</p>
<details>
    <summary>results.txt</summary>
    wall_painting<br>
    crack_filling<br>
    dry_wall<br>
    electrical<br>
    wall_framing<br>
    foundation<br>
    excavation<br>
    construction_permits<br>
</details>    

<p><strong>How does tsort relate to data science？</strong>拓补排序是图论中常见的问题，应用于机器学习，任务调度，项目管理等等。像 Spark，Flink 对任务的调度就用到了拓补排序。</p>

        <h3 id="tee"   >
          <a href="#tee" class="heading-link"><i class="fas fa-link"></i></a>tee</h3>
      <p><strong>What is tee?</strong> tee命令同时向指定文件和标准输出打印信息流</p>
<p>**Why is tee Useful? **如果你想在运行程序的时候，既要记录日志文件，又想观察程序的输出，tee 命令可以帮助你</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">bash test.sh | tee test.log</span><br></pre></td></tr></table></div></figure>
<p><strong>How does tee relate to data science？</strong>tee 命令对数据分析可能并没有用处，但是对我们 debug 很有帮助，比如你有一个非常长的数据处理管道，但是并没有得到你想要的结果，到底是哪一步出了问题了，tee 这时候就可以帮助你</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">cat sales.csv | tail -n +2 | tee after_tail.log | awk -F<span class="string">&#x27;,&#x27;</span> <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | tee after_awk.log | sort | tee after_sort.log | uniq -c | tee after_uniq.log</span><br></pre></td></tr></table></div></figure>

        <h3 id="The-‘-gt-’-Symbol"   >
          <a href="#The-‘-gt-’-Symbol" class="heading-link"><i class="fas fa-link"></i></a>The ‘&gt;’ Symbol</h3>
      <p>**What is the ‘&gt;’ Symbol?**输出重定向</p>
<p><strong>Why is the ‘&gt;’ Symbol Useful?</strong> 输出重定向，输出重定向到文件，而不是在打印到屏幕上</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">cat sales.csv | tail -n +2 | awk -F<span class="string">&#x27;,&#x27;</span> <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | sort | uniq -c &gt; unique_counts.txt</span><br></pre></td></tr></table></div></figure>

        <h3 id="The-‘-lt-’-Symbol"   >
          <a href="#The-‘-lt-’-Symbol" class="heading-link"><i class="fas fa-link"></i></a>The ‘&lt;’ Symbol</h3>
      <p><strong>What is the ‘&lt;’ Symbol?</strong> 输入重定向</p>
<p><strong>Why is the ‘&lt;’ Symbol Useful?</strong> 指定程序的输入</p>
<figure class="highlight sh"><div class="table-container"><table><tr><td class="code"><pre><span class="line">grep Pants &lt; sales.txt</span><br></pre></td></tr></table></div></figure>





]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>常见的SQL优化</title>
    <url>/posts/5c09ddf8/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>关系型数据库中常见的 SQL 优化。挺久前写过的笔记了，应该借鉴了挺多文章，侵删。</p>
<a id="more"></a>


        <h3 id="常见的-SQL-优化"   >
          <a href="#常见的-SQL-优化" class="heading-link"><i class="fas fa-link"></i></a>常见的 SQL 优化</h3>
      
        <h4 id="操作符-lt-gt-优化"   >
          <a href="#操作符-lt-gt-优化" class="heading-link"><i class="fas fa-link"></i></a>操作符 &lt;&gt; 优化</h4>
      <p>通常 &lt;&gt; 操作符无法使用索引，如果 amount 为100的订单极少，这种数据分布严重不均的情况下，有可能使用索引。</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> orders <span class="keyword">where</span> amount != <span class="number">100</span>;</span><br></pre></td></tr></table></div></figure>

<p>鉴于这种不确定性，采用 union 聚合搜索结果。</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line">(<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> orders <span class="keyword">where</span> amount &gt; <span class="number">100</span>)</span><br><span class="line"> <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">(<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> orders <span class="keyword">where</span> amount &lt; <span class="number">100</span> <span class="keyword">and</span> amount &gt; <span class="number">0</span>)</span><br></pre></td></tr></table></div></figure>


        <h4 id="or-优化"   >
          <a href="#or-优化" class="heading-link"><i class="fas fa-link"></i></a>or 优化</h4>
      <p>在Innodb引擎下or无法使用组合索引</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, product_name <span class="keyword">from</span> orders <span class="keyword">where</span> mobile_no = <span class="string">&#x27;13421800407&#x27;</span> <span class="keyword">or</span> user_id = <span class="number">100</span>;</span><br></pre></td></tr></table></div></figure>
<p>OR 无法命中 mobile_no + user_id 的组合索引，可采用 union</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line">(<span class="keyword">select</span> <span class="keyword">id</span>, product_name <span class="keyword">from</span> orders <span class="keyword">where</span> mobile_no = <span class="string">&#x27;13421800407&#x27;</span>)</span><br><span class="line"><span class="keyword">union</span></span><br><span class="line">(<span class="keyword">select</span> <span class="keyword">id</span>, product_name <span class="keyword">from</span> orders <span class="keyword">where</span> user_id = <span class="number">100</span>)</span><br></pre></td></tr></table></div></figure>


        <h4 id="in-优化"   >
          <a href="#in-优化" class="heading-link"><i class="fas fa-link"></i></a>in 优化</h4>
      <p>in 适合主表大子表小，exists 适合主表小子表大。</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> orders <span class="keyword">where</span> user_id <span class="keyword">in</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> <span class="keyword">level</span> = <span class="string">&#x27;VIP&#x27;</span>);</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> orders <span class="keyword">where</span> user_id <span class="keyword">in</span> (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>);</span><br></pre></td></tr></table></div></figure>

<p>有时可以用 join 来替代 in 操作。查询条件是连续的话可以用 between 代替 in</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> o.id <span class="keyword">from</span> orders o <span class="keyword">left</span> <span class="keyword">join</span> <span class="keyword">user</span> u <span class="keyword">on</span> o.user_id = u.id <span class="keyword">where</span> <span class="keyword">level</span> = <span class="string">&#x27;VIP&#x27;</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> orders <span class="keyword">where</span> user_id <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">and</span> <span class="number">3</span>;</span><br></pre></td></tr></table></div></figure>


        <h4 id="不做列运算"   >
          <a href="#不做列运算" class="heading-link"><i class="fas fa-link"></i></a>不做列运算</h4>
      <p>通常在 where 子句里 = 左边进行表达式或者函数操作会导致无法正确使用索引</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> orders <span class="keyword">where</span> <span class="keyword">id</span> / <span class="number">2</span> = <span class="number">4</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span> <span class="keyword">from</span> orders <span class="keyword">where</span> <span class="keyword">substring</span>(<span class="keyword">name</span>, <span class="number">1</span>, <span class="number">3</span>) = <span class="string">&#x27;SQL&#x27;</span>;</span><br></pre></td></tr></table></div></figure>
<p>可以改写成如下格式</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> orders <span class="keyword">where</span> <span class="keyword">id</span> = <span class="number">4</span> * <span class="number">2</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span> <span class="keyword">from</span> orders <span class="keyword">where</span> <span class="keyword">name</span> <span class="keyword">like</span> <span class="string">&#x27;SQL%&#x27;</span>;</span><br></pre></td></tr></table></div></figure>

        <h4 id="like-优化"   >
          <a href="#like-优化" class="heading-link"><i class="fas fa-link"></i></a>like 优化</h4>
      <p>下面这样写在 description 建立索引的情况下并不会命中索引</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> description <span class="keyword">from</span> <span class="keyword">order</span> <span class="keyword">where</span> description <span class="keyword">like</span> <span class="string">&#x27;%keyword%&#x27;</span>;</span><br></pre></td></tr></table></div></figure>
<p>改写成如下</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> description <span class="keyword">from</span> <span class="keyword">order</span> <span class="keyword">where</span> description <span class="keyword">like</span> <span class="string">&#x27;keyword%&#x27;</span>;</span><br></pre></td></tr></table></div></figure>

        <h4 id="进行-null-值判断"   >
          <a href="#进行-null-值判断" class="heading-link"><i class="fas fa-link"></i></a>进行 null 值判断</h4>
      <p>直接判断 null 值 会导致引擎放弃使用索引</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span> <span class="keyword">from</span> orders <span class="keyword">where</span> <span class="keyword">name</span> <span class="keyword">is</span> <span class="literal">null</span>;</span><br></pre></td></tr></table></div></figure>
<p>可以为 null 值填充上有意义的默认值之后在进行过滤判断</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span> <span class="keyword">from</span> orders <span class="keyword">where</span> <span class="keyword">name</span> = <span class="number">0</span>;</span><br></pre></td></tr></table></div></figure>

        <h4 id="limit-优化"   >
          <a href="#limit-优化" class="heading-link"><i class="fas fa-link"></i></a>limit 优化</h4>
      <p>limit 查询起始值越大查询越慢</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> orders <span class="keyword">limit</span> <span class="number">1000000</span>, <span class="number">10</span>;</span><br></pre></td></tr></table></div></figure>
<p>缩小 limit 的查询范围来进行优化</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> orders <span class="keyword">where</span> <span class="keyword">id</span> &gt; (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> orders <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">id</span> <span class="keyword">desc</span> <span class="keyword">limit</span> <span class="number">1000000</span>,<span class="number">1</span>) <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">id</span> <span class="keyword">desc</span> <span class="keyword">limit</span> <span class="number">0</span>,<span class="number">10</span></span><br></pre></td></tr></table></div></figure>

        <h4 id="强制使用索引"   >
          <a href="#强制使用索引" class="heading-link"><i class="fas fa-link"></i></a>强制使用索引</h4>
      <figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span> <span class="keyword">from</span> orders <span class="keyword">with</span> <span class="keyword">index</span>(索引名) <span class="keyword">from</span> orders;</span><br></pre></td></tr></table></div></figure>


        <h3 id="实践"   >
          <a href="#实践" class="heading-link"><i class="fas fa-link"></i></a>实践</h3>
      
        <h4 id="问题"   >
          <a href="#问题" class="heading-link"><i class="fas fa-link"></i></a>问题</h4>
      <p>腾讯 DBBrain 上的参赛题。现有 order，order_item 两张数据表。需要优化 select.sql 和 update.sql  中的 sql 语句来应对模拟访问造成的数据库压力等问题。在本地的话，就是优化一下 sql 的执行实践就可以了。</p>
<p><span class="exturl"><a class="exturl__link"   href="https://pan.baidu.com/s/1J08immFGH5ewjoILKEH9_g" >数据源下载</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 提取密码：qq28</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line">order 表</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`order`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT,</span><br><span class="line">  <span class="string">`name`</span> <span class="built_in">varchar</span>(<span class="number">32</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`creator`</span> <span class="built_in">varchar</span>(<span class="number">24</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`price`</span> <span class="built_in">varchar</span>(<span class="number">64</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`create_time`</span> <span class="built_in">timestamp</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">DEFAULT</span> <span class="keyword">CURRENT_TIMESTAMP</span>,</span><br><span class="line">  <span class="string">`status`</span> <span class="built_in">tinyint</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8;</span><br><span class="line"></span><br><span class="line">order_item 表</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`order_item`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT,</span><br><span class="line">  <span class="string">`name`</span> <span class="built_in">varchar</span>(<span class="number">32</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`parent`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`status`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`type`</span> <span class="built_in">varchar</span>(<span class="number">12</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">DEFAULT</span> <span class="string">&#x27;0&#x27;</span>,</span><br><span class="line">  <span class="string">`quantity`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">DEFAULT</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">`update_time`</span> <span class="built_in">timestamp</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">DEFAULT</span> <span class="keyword">CURRENT_TIMESTAMP</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8;</span><br></pre></td></tr></table></div></figure>
<p>select.sql，执行用时 2.19 秒</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span>   <span class="string">`order`</span> o</span><br><span class="line">       <span class="keyword">INNER</span> <span class="keyword">JOIN</span> order_item i <span class="keyword">ON</span> i.parent = o.id</span><br><span class="line"><span class="keyword">ORDER</span>  <span class="keyword">BY</span> o.status <span class="keyword">ASC</span>,</span><br><span class="line">          i.update_time <span class="keyword">DESC</span></span><br><span class="line"><span class="keyword">LIMIT</span>  <span class="number">0</span>, <span class="number">20</span></span><br></pre></td></tr></table></div></figure>

<p>update.sql，执行用时比较长，在本地运行直接卡住了</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">update</span> <span class="string">`order`</span> <span class="keyword">set</span></span><br><span class="line">create_time = <span class="keyword">now</span>()</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">id</span> <span class="keyword">in</span> (</span><br><span class="line">    <span class="keyword">select</span> <span class="keyword">parent</span> <span class="keyword">from</span> order_item <span class="keyword">where</span> <span class="keyword">type</span> = <span class="number">2</span></span><br><span class="line">)</span><br></pre></td></tr></table></div></figure>



        <h4 id="解决方案"   >
          <a href="#解决方案" class="heading-link"><i class="fas fa-link"></i></a>解决方案</h4>
      <p>update.sql， type 是 varchar 类型，而 where 子句中 type = 2，类型不匹配。对 type，parent 字段上建立组合索引，将 in 子查询改为 join（这里的数据量不是很大）</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> <span class="string">`order`</span> <span class="keyword">add</span> <span class="keyword">index</span> idx_1(<span class="keyword">type</span>, <span class="keyword">parent</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">update</span> <span class="string">`order`</span> o <span class="keyword">inner</span> <span class="keyword">join</span> (</span><br><span class="line">   <span class="keyword">select</span> <span class="keyword">type</span>, <span class="keyword">parent</span> <span class="keyword">from</span> <span class="string">`order_item`</span> <span class="keyword">where</span> <span class="keyword">type</span> = <span class="string">&#x27;2&#x27;</span> <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">type</span>, <span class="keyword">parent</span></span><br><span class="line">) i <span class="keyword">on</span> o.id = i.parent <span class="keyword">set</span> create_time = <span class="keyword">now</span>();</span><br></pre></td></tr></table></div></figure>
<p>select.sql</p>
<figure class="highlight sql"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> o.*,i.*</span><br><span class="line"><span class="keyword">FROM</span>   (</span><br><span class="line">         (<span class="keyword">SELECT</span> o.id, i.id item_id</span><br><span class="line">          <span class="keyword">FROM</span>   <span class="string">`order`</span> o</span><br><span class="line">          <span class="keyword">INNER</span> <span class="keyword">JOIN</span> order_item i <span class="keyword">ON</span> i.parent =o.id</span><br><span class="line">          <span class="keyword">WHERE</span>  o.status = <span class="number">0</span></span><br><span class="line">          <span class="keyword">ORDER</span>  <span class="keyword">BY</span> i.update_time <span class="keyword">DESC</span></span><br><span class="line">          <span class="keyword">LIMIT</span>  <span class="number">0</span>, <span class="number">20</span></span><br><span class="line">         )</span><br><span class="line">          <span class="keyword">UNION</span> <span class="keyword">ALL</span></span><br><span class="line">         (<span class="keyword">SELECT</span> o.id, i.id item_id <span class="keyword">FROM</span> <span class="string">`order`</span> o</span><br><span class="line">           <span class="keyword">INNER</span> <span class="keyword">JOIN</span> order_item i <span class="keyword">ON</span> i.parent =o.id</span><br><span class="line">           <span class="keyword">WHERE</span>  o.status = <span class="number">1</span></span><br><span class="line">           <span class="keyword">ORDER</span>  <span class="keyword">BY</span> i.update_time <span class="keyword">DESC</span></span><br><span class="line">           <span class="keyword">LIMIT</span>  <span class="number">0</span>, <span class="number">20</span></span><br><span class="line">         )</span><br><span class="line">        ) tmp</span><br><span class="line">       <span class="keyword">INNER</span> <span class="keyword">JOIN</span> <span class="string">`order`</span> o <span class="keyword">ON</span> tmp.id = o.id</span><br><span class="line">       <span class="keyword">INNER</span> <span class="keyword">JOIN</span> order_item i <span class="keyword">ON</span> tmp.item_id =i.id</span><br><span class="line"><span class="keyword">ORDER</span>  <span class="keyword">BY</span> o.status <span class="keyword">ASC</span>,</span><br><span class="line">          i.update_time <span class="keyword">DESC</span></span><br><span class="line"><span class="keyword">LIMIT</span>  <span class="number">0</span>, <span class="number">20</span></span><br></pre></td></tr></table></div></figure>


        <h3 id="阅读"   >
          <a href="#阅读" class="heading-link"><i class="fas fa-link"></i></a>阅读</h3>
      
        <h4 id="Blog"   >
          <a href="#Blog" class="heading-link"><i class="fas fa-link"></i></a>Blog</h4>
      <p><span class="exturl"><a class="exturl__link"   href="https://www.programmerinterview.com/database-sql/what-is-an-index/" >How do database indexes work? And, how do indexes help?</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>数据开发岗社招面经</title>
    <url>/posts/df46db3a/</url>
    <content><![CDATA[
        <h3 id="面经"   >
          <a href="#面经" class="heading-link"><i class="fas fa-link"></i></a>面经</h3>
      <p>编码能力，清晰的表达，解决问题的方式，学习的方法，总结的能力，个人的潜力，感觉这几点往往是最重要的。这几点是需要在生活与工作中建立习惯来养成的。<br>明确自己感兴趣的行业，去寻找行业中的头部公司作为目标来去了解，去学习。<br>读书拓展思维，二八原理。</p>
<a id="more"></a>
<ul>
<li>作业帮</li>
<li>元气森林</li>
<li>莉莉丝游戏</li>
<li>米哈游</li>
<li>同策咨询</li>
<li>久谦咨询</li>
<li>本来生活</li>
<li>字节数据中台</li>
<li>腾讯</li>
<li>个推</li>
<li>阿里</li>
<li>叽里呱啦</li>
<li>建信金科</li>
<li>同盾科技</li>
</ul>

        <h3 id="米哈游"   >
          <a href="#米哈游" class="heading-link"><i class="fas fa-link"></i></a>米哈游</h3>
      <p>六月份尝试过一次，主要想看下自己的水平与市场的认可度，挂掉原因虽然了解的比较多，但是项目经验比较少，对方更多的用 Hive， 具体参考<a href="https://timemachine.icu/posts/d6dc8cdf/">米哈游一面</a></p>

        <h3 id="作业帮"   >
          <a href="#作业帮" class="heading-link"><i class="fas fa-link"></i></a>作业帮</h3>
      
        <h4 id="面"   >
          <a href="#面" class="heading-link"><i class="fas fa-link"></i></a>-面</h4>
      <ul>
<li>手写 Spark SQL 从日志中统计活跃用户和非活跃用户的平均年龄，活跃用户判断标准是是否连续登陆两天</li>
<li>leetcode 算法，无序数组中，返回两个数字为指定和的所有下标列表</li>
<li>工作中碰到过哪些 Spark 调优，讲一个</li>
</ul>

        <h4 id="二面"   >
          <a href="#二面" class="heading-link"><i class="fas fa-link"></i></a>二面</h4>
      <ul>
<li>Spark Job 的提交流程，从宏观上来看，从一条 SQL 在 Spark 中执行过程来看</li>
<li>Spark RBO，常见的 RBO 。CBO</li>
<li>怎么查看是否谓词下推？如何自己实现谓词下推</li>
<li>Spark SQL 参数优化，用过哪些参数</li>
<li>Spark JDBC 调优  </li>
<li>Spark Job 提交 YARN 流程</li>
<li>Spark Join 策略有哪些</li>
<li>Broadcast 是怎么实现的，如果广播的变量发生了变动，executor 端怎么感知到</li>
<li>Spark Shuffle 流程，一些底层实现知道吗</li>
<li>Impala + hive</li>
<li>存储有了解过吗，parquet，orc</li>
<li>jvm 内存模型</li>
<li>HashMap，哈希碰撞</li>
<li>手写 SQL<ul>
<li>统计三年级总成绩大于 300 分的学生姓名和总成绩</li>
<li>每个年级 TOP 3的学生姓名和总成绩</li>
</ul>
</li>
<li>你想找一个什么样的工作</li>
<li>为什么年底换工作</li>
<li>工作日晚上，周末一般做什么</li>
<li>有没有技术上擅长但是我没问到的</li>
<li>有啥想问我的吗</li>
</ul>

        <h4 id="三面视频面"   >
          <a href="#三面视频面" class="heading-link"><i class="fas fa-link"></i></a>三面视频面</h4>
      <ul>
<li>简单介绍下自己和自己做过的项目</li>
<li>主要用哪些技术栈</li>
<li>互相了解聊了会，为什么离职，开始问技术</li>
<li>实现斐波那契，并能真正跑出结果，优化成空间复杂度 O(1)，主要看重现场编码能力</li>
<li>手写 SQL<ul>
<li>计算用户行为序列，有一张表存在 id，action，time 字段，连续五次每次执行 action 的时间间隔小于一分钟的用户是活跃用户，统计活跃用户的数量</li>
</ul>
</li>
<li>有用 Spark Streaming 做过实时数据流处理吗，没做过项目，那了解过吗，写过 demo 吗？</li>
<li>目前的薪资    </li>
<li>有什么想了解的吗</li>
</ul>

        <h4 id="hr面"   >
          <a href="#hr面" class="heading-link"><i class="fas fa-link"></i></a>hr面</h4>
      <ul>
<li>离职原因</li>
<li>职业规划</li>
<li>薪资</li>
<li>是否有其他 offer 和面试</li>
<li>简单介绍公司及部门</li>
<li>在线教育行业的看法</li>
</ul>

        <h3 id="元气森林"   >
          <a href="#元气森林" class="heading-link"><i class="fas fa-link"></i></a>元气森林</h3>
      
        <h4 id="面电话面"   >
          <a href="#面电话面" class="heading-link"><i class="fas fa-link"></i></a>-面电话面</h4>
      <ul>
<li>自我介绍 + 项目介绍</li>
<li>Spark 和 MR 的区别</li>
<li>Kakfa 的常用场景</li>
<li>四层数仓理论</li>
<li>如何解决数据不规范</li>
<li>如何判别垃圾需求</li>
<li>启发式问题（探讨下要做什么）</li>
</ul>

        <h3 id="莉莉丝游戏"   >
          <a href="#莉莉丝游戏" class="heading-link"><i class="fas fa-link"></i></a>莉莉丝游戏</h3>
      <p>挂掉原因 - 反馈说需要全栈的，我就 Spark 深入的比较多，其他都是浅层了解</p>

        <h4 id="现场笔试"   >
          <a href="#现场笔试" class="heading-link"><i class="fas fa-link"></i></a>现场笔试</h4>
      <ul>
<li>写 shell 统计销量 TOP N</li>
<li>给一张 MySQL 表，设计索引</li>
<li>SQL 题</li>
<li>leetcode easy 难度算法题</li>
<li>MR 的 shuffle 过程</li>
<li>JVM 内存模型</li>
<li>实现单例模式</li>
</ul>

        <h4 id="面现场面"   >
          <a href="#面现场面" class="heading-link"><i class="fas fa-link"></i></a>-面现场面</h4>
      <ul>
<li>说说笔试题难度</li>
<li>介绍下印象最深刻的项目，你从中得到了什么，有哪些优化的地方</li>
<li>了解 impala 吗</li>
<li>了解 kudu 吗</li>
<li>parquet 文件格式</li>
<li>为什么 impala 比 Spark 更常用作查询引擎，Impala 查询更快的原因是什么</li>
<li>你说熟悉 Spark，读过源码吗，说下 shuffle 过程，说些 SQL 解析过程，说下 Spark JOIN 策略，broadcast 是怎么实现的</li>
<li>部门是做什么的</li>
<li>离职原因</li>
</ul>

        <h3 id="同策咨询"   >
          <a href="#同策咨询" class="heading-link"><i class="fas fa-link"></i></a>同策咨询</h3>
      
        <h4 id="面-1"   >
          <a href="#面-1" class="heading-link"><i class="fas fa-link"></i></a>-面</h4>
      <ul>
<li>hive 内部表，外部表</li>
<li>四层数仓理论</li>
<li>kafka 全局有序？</li>
<li>MySQL 索引的实现</li>
<li>hive，hbase 用过吗</li>
<li>最后讲了下部门是做啥的，说是刚成立，不利于技术成长，劝我不要来，emmm，还有这样的操作。。。。。</li>
</ul>

        <h3 id="字节数据中台"   >
          <a href="#字节数据中台" class="heading-link"><i class="fas fa-link"></i></a>字节数据中台</h3>
      
        <h4 id="面-2"   >
          <a href="#面-2" class="heading-link"><i class="fas fa-link"></i></a>-面</h4>
      <ul>
<li>偏启发式问题，主要看你做没做过，解决问题的方式与思路</li>
<li>简单自我介绍</li>
<li>第一份工作是爬虫，爬过哪些数据，爬过非公开的数据吗</li>
<li>逆向反编译怎么做，讲几个你自己做过的例子</li>
<li>有哪些反爬措施，举个例子</li>
<li>对于模式识别的反爬是怎么解决的</li>
<li>IP 代理池</li>
<li>是怎么对爬下来的数据进行指标计算的，多维度计算</li>
<li>是怎么计算指标的，技术栈是什么的，有没有做维度查询，某个维度出现了误差该怎么解决，计算出来的指标是怎么展示的</li>
<li>如果指标计算出现了偏差，解决的方式是什么，怎么定位问题</li>
<li>举个你拿 Spark SQL 跑指标计算的项目来讲一下</li>
<li>数据库方面了解的多吗，比如 MySQL</li>
<li>手撸算法，数组最短重复数字子串的长度，可以理解为数字重复量最大的数字之间的最小的跨度，比如 <code>1,2,2,3,1</code>，重复次数最多的数字是 1 和 2，但是 1 的跨度是5，2 的跨度是 2，所以返回 <code>2,2</code> 的长度，即结果是 2。</li>
<li>SQL 题<ul>
<li>统计学生的年级排名和班级排名</li>
<li>如果是流式数据处理的话，这个计算排名的过程是什么样子的，比如 Flink</li>
</ul>
</li>
</ul>

        <h4 id="二面（WIP）"   >
          <a href="#二面（WIP）" class="heading-link"><i class="fas fa-link"></i></a>二面（WIP）</h4>
      
        <h4 id="三面"   >
          <a href="#三面" class="heading-link"><i class="fas fa-link"></i></a>三面</h4>
      
        <h3 id="腾讯"   >
          <a href="#腾讯" class="heading-link"><i class="fas fa-link"></i></a>腾讯</h3>
      
        <h4 id="面-3"   >
          <a href="#面-3" class="heading-link"><i class="fas fa-link"></i></a>-面</h4>
      <ul>
<li>自我介绍</li>
<li>简单介绍下做过的项目</li>
<li>从 5000 万个数中找到前 50 个最小的数据</li>
<li>快排</li>
<li>手写 SQL</li>
<li>技术栈</li>
<li>个人优缺点</li>
</ul>

        <h3 id="个推"   >
          <a href="#个推" class="heading-link"><i class="fas fa-link"></i></a>个推</h3>
      
        <h4 id="一面电话面"   >
          <a href="#一面电话面" class="heading-link"><i class="fas fa-link"></i></a>一面电话面</h4>
      <ul>
<li>简单介绍</li>
<li>爬虫相关的一些问题</li>
<li>Saprk JDBC 调节并行度，读过源码吗，原理是什么样子的</li>
<li>Spark Shuffle，相比 MR</li>
<li>还愿意做爬虫吗？</li>
</ul>

        <h4 id="二面（-give-up）"   >
          <a href="#二面（-give-up）" class="heading-link"><i class="fas fa-link"></i></a>二面（ give up）</h4>
      
        <h3 id="同盾科技"   >
          <a href="#同盾科技" class="heading-link"><i class="fas fa-link"></i></a>同盾科技</h3>
      
        <h4 id="一面电话面-1"   >
          <a href="#一面电话面-1" class="heading-link"><i class="fas fa-link"></i></a>一面电话面</h4>
      <ul>
<li>在神策的工作主要是什么，以项目来介绍下</li>
<li>主要用哪些技术栈</li>
<li>Spark Shufle</li>
<li>Spark Join</li>
<li>Spark 执行过程</li>
<li>Spark JDBC  调优</li>
<li>Spark 统一内存模型</li>
<li>jvm，堆内堆外都是干啥的</li>
<li>指标计算是怎么做的</li>
<li>HDFS 架构</li>
<li>parquet， orc 的区别</li>
<li>Hbase</li>
<li>是否愿意出差</li>
<li>有啥想了解的吗</li>
</ul>

        <h4 id="二面-1"   >
          <a href="#二面-1" class="heading-link"><i class="fas fa-link"></i></a>二面</h4>
      
        <h4 id="三面-（-give-up-）"   >
          <a href="#三面-（-give-up-）" class="heading-link"><i class="fas fa-link"></i></a>三面 （ give up ）</h4>
      ]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>树</title>
    <url>/posts/7b216a3b/</url>
    <content><![CDATA[
        <h3 id="树"   >
          <a href="#树" class="heading-link"><i class="fas fa-link"></i></a>树</h3>
      <p>树是一种数据结构。为什么会有树这种数据结构？目前理解的非常不深，回答不上来，在以后的日子里补。</p>
<a id="more"></a>


        <h3 id="判断平衡二叉树"   >
          <a href="#判断平衡二叉树" class="heading-link"><i class="fas fa-link"></i></a>判断平衡二叉树</h3>
      <p>什么是平衡？任意一个节点，其两棵子树的高度差不超过 1。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isBalanced</span></span>(root: <span class="type">TreeNode</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">var</span> abs = <span class="type">Math</span>.abs(getDepth(root.left) - getDepth(root.right))</span><br><span class="line">        <span class="keyword">if</span> (abs &lt;= <span class="number">1</span> &amp;&amp; isBalanced(root.left) &amp;&amp; isBalanced(root.right))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getDepth</span></span>(root:<span class="type">TreeNode</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        <span class="keyword">if</span> (root==<span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">var</span> left = getDepth(root.left)</span><br><span class="line">        <span class="keyword">var</span> right = getDepth(root.right)</span><br><span class="line">        <span class="keyword">return</span> <span class="type">Math</span>.max(left, right) + <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

        <h3 id="判断是否二叉搜索树"   >
          <a href="#判断是否二叉搜索树" class="heading-link"><i class="fas fa-link"></i></a>判断是否二叉搜索树</h3>
      <p>本质上就是树的遍历，这里比较粗暴，二叉搜索树的中序遍历肯定是一个递增序列，如果不是递增序列，就不是二叉搜索树</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">var</span> seq = <span class="type">Seq</span>[<span class="type">Int</span>]()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inOrder</span></span>(root: <span class="type">TreeNode</span>):<span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">if</span>(root != <span class="literal">null</span>) &#123;</span><br><span class="line">            inOrder(root.left)</span><br><span class="line">            seq = seq :+ root.value</span><br><span class="line">            inOrder(root.right)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isValidBST</span></span>(root: <span class="type">TreeNode</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">        </span><br><span class="line">        inOrder(root)</span><br><span class="line">        <span class="keyword">if</span> (seq.length == <span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (seq.length == <span class="number">1</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until seq.length<span class="number">-1</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (seq(i) &gt;= seq(i+<span class="number">1</span>))&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span> </span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

        <h3 id="二叉搜索树的下一个结点"   >
          <a href="#二叉搜索树的下一个结点" class="heading-link"><i class="fas fa-link"></i></a>二叉搜索树的下一个结点</h3>
      <p>遍历元素放入列表，查找列表。</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    <span class="function">def <span class="title">inorderSuccessor</span><span class="params">(self, root: TreeNode, p: TreeNode)</span> -&gt; TreeNode:</span></span><br><span class="line"><span class="function">        <span class="keyword">if</span> root:</span></span><br><span class="line"><span class="function">            <span class="keyword">if</span> p.val &gt;</span>= root.val:</span><br><span class="line">                <span class="keyword">return</span> self.inorderSuccessor(root.right,p)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">            	a = self.inorderSuccessor(root.left,p)</span><br><span class="line">                <span class="keyword">if</span> a is None:</span><br><span class="line">                    <span class="keyword">return</span> root</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">return</span> a</span><br><span class="line">        <span class="keyword">return</span> None</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>树的遍历</title>
    <url>/posts/332b8f17/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>每周刷题202010_12_16</title>
    <url>/posts/986a6022/</url>
    <content><![CDATA[
        <h4 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h4>
      <p>一开始刷题，不看题解，根本不会😂。开始思路的积累。所有的代码提交在 <span class="exturl"><a class="exturl__link"   href="https://github.com/Flyraty/some_thing_python" >some_thing_python</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/fibonacci-number/" >斐波那契数</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/coin-change/" >零钱兑换</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/partition-equal-subset-sum/" >分割等和子集</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/construct-binary-tree-from-preorder-and-inorder-traversal/" >从前序与中序遍历序列构造二叉树</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/construct-binary-tree-from-inorder-and-postorder-traversal/" >从中序与后序遍历序列构造二叉树</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/minimum-absolute-difference-in-bst/" >二叉搜索树的最小绝对差</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/swap-nodes-in-pairs/" >两两交换链表中的节点</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/find-common-characters/" >查找常用字符</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/populating-next-right-pointers-in-each-node/" >填充每个节点的下一个右侧节点指针</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/populating-next-right-pointers-in-each-node-ii/" >填充每个节点的下一个右侧节点指针 II</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/squares-of-a-sorted-array/" >有序数组的平方</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/n-queens-ii/" >N皇后 II</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/remove-nth-node-from-end-of-list/" >删除链表的倒数第N个节点</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<a id="more"></a>



        <h4 id="题解"   >
          <a href="#题解" class="heading-link"><i class="fas fa-link"></i></a>题解</h4>
      <ul>
<li><p>斐波那契数<br>递归解决，dp table 剪枝。</p>
</li>
<li><p>零钱兑换<br>动态规划，加一个 dp 函数，明确 base 状态，状态转移为 min(res, 1 + dp(n-coin))，明确边界及子问题无解的情况</p>
</li>
<li><p>分割等和子集<br>明确等和的条件</p>
<ul>
<li>等和子集，和一定是偶数</li>
<li>等和子集，列表中最大元素肯定要小于等于二分之和</li>
<li>等于二分之和，一定可以切割</li>
<li>大于二分之和，一定不能切割</li>
<li>小于二分之和，排序后依据最大元素找寻子集</li>
</ul>
</li>
<li><p>从前序与中序遍历序列构造二叉树<br>递归解决，其实就是找到根节点，得到左子树长度，划分左右子树，切割出左右子树的前中序遍历，开始递归。</p>
</li>
<li><p>二叉搜索树的最小绝对差<br>一棵 BST，前序遍历是递增序列。最小值的话肯定是两两相邻最小。直接前序遍历，对差值及初始节点赋值一个初始值</p>
</li>
<li><p>两两交换链表中的节点<br>依赖哑节点，temp -&gt; node1 -&gt; node2 =&gt; temp -&gt; node2 -&gt; node1， tmep 初始为哑结点</p>
</li>
<li><p>查找常用字符</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">res &#x3D; []</span><br><span class="line">base_char &#x3D; array(0)</span><br><span class="line">for k in base_char:</span><br><span class="line">	nums &#x3D; [a.count(k) for a in array]</span><br><span class="line">	res +&#x3D; [k]*nums</span><br></pre></td></tr></table></div></figure></li>
<li><p>填充每个节点的下一个右侧节点指针<br>BFS 广度优先遍历，利用 queue 来存储每一层的节点，然后遍历修改指针</p>
</li>
<li><p>有序数组的平方<br>这个最简单了，sorted + 列表生成式。</p>
</li>
<li><p>N皇后II<br>符合条件做选择 -&gt; backtrack 递归 -&gt; 撤销选择。N 皇后横竖斜不能选择转换为皇后可以放置在哪一列</p>
</li>
<li><p>删除链表的倒数第N个节点<br>快慢指针</p>
</li>
</ul>

        <h4 id="总结"   >
          <a href="#总结" class="heading-link"><i class="fas fa-link"></i></a>总结</h4>
      <p>简单的动归问题就是未剪枝后的递归</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">for status in status_list:</span><br><span class="line">	subproblem</span><br><span class="line">	base_statsu 与 subproblem 求最值</span><br></pre></td></tr></table></div></figure>

<p>链表相关，哑节点是个不错的选择</p>
<p>树相关，比如 BST 本质就是前序遍历，在遍历的过程中做一些操作。BFS 依靠队列</p>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>每周刷题202010_19_25</title>
    <url>/posts/94401c28/</url>
    <content><![CDATA[
        <h4 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h4>
      <p>本周碰到的最多的是处理字符串序列，处理链表，用到的最多的是双指针。目前对于很多边界条件，有点面向测试用例编程的感觉</p>
<p><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/backspace-string-compare/" >比较含退格的字符串</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/reorder-list/" >重排链表</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/lowest-common-ancestor-of-a-binary-tree/" >二叉树的最近公共祖先</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/long-pressed-name/" >长按键入</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/serialize-and-deserialize-binary-tree/" >二叉树的序列化与反序列化</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/combine-two-tables/" >组合两个表</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/second-highest-salary/" >第二高的薪水</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/partition-labels/" >划分字母区间</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/palindrome-linked-list/" >回文链表</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/video-stitching/" >视频拼接</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/nth-highest-salary/" >第N高的薪水</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/rank-scores/" >分数排名</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/longest-mountain-in-array/" >数组中的最长山脉</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<a id="more"></a>


        <h4 id="题解"   >
          <a href="#题解" class="heading-link"><i class="fas fa-link"></i></a>题解</h4>
      <ul>
<li><p>比较含退格的字符串<br>对字符串进行还原然后比较是否相等，这里的 # 代表删除符，如果碰到就删除前一个字符，对应的就是 s.pop。还要考虑字符删没了的情况，此时直接跳过，不在删除</p>
</li>
<li><p>重排链表<br>一碰到这个问题，就想到双指针和快慢指针，链表本身无法通过索引获取，所以先把每个节点放到数组里，然后开始双指针遍历。emmn，到写代码的时候双指针遍历就不太会了。。双指针要考虑边界问题和结束条件。这里还算简单，只要看着示例就能写出来。 L0→L1→…→Ln-1→Ln  =&gt;  L0→Ln→L1→Ln-1→L2→Ln-2→…</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="code"><pre><span class="line">i, j = <span class="number">0</span>, len(nodes) - <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> i &lt; j:</span><br><span class="line">	node[i].next = node[j]</span><br><span class="line">	i += <span class="number">1</span></span><br><span class="line">	<span class="keyword">if</span> i == j:</span><br><span class="line">		<span class="keyword">break</span></span><br><span class="line">   node[j].next = node[i]</span><br><span class="line">   j -= <span class="number">1</span></span><br><span class="line">node[i].next = <span class="literal">None</span>   		</span><br></pre></td></tr></table></div></figure>

<ul>
<li><p>二叉树的最近公共祖先](<span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/lowest-common-ancestor-of-a-binary-tree/" >https://leetcode-cn.com/problems/lowest-common-ancestor-of-a-binary-tree/</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>)<br>这个做过两遍了，一直没太看懂。大致意思，就是p, q 分别在左右子树两端，那么祖先肯定是 root</p>
</li>
<li><p>长按键入<br>双指针，确定判断指针移动的条件，确认边界</p>
<ul>
<li>确定指针移动条件，指向 name 的头指针i，指向 typed 的头指针j，指针位置值相等，</li>
<li>确认边界，i 指针不用管，只会小于等于 j 指针，当 j 指针到尾部时，遍历结束，或者在遍历的过程中发现一个字符不匹配，就 break 跳出循环</li>
</ul>
</li>
</ul>
</li>
<li><p>二叉树的序列化与反序列化<br>这个写的比较屎，只超过了 7% 的用户，用的 BFS 遍历，然后在还原回去，提交了多次，实现了真正的面向测试用例编程🤣。需要注意的是，序列化过程中，叶子节点会产生多余的 None</p>
</li>
<li><p><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/palindrome-linked-list/" >回文链表</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br>比较值的逆序与顺序元素，相等既为回文</p>
</li>
<li><p>视频拼接<br>动态规划，自底之上，1 至少需要多少个拼接，2从1的基础上至少需要多少个拼接</p>
</li>
<li><p>数组中的最长山脉<br>动态规划，山顶左侧单调递增，山顶右侧单调递减。每一个元素都有可能是山顶，所以需要求出其可以向左侧拓展多少，向右侧拓展多少</p>
</li>
</ul>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>每周刷题202010_26_1101</title>
    <url>/posts/8a77d52a/</url>
    <content><![CDATA[
        <h4 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h4>
      <p>每月从困难题开始到困难题结束。<br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/binary-tree-preorder-traversal/" >二叉树的前序遍历</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/unique-number-of-occurrences/" >独一无二的出现次数</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/consecutive-numbers/" >连续出现的数字</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/employees-earning-more-than-their-managers/" >超过经理收入的员工</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/duplicate-emails/" >查找重复的电子邮箱</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/customers-who-never-order/" >从不订购的客户</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/sum-root-to-leaf-numbers/" >求根到叶子节点数字之和</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/department-highest-salary/" >部门工资最高的员工</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/island-perimeter/" >岛屿的周长</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/insert-delete-getrandom-o1-duplicates-allowed/" >O(1) 时间插入、删除和获取随机元素 - 允许重复</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/word-break-ii/" >单词拆分 II</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<a id="more"></a>


        <h4 id="题解与思路"   >
          <a href="#题解与思路" class="heading-link"><i class="fas fa-link"></i></a>题解与思路</h4>
      <ul>
<li><p>二叉树的前序遍历<br>递归方式比较简单，容易理解，也是解决树相关问题最常用的方法。循环迭代的方式实际上就是我们自己实现递归出栈入栈。看的大佬的<span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/binary-tree-inorder-traversal/solution/yan-se-biao-ji-fa-yi-chong-tong-yong-qie-jian-ming/" >颜色标记法</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">def preorder(root):</span><br><span class="line">	res &#x3D; []</span><br><span class="line">	stack &#x3D; []</span><br><span class="line">	while (root or stack):</span><br><span class="line">		if root:</span><br><span class="line">			res.append(root.val)</span><br><span class="line">			stack.append(root)</span><br><span class="line">			root &#x3D; root.left</span><br><span class="line">		else:</span><br><span class="line">			root &#x3D; stack.pop().right</span><br><span class="line">	return res			</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>
</li>
<li><p>独一无二的出现次数<br>比较简单，count 统计，去重比较长度即可</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">def uniqueOccurrences(arr) -&gt; bool:</span><br><span class="line">    counts &#x3D; [arr.count(i) for i in set(arr)]</span><br><span class="line">    return len(counts) &#x3D;&#x3D; len(set(counts))</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>
</li>
<li><p>求根到叶子节点数字之和<br>dfs，每个根节点到叶子节点组成的数字值可以看做是父节点的数字 * 10 + 当前节点的 value。</p>
</li>
<li><p>岛屿的周长<br>前提条件是岛中不存在湖，从(0, 0) 开始遍历，假设开始处就是岛屿，那么周长加 4，如果其下边或者右边也是岛屿，会合并掉两条边，所以岛屿周长需要减 2。以此往复，遍历所有网格。</p>
</li>
<li><p>O(1) 时间插入、删除和获取随机元素<br>使用 list 维护元素列表，使用 dict 维护元素值索引。list append O(1) 时间复杂度插入，通过将要删除的元素与列表最后一个元素相交换 + pop 来达到 O(1) 删除元素</p>
</li>
<li><p>单词拆分 II</p>
</li>
</ul>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>WIP</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>每周刷题202011_02_14（WIP）</title>
    <url>/posts/baa69a19/</url>
    <content><![CDATA[
        <h4 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h4>
      <p>最近做的都点迷糊。。。打算把刷题作为一种习惯，其实也是熟悉各种数据结构的一种方法<br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/intersection-of-two-arrays/" >两个数组的交集</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/valid-mountain-array/" >有效的山脉数组</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/insert-interval/" >插入区间</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/word-ladder/" >单词接龙</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/sort-integers-by-the-number-of-1-bits/" >根据数字二进制下 1 的数目排序</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/count-of-range-sum/" >区间和的个数</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-ii/" >买卖股票的最佳时机 II</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/get-maximum-in-generated-array/" >获取生成数组中的最大值</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<a id="more"></a>
<p><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/minimum-deletions-to-make-character-frequencies-unique/" >字符频次唯一的最小删除次数</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/k-closest-points-to-origin/" >最接近原点的 K 个点</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/next-permutation/" >下一个排列</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/sort-array-by-parity-ii/" >按奇偶排序数组 II</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/odd-even-linked-list/" >奇偶链表</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h4 id="题解与想法"   >
          <a href="#题解与想法" class="heading-link"><i class="fas fa-link"></i></a>题解与想法</h4>
      ]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>WIP</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>每日一题-20200728</title>
    <url>/posts/ba6e3336/</url>
    <content><![CDATA[
        <h3 id="题目"   >
          <a href="#题目" class="heading-link"><i class="fas fa-link"></i></a>题目</h3>
      <p>给定一个二叉树，返回树深</p>
<a id="more"></a>


        <h3 id="解答"   >
          <a href="#解答" class="heading-link"><i class="fas fa-link"></i></a>解答</h3>
      <p>递归解决，取左子树，右子树中的最大高度 + 1 即为所求结果</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxDepth</span></span>(root: <span class="type">TreeNode</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">null</span>) &#123; <span class="number">0</span> &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            math.max(maxDepth(root.left), maxDepth(root.right)) + <span class="number">1</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>时间复杂度分析：O(n)，每个节点都遍历一次<br>空间复杂度分析：O(height)，递归需要栈空间，栈空间取决于递归的深度。</p>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>每日一题-2020729</title>
    <url>/posts/5e8578a2/</url>
    <content><![CDATA[
        <h3 id="题目"   >
          <a href="#题目" class="heading-link"><i class="fas fa-link"></i></a>题目</h3>
      <p>给定一个正整数 n，将其拆分为至少两个正整数的和，并使这些整数的乘积最大化。 返回你可以获得的最大乘积。</p>
<a id="more"></a>


        <h3 id="题解"   >
          <a href="#题解" class="heading-link"><i class="fas fa-link"></i></a>题解</h3>
      <p>其实就是剪绳子问题换了个样子。动态规划解决</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">integerBreak</span></span>(n: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> array = <span class="type">Array</span>.ofDim[<span class="type">Int</span>](n + <span class="number">1</span>)</span><br><span class="line">        array(<span class="number">1</span>) = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">2</span> until n+<span class="number">1</span>) &#123;</span><br><span class="line">          <span class="keyword">for</span> (j &lt;- <span class="number">1</span> until i) &#123;</span><br><span class="line">            array(i) = <span class="type">Math</span>.max(array(i), <span class="type">Math</span>.max(j * (i - j),  j * array(i - j)))</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        array(n)</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>每日一题-2020803</title>
    <url>/posts/873ab403/</url>
    <content><![CDATA[
        <h3 id="题目"   >
          <a href="#题目" class="heading-link"><i class="fas fa-link"></i></a>题目</h3>
      <p>给定两个字符串形式的非负整数 num1 和num2 ，计算它们的和。</p>
<a id="more"></a>
<p>注意：</p>
<ul>
<li>num1 和num2 的长度都小于 5100.</li>
<li>num1 和num2 都只包含数字 0-9.</li>
<li>num1 和num2 都不包含任何前导零。<br>你不能使用任何內建 BigInteger 库， 也不能直接将输入的字符串转换为整数形式。</li>
</ul>

        <h4 id="题解"   >
          <a href="#题解" class="heading-link"><i class="fas fa-link"></i></a>题解</h4>
      <p>双指针，每一位置相加可以看成由 x, y, carry 进位组成。<br>时间复杂度 O(max(num1.length, num2.length))<br>空间复杂度 O(n)</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addStrings</span></span>(num1: <span class="type">String</span>, num2: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">        <span class="keyword">var</span> i = num1.length - <span class="number">1</span></span><br><span class="line">        <span class="keyword">var</span> j = num2.length <span class="number">-1</span></span><br><span class="line">        <span class="keyword">var</span> carry = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">var</span> ans = <span class="type">Seq</span>[<span class="type">Int</span>]()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (i &gt;= <span class="number">0</span> || j &gt;=<span class="number">0</span> || carry !=<span class="number">0</span>) &#123;</span><br><span class="line">          <span class="keyword">val</span> x = <span class="keyword">if</span> (i &lt; <span class="number">0</span>) <span class="number">0</span> <span class="keyword">else</span> num1(i) - &#x27;<span class="number">0</span>&#x27;</span><br><span class="line">          <span class="keyword">val</span> y = <span class="keyword">if</span> (j &lt; <span class="number">0</span>) <span class="number">0</span> <span class="keyword">else</span> num2(j) - &#x27;<span class="number">0</span>&#x27;</span><br><span class="line">          <span class="keyword">val</span> result = x + y + carry</span><br><span class="line">          carry = result / <span class="number">10</span></span><br><span class="line">          ans = ans :+ result % <span class="number">10</span></span><br><span class="line">          i -= <span class="number">1</span></span><br><span class="line">          j -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ans.reverse.mkString(<span class="string">&quot;&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>每日一题-2020804</title>
    <url>/posts/195e21a0/</url>
    <content><![CDATA[
        <h3 id="题目"   >
          <a href="#题目" class="heading-link"><i class="fas fa-link"></i></a>题目</h3>
      <p>你这个学期必须选修 numCourse 门课程，记为 0 到 numCourse-1 。<br>在选修某些课程之前需要一些先修课程。 例如，想要学习课程 0 ，你需要先完成课程 1 ，我们用一个匹配来表示他们：[0,1]<br>给定课程总量以及它们的先决条件，请你判断是否可能完成所有课程的学习？</p>
<a id="more"></a>


        <h3 id="题解"   >
          <a href="#题解" class="heading-link"><i class="fas fa-link"></i></a>题解</h3>
      <p>可以理解为课程图求拓补排序，如果存在拓补排序则证明可以学完所有课程，存在环，则无法学完。 首先学习下<span class="exturl"><a class="exturl__link"   href="https://www.cnblogs.com/liushang0419/archive/2011/05/06/2039386.html" >图的表示法</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>。</p>
<p>入度表解决，从入度为0的点开始遍历课程图，如果入度为0，加入队列，需要学习的课程数目 -1，相邻节点课程入度 -1。重复此过程，直到遍历完课程图。<br>遍历完后，如果需要学习课程数为0，则证明可以学完所有课程，课程图不存在环。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">canFinish</span>(<span class="params">self, numCourses: int, prerequisites: List[List[int]]</span>) -&gt; bool:</span></span><br><span class="line">        indegrees = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(numCourses)]</span><br><span class="line">        adjacency = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(numCourses)]</span><br><span class="line">        queue = deque()</span><br><span class="line">        <span class="comment"># Get the indegree and adjacency of every course.</span></span><br><span class="line">        <span class="keyword">for</span> cur, pre <span class="keyword">in</span> prerequisites:</span><br><span class="line">            indegrees[cur] += <span class="number">1</span></span><br><span class="line">            adjacency[pre].append(cur)</span><br><span class="line">        <span class="comment"># Get all the courses with the indegree of 0.</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(indegrees)):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> indegrees[i]: queue.append(i)</span><br><span class="line">        <span class="comment"># BFS TopSort.</span></span><br><span class="line">        <span class="keyword">while</span> queue:</span><br><span class="line">            pre = queue.popleft()</span><br><span class="line">            numCourses -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> cur <span class="keyword">in</span> adjacency[pre]:</span><br><span class="line">                indegrees[cur] -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> indegrees[cur]: queue.append(cur)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">not</span> numCourses</span><br></pre></td></tr></table></div></figure>

]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>每日一题-2020805</title>
    <url>/posts/6e591136/</url>
    <content><![CDATA[
        <h3 id="题目"   >
          <a href="#题目" class="heading-link"><i class="fas fa-link"></i></a>题目</h3>
      <p><span class="exturl"><a class="exturl__link"   href="https://leetcode-cn.com/problems/house-robber-iii/solution/da-jia-jie-she-iii-by-leetcode-solution/" >打家劫舍</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<a id="more"></a>

        <h3 id="题解"   >
          <a href="#题解" class="heading-link"><i class="fas fa-link"></i></a>题解</h3>
      <p>动态规划，主要是找到子问题。每个节点都有选择与不选择两种情况，记选择为 f，不选择为。</p>
<p>选择父节点，则不能选择该节点的孩子节点。则最优解为 <code>f(o) = g(l) + g(r)</code><br>不选择该节点，则可以选择孩子节点，孩子节点同样有两种情况，取 <code>Max(f(l), g(l)) + Max(f(r), g(r))</code></p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">steal</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span>(<span class="params">var value:<span class="type">Int</span>, var left: <span class="type">TreeNode</span>, var right:<span class="type">TreeNode</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">def</span> <span class="title">rob</span>(<span class="params">root: <span class="type">TreeNode</span></span>)</span>: <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> f = <span class="type">Map</span>[<span class="type">TreeNode</span>, <span class="type">Int</span>]()</span><br><span class="line">    <span class="keyword">var</span> g = <span class="type">Map</span>[<span class="type">TreeNode</span>, <span class="type">Int</span>]()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dfs</span></span>(node:<span class="type">TreeNode</span>):<span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span> (node == <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">      &#125;</span><br><span class="line">      dfs(node.left)</span><br><span class="line">      dfs(node.right)</span><br><span class="line">      <span class="keyword">val</span> fl = f.getOrElse(node.left, <span class="number">0</span>)</span><br><span class="line">      <span class="keyword">val</span> fr = f.getOrElse(node.right, <span class="number">0</span>)</span><br><span class="line">      <span class="keyword">val</span> gl = g.getOrElse(node.left, <span class="number">0</span>)</span><br><span class="line">      <span class="keyword">val</span> gr = g.getOrElse(node.right, <span class="number">0</span>)</span><br><span class="line">      f = f + (node -&gt; (node.value + gl + gr))</span><br><span class="line">      g = g + (node -&gt; (scala.math.max(fl, gl) +  scala.math.max(fr, gr)))</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    dfs(root)</span><br><span class="line">    scala.math.max(f.getOrElse(root, <span class="number">0</span>), g.getOrElse(root, <span class="number">0</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> tree = <span class="type">TreeNode</span>(<span class="number">3</span>, <span class="type">TreeNode</span>(<span class="number">3</span>, <span class="literal">null</span>, <span class="literal">null</span>), <span class="type">TreeNode</span>(<span class="number">4</span>, <span class="type">TreeNode</span>(<span class="number">4</span>, <span class="literal">null</span>, <span class="literal">null</span>), <span class="literal">null</span>))</span><br><span class="line"></span><br><span class="line">    println(rob(tree))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>循序渐进学 Spark</title>
    <url>/posts/7caf1424/</url>
    <content><![CDATA[
        <h4 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h4>
      <p>讲一下我个人是从哪些方面来学习 Spark SQL 的？一句话就是自顶向下，逐步下探。</p>
<a id="more"></a>


        <h4 id="整体架构"   >
          <a href="#整体架构" class="heading-link"><i class="fas fa-link"></i></a>整体架构</h4>
      <p>类似于大多数的大数据处理框架，宏观上 Spark JOB 的提交运行主要和三种角色有关，客户端，集群管理器，工作节点。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/00831rSTly1gd3wm9z12rj30gk07ydg6.jpg"></p>
<p>Cluster Manager：在 standalone 模式中即为 Master 主节点，控制整个集群，监控 worker。在 YARN 模式中为资源管理器。</p>
<p>Worker节点：从节点，负责控制计算节点，启动 Executor 或者 Driver。</p>
<p>Driver： 运行 Application 的 main() 函数。</p>
<p>Executor：执行器，某个 Application 运行在 worker node 上的一个进程。运行 task 和 管理运行过程中的数据存储</p>
<p>Spark 应用程序作为独立的进程集运行在集群上，通过 Driver  Program 中的 SparkContext 对象来进行调度。一旦连接上 Cluster Manager（YARN，Spark 自带的 Standalone Cluster），Spark 就会在对应的 Worker 上启动 executor 进程用于计算和存储应用程序运行所需要的数据。接着你的应用程序代码会被发送到各个 executor 。SparkContext 会调度生成 task 在 executor 进程中执行。</p>

        <h4 id="模块"   >
          <a href="#模块" class="heading-link"><i class="fas fa-link"></i></a>模块</h4>
      <p>一个 Spark SQL Appliaction 是怎么被提交运行的呢，其中主要涉及到哪些模块呢。<br>一个 Spark 应用程序运行，首先会经过 BlockManager 和 BroadCastManager 做一些 Hadoop 配置或者变量的广播，然后由 DAGScheduler 将任务转换为 RDD 并组织成 DAG，DAG 还将被划分为不同的 Stage。最后由TaskScheduler 借助 ActorSystem 将任务提交给集群管理器（Cluster Manager）。如果存在 shuffle 过程，其存管理主要会涉及到 ShuffleBlockManager 。集群管理器分配资源，对应的 Worker 节点上启动 Executor 进程运行 task。<br>可以通过一个 action 算子的触发去读源码，看这些模块是如何实现的。</p>

        <h4 id="SQL-执行"   >
          <a href="#SQL-执行" class="heading-link"><i class="fas fa-link"></i></a>SQL 执行</h4>
      <p>一条 SQL 的执行会经过哪些阶段，其实用过 MySQL 就知道，解析执行计划，分析执行计划，优化执行计划，物理执行计划，代码生成。SQL on Hadoop 其实也是一样的。<br>以下是一个典型执行计划的示例</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">== <span class="type">Parsed</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line"><span class="symbol">&#x27;Filter</span> ((<span class="symbol">&#x27;url_name</span> = <span class="number">121119</span>) &amp;&amp; (<span class="symbol">&#x27;score</span> &gt;= <span class="number">3.6</span>))</span><br><span class="line">+- <span class="type">Project</span> [url_name#<span class="number">3</span>, score#<span class="number">11</span>]</span><br><span class="line">   +- <span class="type">RelationV2</span> <span class="type">DefaultSource</span>[created_time#<span class="number">0</span>, created_time_ts#<span class="number">1</span>L, img_url#<span class="number">2</span>, url_name#<span class="number">3</span>, fangyuan#<span class="number">4</span>, <span class="class"><span class="keyword">type</span><span class="title">#5</span>, <span class="title">id#6</span>, <span class="title">city#7</span>, <span class="title">tags#8</span>, <span class="title">url#9</span>, <span class="title">name#10</span>, <span class="title">score#11</span>, <span class="title">is_direct_sell#12</span>, <span class="title">project_address#13</span>, <span class="title">alias#14</span>, <span class="title">the_main_unit#15</span>, <span class="title">recently_opened#16</span>, <span class="title">img_links#17</span>, <span class="title">img_srcs#18</span>, <span class="title">detail_url#19</span>, <span class="title">huxing_url#20</span>, <span class="title">dianping_url#21</span>, <span class="title">notes#22</span>, <span class="title">team_buy#23</span>, ... 81 <span class="title">more</span> <span class="title">fields</span>] (<span class="params"><span class="type">Options</span>: [dbtable=newfangdetail,driver=com.mysql.jdbc.<span class="type">Driver</span>,url=*********(redacted</span>),<span class="title">paths=</span>[]])</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">==</span> <span class="title">Analyzed</span> <span class="title">Logical</span> <span class="title">Plan</span> <span class="title">==</span></span></span><br><span class="line"><span class="class"><span class="title">url_name</span></span>: string, score: string</span><br><span class="line"><span class="type">Filter</span> ((url_name#<span class="number">3</span> = <span class="number">121119</span>) &amp;&amp; (score#<span class="number">11</span> &gt;= <span class="number">3.6</span>))</span><br><span class="line">+- <span class="type">Project</span> [url_name#<span class="number">3</span>, score#<span class="number">11</span>]</span><br><span class="line">   +- <span class="type">RelationV2</span> <span class="type">DefaultSource</span>[created_time#<span class="number">0</span>, created_time_ts#<span class="number">1</span>L, img_url#<span class="number">2</span>, url_name#<span class="number">3</span>, fangyuan#<span class="number">4</span>, <span class="class"><span class="keyword">type</span><span class="title">#5</span>, <span class="title">id#6</span>, <span class="title">city#7</span>, <span class="title">tags#8</span>, <span class="title">url#9</span>, <span class="title">name#10</span>, <span class="title">score#11</span>, <span class="title">is_direct_sell#12</span>, <span class="title">project_address#13</span>, <span class="title">alias#14</span>, <span class="title">the_main_unit#15</span>, <span class="title">recently_opened#16</span>, <span class="title">img_links#17</span>, <span class="title">img_srcs#18</span>, <span class="title">detail_url#19</span>, <span class="title">huxing_url#20</span>, <span class="title">dianping_url#21</span>, <span class="title">notes#22</span>, <span class="title">team_buy#23</span>, ... 81 <span class="title">more</span> <span class="title">fields</span>] (<span class="params"><span class="type">Options</span>: [dbtable=newfangdetail,driver=com.mysql.jdbc.<span class="type">Driver</span>,url=*********(redacted</span>),<span class="title">paths=</span>[]])</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">==</span> <span class="title">Optimized</span> <span class="title">Logical</span> <span class="title">Plan</span> <span class="title">==</span></span></span><br><span class="line"><span class="class"><span class="title">Project</span> [url_name#3, score#11]</span></span><br><span class="line"><span class="class"><span class="title">+-</span> <span class="title">Filter</span> (<span class="params">((isnotnull(url_name#3</span>) <span class="title">&amp;&amp;</span> <span class="title">isnotnull</span>(<span class="params">score#11</span>)) <span class="title">&amp;&amp;</span> (<span class="params">url_name#3 = 121119</span>)) <span class="title">&amp;&amp;</span> (<span class="params">score#11 &gt;= 3.6</span>))</span></span><br><span class="line"><span class="class">   <span class="title">+-</span> <span class="title">RelationV2</span> <span class="title">DefaultSource</span>[created_time#0, created_time_ts#1L, img_url#2, url_name#3, fangyuan#4, type#5, id#6, city#7, tags#8, url#9, name#10, score#11, is_direct_sell#12, project_address#13, alias#14, the_main_unit#15, recently_opened#16, img_links#17, img_srcs#18, detail_url#19, huxing_url#20, dianping_url#21, notes#22, team_buy#23, ... 81 more fields] (<span class="params"><span class="type">Options</span>: [dbtable=newfangdetail,driver=com.mysql.jdbc.<span class="type">Driver</span>,url=*********(redacted</span>),<span class="title">paths=</span>[]])</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">==</span> <span class="title">Physical</span> <span class="title">Plan</span> <span class="title">==</span></span></span><br><span class="line"><span class="class"><span class="title">*</span>(<span class="params">1</span>) <span class="title">Project</span> [url_name#3, score#11]</span></span><br><span class="line"><span class="class"><span class="title">+-</span> <span class="title">*</span>(<span class="params">1</span>) <span class="title">Filter</span> (<span class="params">score#11 &gt;= 3.6</span>)</span></span><br><span class="line"><span class="class">   <span class="title">+-</span> <span class="title">*</span>(<span class="params">1</span>) <span class="title">ScanV2</span> <span class="title">DefaultSource</span>[url_name#3, score#11] (<span class="params"><span class="type">Filters</span>: [isnotnull(url_name#3</span>), <span class="title">isnotnull</span>(<span class="params">score#11</span>), (<span class="params">url_name#3 = 121119</span>)], <span class="title">Options</span></span>: [dbtable=newfangdetail,driver=com.mysql.jdbc.<span class="type">Driver</span>,url=*********(redacted),paths=[]])</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>
<ul>
<li><p>解析执行计划<br>Spark SQL Parser 负责解析执行计划到 unresolved plan，这个阶段解析出来的执行计化可能没有数据源来自哪里，字段属性等信息，比如 unsolverelation</p>
</li>
<li><p>分析执行计划<br>Spark SQL Analysr 利用 catlog 元数据信息将 unresolved plan 依据一些 rule 生成 Analyzed logical plan，其实就是一颗完整的 SQL 语法树，到这一步其实我们已经知道了数据来自哪里，属性等元数据信息。</p>
</li>
<li><p>优化执行计划<br>Catalyst Optimization 负责优化执行计划。主要分为 RBO 和 CBO，默认是关闭了 CBO。常见的 RBO 有谓词下推，列裁剪，常量替换，常量累加。这一部其实做的就是上步分析过程生成这棵 SQL 语法树。<br>如果看过执行计划，其实可以经常看到谓词下推，比如 filiter 算子被下推到数据源端。<br>为什么默认关闭了 CBO，个人的理解是，RBO 已经满足了大部分效率方面的需求，并且 CBO 本身也需要收集统计信息进行代价计算，这也是有代价的。故应该根据自己的场景来判断是否开启。</p>
</li>
<li><p>物理执行计划<br>经过 RBO，CBO 之后，会选出相对最优的执行计划作为最终执行的计划。</p>
</li>
<li><p>代码生成<br>物理执行计划生成后，需要通过代码生成阶段生成类似手写代码运行计算。这也是在 DAG 图里常见的 WholeStageCodegen</p>
</li>
</ul>

        <h4 id="DAG-图"   >
          <a href="#DAG-图" class="heading-link"><i class="fas fa-link"></i></a>DAG 图</h4>
      <p>物理执行计划的展现，更直观。这里只提几个问题，</p>
<ul>
<li>通过物理执行图，你知道是怎么被切分 stage 的吗？</li>
<li>Shuffle Dependency 是怎么被 attach 到执行图当中的</li>
<li>执行图中的各种类型的 RDD 到底是什么意思，像 MapPartitionRDD 等，什么情况下会生成他？</li>
</ul>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>漫谈数据库索引</title>
    <url>/posts/83cc46f2/</url>
    <content><![CDATA[
        <h4 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h4>
      <p>为什么要了解数据索引呢？我们都知道索引结构一般会加速查询，那这些索引是如何加速查询的呢？我们有时候建立了索引，但是查询比没建索引都要慢，这又是为什么呢？我们有时候会根据不同的业务场景选择不同的索引类型，选择的依据又是什么呢？</p>
<p>带着以上几个疑惑，让我们开始了解数据库索引到底是啥，它的底层数据结构是什么样子的，是怎样加速查询的，索引本身又做了哪些优化。希望读完本文后，可以为你解决上述存留在心中的疑问。</p>
<p>在这里也顺便提下个人对大数据的理解，大数据其实就是海量数据的存储和检索，前者关注于如何更高效的把数据放到存储介质上，后者关注于如何更高效的把数据从存储介质上检索出来，需要注意这里的检索指的是检查数据是否存在，如果数据存在则返回。而索引又是检索的核心，了解索引之后会对理解整个检索过程有更深刻的理解，比如执行计划的解析，join order 等等的影响。</p>
<a id="more"></a>


        <h4 id="数组和链表"   >
          <a href="#数组和链表" class="heading-link"><i class="fas fa-link"></i></a>数组和链表</h4>
      <p>数据和链表作为最基础的数据结构，往往也是被容易忽视的。但是却又非常重要，可以说我们接触的大部分索引结构都是由数组和链表结合而成的，比如位图本身就是以 bit 为单位的数据，再比如二叉查找树，红黑树，调表本身就是每个节点带多个指针的链表，再比如更高阶的 B+ 树，就是一个多叉树（带多个指针的链表），其每个节点又是一个数组，每层节点之间又会互相引用形成双向链表，再比如…….。</p>
<p>可以看到就算在复杂的数据结构，其基本单元依然是数组和链表，只有了解数组和链表本身的特性以及查询原理才能更深入了解高阶的数据结构是如何加速查询的，以及如何从低阶数据结构转向高阶数据结构。</p>
<p>首先看下数组，你认为的数组是什么呢，是存放元素的容器还是仅仅简单的列表。都不是，数组只是开辟在内存区域上的一块连续空间，有时候甚至不是物理连续的，但是一定具有逻辑连续性。因为在内存上具有连续性，所以数组具有随机访问的特性，并且由于内存局部性原理，在小数据量下具有更高的查询性能。</p>
<p>tips：一般加载运行程序都是从外设中加载到内存，然后 cpu 在从内存中取出相应数据放到寄存器中，而现代 pc 一般都会有多层寄存器缓存，为了加速 cpu 运算，一般会把相邻内存空间区域下的数据也加载放入到缓存中。这样的话，下次则不需要再访问这些内存空间中的数据，而是直接加载 cpu 缓存即可。而数组正是应用了这个原理来实现的查询加速。</p>
<p>数组在有序情况下，因为随机访问的特性，可以方便的利用二分查找来定位数据，这也是为何绝大多数查询，计算引擎（比如 mapreduce）为啥会有默认分区排序的原因。</p>
<p>既然数组已经这么好了，为啥还需要链表呢？让我们思考这样的场景，数组放不下了该怎么办，答案是需要重新开辟空间，复制数据。再数据高频写入场景下，采用数组来存放数据显然不行，重新申请内存的开销太大。还有有序数据插入删除数据的时候，更不适合。这时候就需要链表出场了，链表通过指针的方式来连接每个元素，当需要插入删除时只需要修改对应节点的前后两个指针即可，极大的降低了增删改查的开销。但是链表的问题也随之而来，因为每个节点只能通过前置指针去访问，失去了随机访问的能力，即使在链表有序的情况下，也无法做到二分查找。</p>
<p>好了，我们现在总结一下。</p>
<p>数组随机访问，范围查询，基于内存的局部性原理在小数据规模下具有更良好的检索性能，但是插入删除开销比较大，尤其是在有序数组情况下。<br>链表增删方便，但是失去了随机访问能力，即使数据有序情况下，也无法提高检索性能。</p>
<p>我们自己也可以思考下该如何改造链表使得其具有二分查找的能力呢？</p>

        <h4 id="二叉查找树（BST-→-RBT-amp-跳表）"   >
          <a href="#二叉查找树（BST-→-RBT-amp-跳表）" class="heading-link"><i class="fas fa-link"></i></a>二叉查找树（BST → RBT &amp; 跳表）</h4>
      <p>基于上一节中的留给我们的问题，对链表进行了改造。也就是我们常说的 BST（二叉查找树），那到底是如何改造的呢。想想数组的二分查找是如何做的呢，先找到中间节点，在折半数组中接着找中间节点，直到定位到目标值或者遍历完为止。那链表该如何快速定位到中间节点呢，一个办法是每个节点除了指向下一个节点的指针，在额外增加一个指向中间节点的指针，那到了中间节点指针之后，还需要知道中间节点左右两边的中间节点指针（ps：此处有点绕口），那么中间节点也需要有前向指针。好了，到此处，我们应该明白了，为链表每个节点设置两个指针，左指针元素小于 &lt; 中间节点 &lt; 右指针元素。emmm，这样一个可以二分查找的有序链表就出来了，可以看到 BST 本身就是链表。</p>
<p>通过上述链表改造，链表也可以二分查找了，具有更高效的检索能力了。但是所有情况下，BST 一定比普通链表查找快吗？非也，BST 在倾斜的情况下会退化成链表，所以为了解决倾斜问题，又衍生出来非常多的数据结构，比如红黑树（RBT），跳表，AVL 等。实际上就是在增删过程中应用一些规则尽可能的保持 BST 的平衡性。</p>
<p>这里可以说下红黑树和跳表。</p>
<p>红黑树大家应该相对熟悉，java 的 hashmap 就是数组+红黑树实现的，加入红黑树是为了解决大量哈希冲突下元素的检索性能。</p>
<p>跳表和红黑树解决平衡性问题的思路就大相径庭了，不过熟悉 es，redis 中的人应该知道，比如 es 的倒排索引就是用跳表来加速检索的。跳表可以理解为对普通链表加上了高速缓存，即每个节点存在有多个指针指向后续节点（每个节点的指针个数称之为层数，层数会通过概率函数随机生成，这也是为了保证跳表的指针分布更加均匀）。在大数据规模下，跳表的实现相对于红黑树这些更加简单，且也能实现 O(logn) 的检索性能。</p>
<p>这里在提个问题，跳表的增删节点怎么处理呢，毕竟每个节点都有可能指向同一个指针，难道需要全部指针都需要处理吗？</p>

        <h4 id="哈希表"   >
          <a href="#哈希表" class="heading-link"><i class="fas fa-link"></i></a>哈希表</h4>
      <p>哈希表来了，我们想起 O(1) 时间复杂度的数据结构，第一个想起的总会是哈希表。那么哈希表是如何做到 O(1) 时间复杂度的呢。哈希表顾明思议是先哈希后落表。针对 int 类型的数据，我们可以开辟一个数组，int 是啥就在数据的哪个位置上放置该元素，当查询时，只需要通过数据的随机访问能力就可以快速查询到对应值以及对应值是否存在。针对 string 类型的数据，我们可以做编码处理转换成 int 值（这个过程也叫散列），举个例子，Tom，我们就可以按照 26 个字母的顺序进行编码，类似于二进制计算的方式将一个字符串转换为数值类型。当然散列函数的方法是多种多样的，这里就不过多赘述了。</p>
<p>好了，现在我们已经明确哈希表就是数组实现的了。但是数组也是有限制的，不可能无限开辟连续内存的空间，因此一定会碰到哈希冲突。哈希冲突又有哪几种解决办法呢？开放寻址和链表法。</p>
<p>开放寻址法，在碰到哈希冲突后，继续向后移寻找位置，容易造成位置抢占。或者通过多个 hash 函数来处理。<br>链表法，在碰到哈希冲突后，直接将冲突元素加到冲突节点上，形成链表。缺点是哈希冲突过多后，o(1) 的时间复杂度会退化成 o(n)，因为需要遍历链表。因此有些 hashmap 的实现会将链表达到一定规模后再进化成红黑树。</p>
<p>哈希表具有 O(1) 的时间复杂度，看起来很美好，但是本质上是无序数组，失去了遍历能力，并且为了避免频繁的哈希冲突，一般还会预留额外空间（可以了解下装载因子），rehash 的代价也是比较大的。</p>
<p>从上我们也可以看出，hash 表适合用于检测数据存在的场景，这点和布隆过滤器类似，只不过布隆过滤器的压缩比更好。此处也可以联想到 hashshuffle 和 hashjoin，都是采用的这种结构。</p>
<p>问题又来了，哈希表的删除操作在开放寻址法和链表法下有啥不同呢？</p>

        <h4 id="布隆过滤器"   >
          <a href="#布隆过滤器" class="heading-link"><i class="fas fa-link"></i></a>布隆过滤器</h4>
      <p>有了哈希表的 o(1)，为何还需要布隆过滤器。在大规模数据量下，哈希表所占用的内存空间极大，比如向一些基于 lsm 的引擎，比如 doris，druid ，kafka 这些，往往需要预判数据，如果采用哈希表做预判的话则会占用大量内存空间。那么布隆过滤器是如何做压缩的呢？</p>
<p>一个数据结构的大小可以用（数据大小 * 数据个数） 来表达，布隆过滤器做的第一个优化就是该表数据大小，以 bit 为单位进行处理，因为往往我们做预判只是需要是否的概念，用0 和 1 就可以表达。第二个优化就是减少数据个数，利用 k 个哈希函数来对 k 个位置进行 bit 位赋值，这里的 k 的个数也是有一套算法在里面的。通过这样的方式极大大压缩了数据结构的大小。</p>
<p>选用 k 个哈希函数也带了一定预判误差率，因为有可能一个没有的元素的 k 个位置都被置为了 1 bit。</p>
<p>布隆过滤器常用于预判场景（在基于 lsm 引擎的 olap 中尤为常见），并且适合于高基维字段，这里可以自己思考下为什么适用于高基维呢？</p>

        <h4 id="位图"   >
          <a href="#位图" class="heading-link"><i class="fas fa-link"></i></a>位图</h4>
      
        <h4 id="倒排索引"   >
          <a href="#倒排索引" class="heading-link"><i class="fas fa-link"></i></a>倒排索引</h4>
      <p>倒排索引是啥？我们可以先想下正排索引是什么？举个例子类比，通过诗歌名找到诗歌是正排索引，通过诗歌内容找到包含所有该内容的诗歌名称是倒排索引。倒排索引是如何构建的。</p>
<p>倒排索引常用于搜索引擎，比如 mysql 中的全文搜索，es 中的检索，都有倒排索引的出现。</p>

        <h4 id="B-树-vs-LSM"   >
          <a href="#B-树-vs-LSM" class="heading-link"><i class="fas fa-link"></i></a>B+树 vs LSM</h4>
      <p>上面我们了解的很多数据结构，也可以说是索引都是放在内存中的（这里不严谨，也是可以落到磁盘的）。那么在大规模索引情况下，索引该如何下方到磁盘中，我们又该如何访问磁盘中的数据索引呢？B+ 树和 LSM 就是索引落盘的经典实现</p>

        <h4 id="一些实际问题"   >
          <a href="#一些实际问题" class="heading-link"><i class="fas fa-link"></i></a>一些实际问题</h4>
      
        <h4 id="小结"   >
          <a href="#小结" class="heading-link"><i class="fas fa-link"></i></a>小结</h4>
      <p>万丈高楼皆起于平地之间，技术如此，我们亦如此。</p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>数据索引</tag>
      </tags>
  </entry>
  <entry>
    <title>碰到的一些精彩文章</title>
    <url>/posts/2305d870/</url>
    <content><![CDATA[
        <h4 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h4>
      <p>有时候喜欢瞎看，碰到挺多不错的文章与视频，然后总是看完就忘，手动尴尬，或者看到一半就有其他事情了。这里放个文章链接的总集，便于找到重看。如有侵犯，请勿动怒，联系即删。</p>
<a id="more"></a>


        <h3 id="技术"   >
          <a href="#技术" class="heading-link"><i class="fas fa-link"></i></a>技术</h3>
      <ul>
<li><span class="exturl"><a class="exturl__link"   href="https://juejin.im/post/6844903695717498887" >一个小小的 Shell 管道符，内部实现可真不简单！</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><span class="exturl"><a class="exturl__link"   href="https://cloud.tencent.com/developer/column/2291/tag-10682" >腾讯云社区上的 Spark 系列文章</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
</ul>

        <h3 id="生活"   >
          <a href="#生活" class="heading-link"><i class="fas fa-link"></i></a>生活</h3>
      
        <h3 id="社会"   >
          <a href="#社会" class="heading-link"><i class="fas fa-link"></i></a>社会</h3>
      <ul>
<li><span class="exturl"><a class="exturl__link"   href="https://www.zhihu.com/question/393696749/answer/1389114190?utm_source=wechat_session&utm_medium=social&utm_oi=749926370829697024" >中国怎样走出内卷</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
</ul>

        <h3 id="历史"   >
          <a href="#历史" class="heading-link"><i class="fas fa-link"></i></a>历史</h3>
      
        <h3 id="搞笑"   >
          <a href="#搞笑" class="heading-link"><i class="fas fa-link"></i></a>搞笑</h3>
      
        <h3 id="书籍"   >
          <a href="#书籍" class="heading-link"><i class="fas fa-link"></i></a>书籍</h3>
      <p>Scala 实用指南<br>说话的魅力</p>

        <h3 id="财富"   >
          <a href="#财富" class="heading-link"><i class="fas fa-link"></i></a>财富</h3>
      ]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>文章集</tag>
      </tags>
  </entry>
  <entry>
    <title>磁盘与内存</title>
    <url>/posts/49ba6d75/</url>
    <content><![CDATA[
        <h3 id="程序的运行方式"   >
          <a href="#程序的运行方式" class="heading-link"><i class="fas fa-link"></i></a>程序的运行方式</h3>
      <p>磁盘中存储的应用程序必须加载到内存中才能执行，这是因为解析和运行程序的 CPU 需要通过程序计数器指定内存地址来读取程序指令的。</p>
<a id="more"></a>
<p><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1gek0j2l2ysj30m20gg3z2.jpg"></p>

        <h3 id="磁盘与内存"   >
          <a href="#磁盘与内存" class="heading-link"><i class="fas fa-link"></i></a>磁盘与内存</h3>
      <p>磁盘是用来存储应用程序和数据的，低速廉价，断电数据不会丢失，常见的就是硬盘。内存也是用来存放数据和应用程序的，是靠电流实现的，断电内存里面的数据会丢失，高速并且造价昂贵。有兴趣的可以看下内存 IC，其通过 IC 引脚来表示电源，数据位，地址位，读写标志位等数据，比如地址位有10个引脚，那么该内存 IC 就能存取 1KB 的数据。按照现在 G 级别的内存使用来看，一个内存的 IC 引脚有多少就不必多说了，毕竟我们不能搞这么多内存 IC，只能从添加引脚上下功夫了。</p>

        <h4 id="磁盘缓存"   >
          <a href="#磁盘缓存" class="heading-link"><i class="fas fa-link"></i></a>磁盘缓存</h4>
      <p>在开发 web 应用程序的过程中，我们经常用到缓存，用高速设备来存储经常会有到的数据。表现在磁盘缓存其实就是拿一部分内存作为磁盘，程序访问相应的数据时先从看看磁盘缓存中有没有，如果缓存命中，那么直接读内存，就不用读低速度的磁盘了。听上面这么一说和内核缓冲区有点相似。</p>

        <h4 id="虚拟内存"   >
          <a href="#虚拟内存" class="heading-link"><i class="fas fa-link"></i></a>虚拟内存</h4>
      <p>计算机中运行的进程是非常多的，我们往往看到内存只剩下几百M的时候，也可以运行占用内存G级别的程序，操作系统并没有因为内存不足而崩溃，这是为什么呢？其实就是虚拟内存，把磁盘的一部分划分给内存使用。copy 一段话</p>
<p>虚拟内存是计算机系统内存管理的一种技术。它使得应用程序认为它拥有连续可用的内存（一个完整的地址空间），但是实际上，它通常被分割成多个物理碎片，还有部分存储在外部磁盘管理器上，必要时进行数据交换。</p>
<p>说到了数据交换，虚拟内存是如何与内存进行数据交换的呢？有分页和分段2种，具体还要在看看</p>

        <h4 id="如何节约内存"   >
          <a href="#如何节约内存" class="heading-link"><i class="fas fa-link"></i></a>如何节约内存</h4>
      <p>上面说到了内存高速但是昂贵，那么该如何节省内存资源呢？</p>
<ul>
<li>上面说到的虚拟内存是一种办法，但是其涉及到低速的磁盘访问和数据交换。</li>
<li>想想上面讲到的东西，内存是用来干什么的，加载应用程序和数据的，内存的物理表现形式就是内存IC，所以我们有2个方面来节约内存，一减小程序大小，二增加 IC 引脚，加内存。加内存涉及到 money，还是说说我们是怎么减小程序的大小的吧</li>
<li>stdcall 标准调用来减少内存的使用，具体是怎么个情况呢？我们在编写程序时，往往会提供一些公共函数用于在多个地方重复调用。在调用函数完成的时候，需要清理函数参数以及所处理的数据的栈区域。难道我们每次调用函数都需要栈清理吗？ _stdcall 就是这样的东西，只在我们反复调用该函数的地方触发栈清理，而不用每次这样做，通过减少栈清理的开销来减小程序运行所需要的内存</li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>算法是什么</title>
    <url>/posts/9104473a/</url>
    <content><![CDATA[
        <h2 id="算法"   >
          <a href="#算法" class="heading-link"><i class="fas fa-link"></i></a>算法</h2>
      
        <h3 id="算法是用来干什么的？"   >
          <a href="#算法是用来干什么的？" class="heading-link"><i class="fas fa-link"></i></a>算法是用来干什么的？</h3>
      <p>算法是用来处理数据，解决问题的一组方法。对于同一个问题可以使用不同的算法来解决，但是其中消耗的时间和资源却可能不一样。</p>

        <h3 id="算法的优劣与否？"   >
          <a href="#算法的优劣与否？" class="heading-link"><i class="fas fa-link"></i></a>算法的优劣与否？</h3>
      <p>一般会通过算法的时间复杂度（程序耗时）和空间复杂度（程序所占用的内存）来衡量算法的优劣</p>
<a id="more"></a>


        <h2 id="时间复杂度"   >
          <a href="#时间复杂度" class="heading-link"><i class="fas fa-link"></i></a>时间复杂度</h2>
      
        <h3 id="时间复杂度是什么？"   >
          <a href="#时间复杂度是什么？" class="heading-link"><i class="fas fa-link"></i></a>时间复杂度是什么？</h3>
      <p>时间复杂度用来反映程序的运行时间。常用的表示方法是大O表示法，即 T(n) = O(f(n))。可以通过下面的一段代码来解释下</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">for (i &lt;- 1 until n)&#123;</span><br><span class="line">    j &#x3D; i</span><br><span class="line">    j +&#x3D; 1</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></div></figure>
<p>在大O表示法中，f(n)表示每行代码的执行次数之和，O 表示正比例关系。所以上面的代码的时间复杂度就可以根据代码执行次数来看。for 循环执行n次，循环体里面的每行语句执行n次，所以执行次数是 3n，即时间复杂度是 O(3n)，当 n<br>无穷大时，就可以表示为 O(n)。下面介绍下常见的时间复杂度量级.</p>
<p>tip: 大O表示法主要用来表示程序的执行时间趋势，并不要求精准，因此上面的 O(3n) 可以变为 O(n)。</p>

        <h3 id="常数阶-O-1"   >
          <a href="#常数阶-O-1" class="heading-link"><i class="fas fa-link"></i></a>常数阶 O(1)</h3>
      <figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">i &#x3D; 1</span><br></pre></td></tr></table></div></figure>
<p>i = 1 只执行一次，时间复杂度 O(1)</p>

        <h3 id="线性阶-O-n"   >
          <a href="#线性阶-O-n" class="heading-link"><i class="fas fa-link"></i></a>线性阶 O(n)</h3>
      <p>上面提到的单层 for 循环的时间复杂度就是 O(n)</p>

        <h3 id="对数阶-O-logN"   >
          <a href="#对数阶-O-logN" class="heading-link"><i class="fas fa-link"></i></a>对数阶 O(logN)</h3>
      <figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">i &#x3D; 1</span><br><span class="line">while(i &lt; n)</span><br><span class="line">    i &#x3D; i * 2</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>
<p>while 循环中，i 经过 log2n(2为底数)次大于 n 并结束循环。时间复杂度为 O(logN)</p>

        <h3 id="线性对数阶-O-n-logN"   >
          <a href="#线性对数阶-O-n-logN" class="heading-link"><i class="fas fa-link"></i></a>线性对数阶 O(n * logN)</h3>
      <p>就是 O(logN) 的代码循环执行了 n 次，比如下面的代码</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> until n)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">while</span>(i &lt; n)</span><br><span class="line">    &#123;</span><br><span class="line">        i = i * <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>



        <h3 id="k次方阶"   >
          <a href="#k次方阶" class="heading-link"><i class="fas fa-link"></i></a>k次方阶</h3>
      <p>像 O(n2), O(n3), O(nK)</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(x &lt;- <span class="number">1</span> until n)</span><br><span class="line">&#123;</span><br><span class="line">   <span class="keyword">for</span>(i &lt;- <span class="number">1</span> until n)</span><br><span class="line">    &#123;</span><br><span class="line">       j = i;</span><br><span class="line">       j += <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>


        <h3 id="最好，最坏，平均情况时间复杂度"   >
          <a href="#最好，最坏，平均情况时间复杂度" class="heading-link"><i class="fas fa-link"></i></a>最好，最坏，平均情况时间复杂度</h3>
      <figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find</span></span>(seq:<span class="type">Seq</span>[<span class="type">String</span>], x:<span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="keyword">var</span> pos = <span class="number">0</span></span><br><span class="line">    <span class="keyword">import</span> scala.util.control.<span class="type">Breaks</span>.&#123;<span class="keyword">break</span>, breakable&#125;</span><br><span class="line">    breakable &#123;</span><br><span class="line">        <span class="keyword">for</span>(i &lt;- <span class="number">0</span> until seq.length)&#123;</span><br><span class="line">            <span class="keyword">if</span> (seq(i) == x)&#123; pos = i; <span class="keyword">break</span> &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    pos </span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>
<p>find 函数用来寻找指定元素在数组中的位置，时间复杂度是和数组的长度 n 有关。根据上面的讲解我们可以判定为 O(n)。但是事实上情况并不是这样，我们要找的数据元素可能在数组开头，数组结尾，甚至根本没有我们要找的东西。<br>对应种种不同情况，由此引出了最好，最坏，平均情况时间复杂度的概念。最好的情况是数据在数组开头，我们查找一次就可以，时间复杂度是 O(1)。最坏的情况我们遍历了整个数组才找到，时间复杂度是 O(n)。</p>
<p>find 函数的平均情况时间复杂度可以表示为查找需要遍历的平均数。要查找的变量 x 在序列 seq 中有 n+1 种情况：0~n-1个位置和不在序列中。假设查找值在序列中和不在序列中概率一样都为 1/2，数据出现在在任意位置上的概率为 1/n。<br>那么任意位置上查找到数据的概率是 1/2n。所以整体的复杂度计算为 1<em>1/2n + 2</em>1/2n + …. + n*1/2n + n * 1/2 = (3n+1) / 4 ，所以平均时间复杂度为O(n)</p>
<p>很多时候，我们使用一个复杂度就可以满足需求了。只有同一块代码在不同的情况下，时间复杂度有量级的差距，我们才会使用这三种复杂度表示法来区分。</p>
<p>平均时间复杂度经常需要考虑到概率，因此也叫加权平均时间复杂度</p>

        <h3 id="均摊时间复杂度"   >
          <a href="#均摊时间复杂度" class="heading-link"><i class="fas fa-link"></i></a>均摊时间复杂度</h3>
      <figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> array = <span class="type">Array</span>.ofDim[<span class="type">Int</span>](<span class="number">10</span>)</span><br><span class="line"><span class="keyword">var</span> len = <span class="number">10</span></span><br><span class="line"><span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(a:<span class="type">Int</span>) = &#123;</span><br><span class="line">    <span class="keyword">if</span>(i &gt;= len)&#123;</span><br><span class="line">        <span class="keyword">var</span> new_array = <span class="type">Array</span>.ofDim[<span class="type">Int</span>](len * <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until len)&#123;</span><br><span class="line">            new_array(i) = array(i)</span><br><span class="line">        &#125;</span><br><span class="line">        array = new_array</span><br><span class="line">        len = <span class="number">2</span> * len</span><br><span class="line">    &#125;</span><br><span class="line">    array(i) = a</span><br><span class="line">    i = i + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>
<p>下面分析一下上述函数的的均摊时间复杂度，一般用摊还分析(耗时操作均摊给给非耗时操作)的方法来分析，add 函数用于像数组中添加元素，每当达到数组长度上限时，将数组扩充 2 倍。<br>可以很容易看到，正常添加情况下，时间复杂度是 O(1)。只有当数组长度达到上限需要复制扩充时，时间复杂度为 O(array.len)。<br>调用10次，数组扩充到20，复制10次<br>调用10次，数组扩充到40，复制20次<br>调用20次，数组扩充到80，复制40次<br>不考虑第一次特殊情况，每次数组倍增之前，函数都会被调用n/2次，倍增时执行一轮n次拷贝操作。所以平均时间复杂度为 1/(n/2)*(n/2-1)+n/(n/2)<br>摊还分析，每一次O(n)操作后，都跟着n次O(1)操作，耗时操作均摊给给非耗时操作，就变成了常量级 O(1) 操作</p>

        <h2 id="空间复杂度"   >
          <a href="#空间复杂度" class="heading-link"><i class="fas fa-link"></i></a>空间复杂度</h2>
      <p>空间复杂度是程序运行时所占用的存储空间大小的量度，并非具体值，同样反映的也是趋势，用 S(n) 来表示。空间复杂度比较常用的有：O(1)、O(n)、O(n²)</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> a = <span class="number">1</span></span><br><span class="line">a = a + <span class="number">1</span></span><br><span class="line"><span class="keyword">var</span> array = <span class="type">Array</span>.ofDim[<span class="type">Int</span>](<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> (i &lt;- <span class="number">0</span> until array.length)&#123;</span><br><span class="line">    array(i) = i</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>a = 1，a 算法执行所需要的临时空间不随着某个变量n的大小而变化，空间复杂度为 O(1)。<br>第三行声明长度为10的数组，后面的循环不会对数组的空间产生变化，空间复杂度为 O(n)</p>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机网络-因特网的发展（WIP）</title>
    <url>/posts/8d1b737/</url>
    <content><![CDATA[
        <h3 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h3>
      <p>最近在重读计算机网络，好多概念都忘记了，本系列记作重读的笔记。</p>
<a id="more"></a>


        <h3 id="因特网的发展"   >
          <a href="#因特网的发展" class="heading-link"><i class="fas fa-link"></i></a>因特网的发展</h3>
      <p>因特网的发展大体上分为了三个阶段</p>
<ul>
<li>从单个网络 ARPANET 向互联网发展，由于 ARPANET 单个互联网络不能满足所有的通信问题，互联网开始发展。1983 年，TCP/IP 协议成为 ARPANET 网络的协议标准，所有满足 TCP/IP 协议的计算机都能通过互联网进行通信，至此，形成了现代因特网的雏形。</li>
<li>逐步形成三级结构的因特网，分为主干网，地区网，和校园网。</li>
<li>逐步形成了多层次 ISP 结构的因特网。</li>
</ul>

        <h3 id="分组交换"   >
          <a href="#分组交换" class="heading-link"><i class="fas fa-link"></i></a>分组交换</h3>
      <p>分组交换是现代计算机网络的基石</p>
]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>WIP</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark JDBC 及 join BNLJ 调优</title>
    <url>/posts/1c8723dc/</url>
    <content><![CDATA[
        <h4 id="前言"   >
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a>前言</h4>
      <p>在用 Spark load impala 表做非等值连接时碰到了一些问题。表现为加载源数据慢及做 join 操作异常慢。本文记录逐步解决这些问题的过程。</p>
<a id="more"></a>


        <h4 id="记录"   >
          <a href="#记录" class="heading-link"><i class="fas fa-link"></i></a>记录</h4>
      <p>需求其实很简单，存在两张表，表A 和表B。<br>表A schema 如下。login_as_cfid 字段存在于表B 中的 $device_id_list 列表中，数据量大概 150w</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: long (nullable &#x3D; true)</span><br><span class="line"> |-- first_id: string (nullable &#x3D; true)</span><br><span class="line"> |-- $update_time: string (nullable &#x3D; true)</span><br><span class="line"> |-- login_as_cfid: string (nullable &#x3D; true)</span><br></pre></td></tr></table></div></figure>
<p>表B schema 如下，$device_id_list 为设备列表，其实是以 \n 分隔的字符串。。。。数据量大概 400w</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: long (nullable &#x3D; true)</span><br><span class="line"> |-- second_id: string (nullable &#x3D; true)</span><br><span class="line"> |-- $device_id_list: string (nullable &#x3D; true)</span><br><span class="line"> |-- $update_time: double (nullable &#x3D; true)</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>
<p>现在需要找到 first_id 和 second_id 的对应关系。首先很自然的想到就是两张表做 join，join 的连接条件如下。</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line">array_contains(col(<span class="string">&quot;$device_id_list&quot;</span>), $<span class="string">&quot;login_as_cfid&quot;</span>)</span><br></pre></td></tr></table></div></figure>
<p>emmn，很快便有了以下代码</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> load_imapla:(<span class="type">String</span>, <span class="type">String</span>) =&gt; <span class="type">Dataset</span>[<span class="type">Row</span>] = (impala_url, table) =&gt; &#123;spark.read</span><br><span class="line">  .format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;url&quot;</span>, <span class="string">s&quot;<span class="subst">$impala_url</span>&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;driver&quot;</span>, <span class="string">&quot;com.cloudera.impala.jdbc41.Driver&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">s&quot;<span class="subst">$table</span>&quot;</span>)</span><br><span class="line">  .load()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> a = load_impala(<span class="string">&quot;xx&quot;</span>, <span class="string">&quot;UsersA&quot;</span>).<span class="keyword">with</span></span><br><span class="line"><span class="keyword">val</span> b = load_imapla(<span class="string">&quot;xx&quot;</span>, <span class="string">&quot;UsersB&quot;</span>)</span><br><span class="line">	.withColumn(<span class="string">&quot;$device_id_list&quot;</span>, strToListUDF(col(<span class="string">&quot;$device_id_list&quot;</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> syncUsersDF = a.join(b, array_contains(col(<span class="string">&quot;$device_id_list&quot;</span>), $<span class="string">&quot;login_as_cfid&quot;</span>), <span class="string">&quot;left&quot;</span>)</span><br><span class="line">syncUsersDF.show()</span><br></pre></td></tr></table></div></figure>
<p>好了，那现在就跑一把看看，等了快1h左右还没有跑完，去 Spark UI 上看执行 DAG 图如下。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1gj1jee8g62j31gl0u0gth.jpg"></p>
<p>可以看到耗时较长的原因有以下两点，其中 BNLJ 巨慢，是主要原因</p>
<ol>
<li>数据源端，load 一张30w 的表花了1.7min</li>
<li>BNGL 处。这里广播了右表，然后遍历左表进行 nest loop join，每秒钟几百条的速度在处理。<br>针对以上两点逐步优化，查看load 表的job，发现只有一个 task 在跑，单线程的，遂想到 jdbc 调参，在加载表的时候加大并发度。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1gj1jl8zyqmj3272062q4j.jpg"></li>
</ol>
<p>JDBC 调优主要涉及到以下 4 个参数。numPartitions 用于指定最大分区数，后面三个字段分别用来指定分区字段及划分每个分区的参数。在<code>org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo</code> 中可以看到计算分区的 <code>columnPartition</code> 方法。<br>这里突然想到在自定义数据源时，也可以自定义分区方法。</p>
<div class="table-container"><table>
<thead>
<tr>
<th>options</th>
<th>description</th>
</tr>
</thead>
<tbody><tr>
<td>numPartitions</td>
<td>最大分区数，最终的分区数并不一定是这个值，当 <code>upperBpund-lowerBound&lt;numPartitions时</code>，最终的分区数为<code>upperBpund-lowerBound</code>，具体的分区计算逻辑可以从源码中看到</td>
</tr>
<tr>
<td>partitionColumn</td>
<td>指定分区字段，分区字段类型必须为数值和时间类型</td>
</tr>
<tr>
<td>lowerBound</td>
<td>分区下界</td>
</tr>
<tr>
<td>upperBpund</td>
<td>分区上界</td>
</tr>
</tbody></table></div>
<p>修改加载代码如下</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> load_imapla:(<span class="type">String</span>, <span class="type">String</span>) =&gt; <span class="type">Dataset</span>[<span class="type">Row</span>] = (impala_url, table) =&gt; &#123;spark.read</span><br><span class="line">  .format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;url&quot;</span>, <span class="string">s&quot;<span class="subst">$impala_url</span>&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;driver&quot;</span>, <span class="string">&quot;com.cloudera.impala.jdbc41.Driver&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">s&quot;<span class="subst">$table</span>&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;numPartitions&quot;</span>, <span class="number">32</span>)</span><br><span class="line">  .option(<span class="string">&quot;partitionColumn&quot;</span>, <span class="string">&quot;id&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;lowerBound&quot;</span>, <span class="string">&quot;1&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;upperBound&quot;</span>, <span class="string">&quot;100000&quot;</span>)</span><br><span class="line">  .load()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>继续查看执行情况，可以看到加载数据源任务的并发度为 32，job 的执行时长缩减到了1.1min，并没有优化太多，可能是分区不均匀导致的。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1gj1jyxzl23j326y0bwtar.jpg"></p>
<p>针对 BNLG，因为这里是非等值连接，默认必然是这样的执行计划，inner join 情况下是做 crossjoin 笛卡尔积，更慢。那么有没有办法转换为等值连接呢，我这里的连接条件是列表包含，那么其实可以先做一层预处理 explod 列表，做等值连接。这样 join 的执行计划就是 sortMergeJoin。修改 join 代码如下</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> b = load_imapla(<span class="string">&quot;xx&quot;</span>, <span class="string">&quot;UsersB&quot;</span>)</span><br><span class="line">	.withColumn(<span class="string">&quot;$device_id_list&quot;</span>, strToListUDF(col(<span class="string">&quot;$device_id_list&quot;</span>)))</span><br><span class="line">	.withColumn(<span class="string">&quot;$device_id_list&quot;</span>, explode(col(<span class="string">&quot;$device_id_list&quot;</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> syncUserDF = a.join(b, col(<span class="string">&quot;$device_id_list&quot;</span>) === $<span class="string">&quot;login_as_cfid&quot;</span>)</span><br></pre></td></tr></table></div></figure>
<p>查看执行情况，已经没有了 BNLJ。最后整个 Job 的执行情况耗时1.9 min，之前BNLJ时一个多小时。<br><img src="https://timemachine-blog.oss-cn-beijing.aliyuncs.com/img/007S8ZIlly1gj1jbrbc0tj31jc0u07bg.jpg"></p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
</search>
